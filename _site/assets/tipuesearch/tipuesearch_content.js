var tipuesearch = {"pages": [{
    "title": "Django - 프로젝트 생성 및 관리자 페이지 구현하기",
    "text": "이번 토이 프로젝트는 Django기반의 라벨링툴을 개발하려고 합니다. 우선 기초 공사부터 시작하겠습니다!! 이번 포스팅에서는 관리자 페이지 구현에 대해 다루어보겠습니다. Django 프로젝트 생성 장고가 설치되어 있지 않다면 아래 명령어로 설치해주세요. python -m pip install Django 프로젝트는 아래와 같이 생성합니다. django-admin startproject LabelingTools 저는 LabelingTools 이라는 이름으로 프로젝트를 생성했습니다. Django Setting 처음에 세팅해야 하는 부분은 아래와 같습니다. #settings.py INSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', '' # 앱 이름 추가 ] ... # LANGUAGE_CODE = 'en-us' LANGUAGE_CODE = 'ko' # 한글로 보고싶으면 언어설정 해주기 프로젝트가 제대로 동작하는지 확인하려면 manage.py 파일이 있는 위치에서 아래와 같이 명령어로 실행합니다. python manage.py runserver 기본 포트는 8000번이지만, 포트를 변경하고 싶다면 아래와 같이 수정하여 사용할 수 있습니다. python manage.py runserver 8080 # 또는 python manage.py runserver 0:8080 추가로 매번 python manage.py runserver를 치는 것이 불편하여 bat파일을 작성해줍니다. @echo off echo ===============Run Django=================== python ./labeling/manage.py runserver 저는 앞으로 python manage.py runserver 라는 명령어 대신 call run.bat을 입력하면 장고가 실행됩니다. Django 관리자 페이지 Django는 많은 기능을 제공하는데, 그 중 관리자 페이지도 기본적으로 제공됩니다. 그렇기 때문에 많은 작업을 하실 필요는 없습니다. 아래의 명령어를 실행해서 슈퍼유저를 생성합니다. python manage.py migrate python manage.py createsuperuser Username, Email address, Password 입력까지 끝냈다면 http://127.0.0.1:8000/admin 으로 들어가면 아래와 같은 화면을 볼 수 있습니다. 로그인하면 아래와 같은 화면을 볼 수 있으며 여기서 유저 추가, 유저 권한 부여도 가능합니다. 가끔 관리자 인증 문제가 있어서 다른 컴퓨터로 pull 받고 실행이 안될 때가 있어서 settings.py에 아래 코드를 추가해줍니다. # settings.py AUTHENTICATION_BACKENDS = [ 'django.contrib.auth.backends.ModelBackend', ] 이제 어디서든 관리자 페이지에서 로그인이 가능합니다.",
    "tags": "ToyProject",
    "url": "/toyproject/2022/09/03/django-labeling-tools.html"
  },{
    "title": "Docker와 YOLOv4 사용하기",
    "text": "Docker환경에서 YOLOv4를 학습시키려고 한다. GPU로 돌릴 예정이기 때문에 Docker Hub에서 Nvidia CUDA 이미지를 사용했다. docker pull nvidia/cuda:11.6.0-cudnn8-devel-ubuntu20.04 버전은 각자 컴퓨터 사양에 맞게 하면 되는데 나는 11.6 버전으로 진행하였다. 해당 이미지는 cudnn까지 설치되어 있어서 따로 cudnn까지 설치는 하지 않아도 된다. 이미지를 실행 시킬때 --runtime=nvidia 라던가 --gpus 옵션을 사용하지 않는다면 cpu로 돌아가니 주의하자. docker run -it --gpus all -e DISPLAY=unix$DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix --privileged nvidia/cuda:11.6.0-cudnn8-devel-ubuntu20.04 나는 Docker에서 yolo를 돌리면서 이미지나 그래프를 확인하고 싶어서 내 화면에 띄우기 위해 DISPLAY 옵션을 넣었다. 나처럼 도커에서 실행한 이미지결과를 확인하고 싶다면 Xming X Server for Windows를 설치, 도커 실행 후 apt-get install x11-apps 와 export DISPLAY=WINDOWS_IP:0.0 를 해주면 된다. 이미지 실행 후 nvidia-smi 와 nvcc -V으로 CUDA가 잘 잡히는지 확인하자. 참고로 nvidia-smi 에서 보이는 CUDA Version의 경우에는 현재 driver와 호환이 잘되는 CUDA버전을 추천해주는 것이지 현재의 CUDA 버전을 이야기하는 것은 아니다. 그래서 nvcc -V로 현재의 버전을 보면 된다. 이제 yolo를 사용하기 위해 아래와 같이 Git clone 받자 apt-get update apt install git apt install vim apt install wget sudo apt-get install libopencv-dev # opencv=0으로 할꺼면 빼도 된다. git clone https://github.com/AlexeyAB/darknet.git 아래와 같이 파일이 들어와 있다면 잘 된 것이다 우선 Makefile을 아래와 수정하자. 그리고 make 를 입력하면 아래와 같이 darknet 파일이 생기는 것을 볼 수 있다. 가중치 파일을 다운 받아야하는데 나는 yolov4.conv.137 를 사용하였다. wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.conv.137 ./darknet detector train cfg/coco.data cfg/yolov4.cfg yolov4.conv.137 vi로 coco.data 경로 맞춰주고 일부 수정한다면 아래와 같이 돌아가는 모습을 볼 수 있다. 만약 학습말고 기존의 가중치로 결과를 확인하고 싶다면 아래와 같이 명령어를 입력하면 확인할 수 있다. wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights ./darknet detect cfg/yolov4.cfg yolov4.weights data/dog.jpg 도커에서 실행한 이미지를 내 컴퓨터로 띄어서 확인할 수 있다.",
    "tags": "MachineLearning",
    "url": "/machinelearning/2022/08/20/docker-yolo4.html"
  },{
    "title": "가짜 상관관계(spurious correlation)란?",
    "text": "Spurious Correlation 두 변수가 상관관계가 있다고 이야기 했지만, 알고보니 두 변수가 전혀 관계 없는 경우 Spurious Correlation이라고 말한다. 예를 들어 해변에서의 아이스크림 판매와 상어의 공격은 양의 상관관계가 있다. 아이스크림 판매의 증가함에 따라 상어의 공격 횟수가 증가했다고 한다. 그러나 상식적으로 생각했을 때, 아이스크림 판매는 상어의 공격과 전혀 상관없다. 어떻게 이런 결과가 나올 수 있을까 ? Confounding Variables 해당 예시에서는 세 번째 변수가 해당 두 변수 사이에서 다른 상관 관계를 생성할 때 발생한다. A가 증가하면 B와 C가 함께 증가한다. 따라서 B-&gt;C로 보이는 경우다. 예시의 아이스크림 판매의 경우 높아진 기온에 의해 더 많은 사람들이 아이스크림을 사거나 해변에서 수영하게 되어 상어의 공격 기회가 증가하게 되었을 뿐이다. 따라서 아이스크림 판매와 상어의 공격 사이에는 아무런 관련이 없음에도 불고하고 그래프로 표시하게 되면 함께 오르락 내리락 하는 경향이 있다. Mediating Variables 다른 경우는 일련의 상관관계가 가짜 상관관계를 만든다. 예를들어 A-&gt;B-&gt;C 일 경우인데, A와 C의 값만 있으면 상관관계를 만들수 있다. 실제로는 A와 C사이에는 직접적인 연결이 없음에도 가짜 상관관계 현상을 볼 수 있다. Random Sampling Error 모집단에서 추출한 어떠한 표본을 사용하여 연구한다고 해보자. 해당 표본에서 찾은 상관관계가 모집단에 존재하지 않을 수 있다. Solution 그럼 가짜 상관관계를 어떻게 파악할 수 있을까 ? 아쉽게도 가짜 상관관계의 경우 단번에 파악할 수 있는 것은 없어서 직접 통계적인 방법이나 실험적 방법, 다양한 그래프를 그려가며 이 상관관계가 진짜인지 가짜인지 연구하는 방법뿐이다. 참고로 다중공선성(multicollinearity)의 경우는 독립변수들간의 상관관계가 강하게 나타나서 회귀분석의 전체가정인 독립변수들간에 상관관계가 높으면 안된다는 조건을 위배하는 경우를 의미하기 때문에 spurious correlation과 다른 문제임을 기억하자.",
    "tags": "MachineLearning",
    "url": "/machinelearning/2022/08/14/spurious-correlation.html"
  },{
    "title": "머신러닝에서 추론과 예측",
    "text": "머신러닝 개발자로 일을하다보면, “예측”과 “추론” 이라는 단어를 많이 듣는다. 이 둘의 차이가 무엇일까? 예측 예측은 미래 가치를 추정하는 값을 말한다. 예를 들어 주식 예측, 집값 예측, 버스 도착 시간 예측 등이 있을 수 있다. 그리고 예측에는 온라인 예측과 배치 예측 두가지 종류가 있다. 온라인 예측(oneline prediction)은 거의 실시간으로 적은 수의 예측값을 얻고자할 때 사용된다. 배치 예측(batch prediction)은 오프라인에서 많은 양의 데이터에 대한 예측을 생성하는 것을 말한다. 온라인 예측의 경우 어떻게 사용할 수 있을까? 쉽게 생각하면 인스타그램에서 사용될 수 있는데, 예를 들어 내가 어떠한 해시태그를 검색 했을 때, 실시간 데이터에 의해 30분 후 관련 추천 게시물이 나에게 뜨는 것이 예로 볼 수 있다. 30분마다 데이터를 받아서 예측하는 모델이라 가정 배치 예측 작업은 많은 양의 데이터에 대한 예측을 분석할 때 유용하다. 배치 예측은 온라인 예측보다 더 많은 데이터를 다루기 때문에 배치 예측이 온라인 예측보다 오래걸린다. 추론 그렇다면 추론은 무엇일까? 우리는 이미지를 분류하거나, 텍스트 리뷰에 대한 감정 분류와 같은 모델을 보고 예측이라는 말을 사용하지 않는다. 예측은 미래를 뜻하는 단어이기 때문이기 때문에 ‘예측’이라는 용어를 사용하긴 어렵다. 이런 경우에는 ‘예측’이라는 용어를 대체할 수 있는 ‘추론’이라는 용어를 사용한다.",
    "tags": "MachineLearning",
    "url": "/machinelearning/2022/08/11/ml-predict-inference.html"
  },{
    "title": "Deep Learning에서의 DAG구조",
    "text": "DAG는 Directed Acyclic Graph의 약자이다. 순환 그래프가 아닌 비순환 그래프를 말하며, 순환하는 싸이클은 존재하지 않고 일방향성만 가진다. 그래프 종류와 딥러닝에서의 DAG구조에 대해 알아보자. Graph 종류 그래프에 대해 전부 다루게 되면 논점이 흐리게 될 것 같아, 무방향 그래프(Undirected Graph)와 방향 그래프(Directed Graph)만 간단하게 다룰 것이다. 무방향 그래프 (Undirected Graph) 무방향 그래프는 말 그대로 방향이 없는 그래프를 말한다. 간선을 통해 노드는 양방향으로 갈 수 있다. 방향 그래프 (Directed Graph) 방향 그래프는 간선에 방향이 있는 그래프를 말한다. 사이클(Cycle)과 비순환 그래프 (Acyclic Graph) 사이클은 단순 경로의 시작 노드와 종료 노드가 동일할 경우를 말하고 비순환 그래프는 사이클이 없는 그래프를 말한다. (좌)비순환 그래프 / (우)사이클 Directed Acyclic Graph DAG는 비순환 그래프를 말하며, 순환하는 사이클이 존재하지 않고 일방향성만 가진다. 순환한다는 것은 출발한 노드에서 시작하여 끝내 다시 시작노드로 돌아가는 것이 순환 반복될 수 있는 그래프인데, 위 그래프처럼 다시 되돌아갈 간선이 없는 그래프라면 비순환 그래프라고 한다. Deep Learning DAG 일반적으로 DAG는 작업들의 우선순위를 표현할 때, DAG구조를 사용한다. 예를 들어 공장에서 작업 스케줄링을 할 때, A라는 작업이 끝나고 B를 해야하고 B가 끝난 다음에는 C,D를 해야한다는 것을 DAG로 표현할 수 있다. 딥러닝에서는 여러 개의 Task로 나뉘어서 순차적으로 실행할 필요가 있다. 만약 DAG가 아닌 사이클 구조를 가진다면 그 작업은 영원히 완수되기 어렵기 때문이다. 따라서 작업간의 순서를 그래프로 표현할때는 DAG로 표현하는 것이 일반적이다. 요즘은 이런 작업 흐름을 관리하기 위해 workflow 도구가 나와있으며 그 중 하나인 Airflow는 Graph View 기능을 제공한다.",
    "tags": "MachineLearning",
    "url": "/machinelearning/2022/08/10/ml-directed-acyclic-graph.html"
  },{
    "title": "파이썬 for-else 사용하기",
    "text": "if문에 else문이 있는데, else는 for문에도 있다. 친숙하지 않겠지만, 사용법을 이해하면 유용하게 사용할 수 있다. for-else 보통 for문은 아래와 같이 사용한다. for n in range(2, 10): for x in range(2, n): if n % x == 0: print(n, 'equals', x, '*', n//x) break 위 코드는 공식 문서에서 가져온 간단한 예시고, 소수를 찾는 코드이다. 여기서 else를 사용하면 아래와 같이 출력이 해볼 수 있다. for n in range(2, 10): for x in range(2, n): if n % x == 0: print(n, 'equals', x, '*', n//x) break else: print(n, 'is a prime number') else는 루프가 정상적으로 완료되면 실행되며, 이것은 어떠한 break문도 만나지 않았다는 것을 말한다. 루프를 실행하고 각 아이템을 검색하는 것이 일반적인 구조인데, 루프를 종료시키는 시나리오에는 두가지가 있다. 첫번째는 아이템을 발견하고 break를 만나 루프를 빠져나오거나, 두번째는 루프를 끝까지 도는 것이다. 이때 어떤 것이 루프를 종료시키는 원인인지 궁금할 때, 위 코드처럼 루프가 끝나면 플래스를 확인하는 방법으로 사용할 수있다.",
    "tags": "Python",
    "url": "/python/2022/07/12/python-for-else.html"
  },{
    "title": "에러 - Anconda 가상환경 구축 후, env name 사라지는 현상",
    "text": "문제점 평소 keras만 쓰다가, 갑자기 pytorch를 써야되서 conda 환경 하나 팠다. 그리고 몇칠 후, 다른 환경으로 가려고 평소 쓰던 환경으로 conda activate [env name]을 쳤는데, 안된다? 급히 conda env list 명령어로 확인하니, 이름이 사라진 것을 보게 되었다. 진짜 가상환경 이름만 사라져서 당황스럽다. 확인해보니, [env name] 을 적는 부분에 직접 경로를 입력해주니까 해당 환경으로 들어가진다. 해결 방안 Stackoverflow에서는 conda update 하라고 했는데, 나는 소용없었고, conda config --add envs_dirs &lt;path to envs&gt; 명령어로 해결하였다. 좀더 찾아보니, 가끔 conda에서 업데이트나 여러 환경을 만들다가 발생할 수 있는 문제라고 한다.",
    "tags": "ETC",
    "url": "/etc/2022/07/03/conda-env-name-missing.html"
  },{
    "title": "프로그래밍의 패러다임에 대해 알아보자.",
    "text": "이전 포스팅에서 파이썬은 ‘멀티 패러다임’ 이라고 말했다. 오늘은 그에 대해 조금 더 자세히 이야기 해볼까한다. 프로그래밍 패러다임 프로그래밍의 패러다임이란, 개발자가 코드를 짤 때, 어떤 관점을 가지고 짜느냐에 결정적인 역활을 한다. 크게 선언형과 명령형으로 나눌 수 있고, 이 안에는 객체 지향과 절차 지향, 함수형 프로그래밍이 포함되어 있다. 명령형 프로그래밍 : 절차 지향 프로그래밍, 객체 지향 프로그래밍 선언형 프로그래밍 : 함수형 프로그래밍 프로그래밍 패러다임의 특징 명령형 프로그래밍 : 알고리즘을 명시하고 목표는 명시하지 않음. 선언형 프로그래밍 : 알고리즘을 명시하지 않고 목표만 명시함. 사람이 횡단보도를 건너는 프로그래밍을 짠다고 가정한다면 아래와 같이 볼 수 있다. 명령형 프로그래밍 : 횡단보도까지 약 5m이동하고, 초록불 일 때까지 기다렸다가 초록불이면 약 10m 앞으로 이동한다 선언형 프로그래밍 : 초록불이 되면 길을 건넌다. 이렇게 명령형은 사람이 걸어가는 과정(how)의 알고리즘을 명시하지만, 프로그램의 목적은 명시하지 않는다. 반대로 선언형은 초록불이 되면 길을 건넌다라는 목표만(what)을 명시한다. 절차 지향과 객체 지향 프로그래밍 절차 지향과 객체 지향 개념은 서로 반대 개념이 아니다. 둘다 명령형 프로그램의 하위 개념이기에 공유하는 것이 많다. 절차지향 프로그래밍 프로그램의 순서와 흐름을 중점으로 필요한 자료구조와 함수를 설계 (데이터 중심으로 절차적으로 실행에 초점) 객체지향 프로그래밍 자료구조와 함수를 먼저 설계하고 그 후에 실행 순서와 흐름을 설계 (객체간의 관계에 초점) 관점 지향 프로그래밍(Aspect Oriented Programming) 관점지향 프로그래밍의 핵심은 공통 모듈을 분리시켜 해당 소스코드가 외부의 다른 클래스에서 존재하는 것이다. 참고로 객체지향을 보조하는 역활이기 때문에 객체지향 코드위에서 이루어진다. 객체지향은 객채(Object)라는 개념을 활용함으로써 큰 프로그램을 모듈 단위로 축소시켜 작성할 수 있게 했지만, 프로그램의 크기가 엄청나게 커지면서 모듈 안에서마저 중복되는 코드가 생기게 된다. 이렇게 모듈 안에서 중복코드가 생기게 되는 것을 횡단 관심사(Crosscutting-Concerns) 라고 한다. 그 중에 자주 언급되는 것은 트랜잭션, 로깅, 성능 분석 등이 있다. 횡단 관심사들은 여러 모듈들을 말 그대로 횡단하면서 존재하게 되며 AOP를 이해함에 있어서 매우 중요한 개념이며, AOP를 통해 중복 코드 제거, 효율적인 유지복수, 높은 생산성, 재활용성 극대화 등의 이점을 얻을 수 있다. 함수형 프로그래밍 쉽게 말하면 함수형 프로그래밍은 변수 사용을 최소화함으로써 오류를 줄이는 프로그래밍이다. 정의는 자료 처리를 수학적 함수의 계산으로 취급하고, 상태와 가변 데이터를 멀리하는 프로그래밍 패러다임 중 하나이다. 왜 변수 사용을 최소화 해야하는 걸까 ? 변수는 프로그램에서 개발자가 메인 메모리 공간에 올려놓은 값인데, 값이 대입되면 바뀔 수 없는 상수와 달리 변수는 언제든 값을 변경할 수 있다. 적절히 사용하면 좋겠지만, 여러 스레드가 돌아가면 변수는 오류의 원인으로 작용할 가능성이 크다. 함수형 프로그래밍은 외부에 따로 상태 값을 두지 않고 내부에서 연쇄적으로 기능을 사용해서 결과를 연산하기 때문에 외부 요소의 영향을 받지 않고 안전한 코드를 작성 할 수 있다. 함수형 프로그래밍의 특징 함수형 프로그래밍에는 5가지의 특성이 있다. 순수 함수 (Pure function) 함수형 프로그래밍에서는 일반적인 함수의 조합이 아니라 순수함수의 조합이다. 동일한 입력에는 항상 같은 값을 반환해야 한다. 함수의 출력(return)은 그 함수의 입력(input)에만 의존한다. 함수의 실행은 프로그램의 실행에 영향을 미치지 않아야 한다. (return만 수행한다는 의미) 고차 함수 (Higher-Order Functions) 함수를 인자(argument)로 받는다. 함수의 리턴값으로 함수를 사용할 수 있다. 불변성(Immutability) 변하지 않는 성질을 말한다. 불변성을 지키기 위해서는 데이터 변경이 필요할 경우 원본을 유지한 채 복사본을 만들어 작업해야한다. 1급 객체(First-Class Object) 1급 객체란 함수를 값으로 다루는 것을 이야기한다. 특징으로는 다음과 같은 것들이 가능한 객체를 의미한다. 변수나 데이터 구조 안에 담을 수 있다. 파라미터로 전달 할 수 있다. 반환값으로 사용할 수 있다. 할당에 사용된 이름과 무관하게 고유한 구별이 가능하다. 참고로 순수 함수들과는 다르다는 것을 인지해야한다. 익명 함수(Anonymous function) 이름 없는 함수를 말하며 람다식으로 표현되는 함수 구현을 말한다. 파이썬은 멀티패러다임 언어 파이썬은 절차지향, 객체지향, 함수형 프로그래밍을 지원하는 멀티패러다임 언어이다. 예시를 통해 알아보자. 절차 지향 - Jupyter Notebook 절차 지향은 위에서도 언급했듯 데이터를 중심으로 순차적으로 코드를 구현하는 프로그래밍 방식이다. Jupyter Notebook을 이용하여 데이터 분석을 하는데, 이것은 절차지향적 방식이라고도 볼 수 있다. 주피터의 경우 셀 단위로 전처리, 모델링, 시각화 코드를 순차적으로 작성하고 실행시켜 결과를 확인하기 때문이다. 객체 지향 - ML Model, Django 객체 지향은 추상화된 클래스와 이에 대한 속성과 행위를 정의하고 이를 중심으로 개발하는 프로그래밍 방식이다. 머신러닝 모델은 대부분 클래스로 정의 되어 있고, fit(), predict()로 훈련과 예측을 할 수 있도록 모델이 구현되어 있다. 머신러닝 개발자들은 모델 객체를 선언하고 활용하는 법만 알면 따로 모델을 구현할 필요 없이 머신러닝 모델을 활용할 수 있다. 또한 Django 역시 파이썬 객체지향 프로그래밍의 대표적인 예시이다. 야놀자 같은 사이트를 만든다고 가정할 때, 사용자, 방, 예약, 리뷰 등의 개념을 객체로 구성하고 이를 관리하기 위한 데이터 모델이나 화면에 표시하기 위한 뷰를 만들어 웹 사이트를 만들게 된다. 함수형 - PySpark 데이터 분석에서 함수형 프로그래밍의 특징은 thread-safe하고 병렬계산을 가능하게 하기 때문에 대용량 데이터를 다룰 때 활용된다. 예시 코드는 아래와 같다. lines = sc.textFile(\"data.txt\") lineLengths = lines.map(lambda s: len(s)) totalLength = lineLengths.reduce(lambda a, b: a + b) 코드를 보면 별도의 변수의 선언 없이 함수들의 조합만으로 원하는 결과를 얻을 수 있다. 또한 리스트 컴프리헨션을 이용해 새로운 리스트를 선언 없이 리스트 값을 필터링 한다는 등이 있다. 관점지향 - Decorator 관점지향 프로그래밍의 경우 decorator를 잘 사용하는 것이라 보면 된다. reference https://velog.io/@gillog https://hoi5088.medium.com/ https://docs.python.org/3/howto/functional.html https://jinwoo1990.github.io/dev-wiki/python-concept-1/",
    "tags": "Python",
    "url": "/python/2022/06/23/multi-paradigm-languages.html"
  },{
    "title": "파이썬의 특징에 대해 자세히 알아보자.",
    "text": "최근 내가 자주 즐겨보는 개발자 유튜버분이 책을 한 권 내셨는데, 난 그분의 팬이라 아묻따 바로 책 구입했다. 역시나, 책은 알차다. 오늘은 그 책을 보면서 파이썬에 대해서 전체적으로 이야기 해볼까한다. 인터프리터식, 객체지향적, 동적 타이핑(dynamically typed) 대화형 언어이다. 인터프리터는 무엇이고 객체지향과 동적 타이핑은 무엇일까 ? 인터프리터(Interpreter)와 컴파일(Compiler) 컴파일과 인터프리터는 간단하게 말하면 내가 작성한 코드가 컴퓨터가 읽을 수 있는 ‘기계어로 번역하는 과정’에서 어떤 방식으로 번역하는 것인지를 분류한거라고 볼 수 있다. 컴파일 언어의 경우 프로그래밍이 마친 뒤 코드 전체를 미리 기계어로 번역(컴파일)한 다음에 실행 파일을 생성해서 실행하는 언어이다. 인터프리터의 경우 작성된 코드를 한 줄 한줄 읽으면서 기계어로 번역하면서 실행되는 언어이다. 위 글을 읽고 눈치 챈 사람도 있겠지만, 인터프리터의 경우 컴파일 과정이 없기 때문에 따라서 컴파일 에러 또한 없다. 컴파일 언어와 인터프리터 언어의 가장 큰 차이는 속도다. 컴파일 언어의 단점으로 컴파일이라는 과정 때문에 개발 시간은 늘어나지만, 장점으로는 오류로부터 비교적 안전하고 프로그램 실행 속도가 빠르다. 인터프리터의 경우 장점으로 빠른 개발이 가능하지만, 단점으로는 실행 속도가 느리다. 대표적인 컴파일 언어와 인터프리터 언어는 아래 표와 같다. 컴파일 언어 :C , C++, Java , 타입 스크립트, 코틀린, 스위프트, 스칼라, GO, 러스트 인터프리터 언어 : 자바스크립트, 파이썬, 루비, PHP, 펄, R 객체지향과 절차지향 프로그래밍 이제 파이썬이 인터프리터 언어라는 것을 알았다. 그렇다면 객체지향적이라는 뜻은 무엇을까? 먼저 객체지향과 절차지향 프로그래밍에 대해 알아보자. 절차와 객체지향 프로그래밍은 소스코드를 어디서부터 읽느냐에 따라 나눌 수 있다. 절차지향 프로그래밍 소스코드를 위에서부터 차례대로 읽는 방법을 절차지향 프로그래밍이라고 이야기한다. 소스코드를 순차적으로 실행하기 때문에 소스코드의 순서가 굉장히 중요하다. 예를 들어 쿠키를 만든다고 해보자. 쿠키 반죽을 만들고, 토핑을 뿌리고, 쿠키를 굽고, 쿠키를 포장한다. 이 과정은 분리가 되면 안되고, 순서가 틀려서도 안된다. 절차지향 프로그래밍 역시 이와 같으며, 컴퓨터의 처리 구조와 비슷하다. 장점으로는 실행 속도가 빠르다는 점이 있지만, 단점으로는 모든 구성 요소가 연결되어 있기 때문에 사소한 문제 하나만 생겨도 프로그램 전체가 돌아가지 않는다. 또한 실행 순서가 바뀌게 된다면 전혀 다른 결과를 볼 수 있다. 대표적인 절차 지향 언어로는 C언어이다. 이런 단점을 보완하기 위해 나온 개념이 객체지향이며, 대표적인 객체지향 언어가 C++이다. 객체지향 프로그래밍 프로그램을 다수의 객체로 만들고 이 객체끼리 서로 상호 작용하는 방법을 말한다. 객체지향에서는 각 객체의 역활이 나누어져있고, 사용하고자 하는 부분에서 각 객체가 따로 동작한다. 객체지향의 장점으로는 객체 하나에 대해 문제가 생겨도 해당 객체만 수정하면 되기 때문에 유지보수나 생산성이 좋지만, 단점으로는 설계할 때 시간이 많이 걸리며 절치지향보다 처리속도가 느리다. 또한 객체지향 언어는 대체로 난이도가 높다. 그러면 왜 파이썬을 객체지향적 언어라고 하는걸까? 사실 파이썬은 ‘절차’를 지향할 수 있고 ‘객체’를 지향할 수 있다. 파이썬은 멀티패러다임 언어이기 때문에 방식에 따라 다르게 사용할 수있다. 절차지향 언어인 C를보면 함수(function)기능은 사용할 수 있지만, Class처럼 변수와 함수를 집합으로 활용하는 개념이 없다. 여기서 Class가 중요한 이유는 객체를 만드는 설계도이기 때문이다. 따라서 파이썬에서의 Class 기능이 파이썬을 객체지향 언어라고 부르는 대표적인 이유이다. 동적 타이핑(dynamically typed) 동적 타이핑은 코드를 작성하는데 있어서 컴퓨터적 구조를 생략한 것을 이야기한다. 변수를 지정할 때, 변수의 데이터 타입 등을 명시하지 않아도 컴퓨터가 알아서 해석하는 것을 말하며, 실행속도가 느리다. 반대로 정적 타이핑은 컴퓨터적 구조를 명시해야한다. 변수를 지정할 때, int, str타입인지를 반드시 직접 명시해줘야하며, 컴퓨터가 해야할 일을 덜어주기 때문에 동적 타이핑에 비해 실행속도가 무척 빠르다. 파이썬의 특징 인터프리터 언어이다. 동적 타이핑 대화형 언어이다. 간단하고 쉬운 문법. 메모리 자동 관리( Garbage Collection) 높은 확장성 무료 파이썬 사용 분야 파이썬으로 할 수 있는 것은 굉장히 많다. 인공지능 개발 통계 및 분석 웹 애플리케이션 핀테크 및 블록체인 게임 제작 임베디드 시스템의 응용 프로그램 제작 업무 자동화 이미지 출처 http://soen.kr/lecture/ccpp/cpp1/1-1-3.htm https://velog.io/@daon9apples/ https://velog.io/@qnddj-kjh/Java-%EA%B0%9D%EC%B2%B4-%EC%A7%80%ED%96%A5",
    "tags": "Python",
    "url": "/python/2022/06/22/interpreter-compiler.html"
  },{
    "title": "백준 - 브루트포스(Brute Force) 단계별 문제 풀이(Python).",
    "text": "링크 : 브루트 포스 문제집 앞으로 꾸준하게 알고리즘을 풀려고 한다. 최근 여러 일이 겹치다보니, ‘자기계발을 꾸준히 해야겠다!’는 생각이 많이 들어서 최대한 1일 1알고리즘을 해보려고 한다. 물론 어려운 문제 만나면 그게 안될 수 있겠지만, 한번 풀어봐야지! 5월 29일~31일동안 백준에 있는 단계별 - 브루트 포스 문제집 풀었다. 총 5문제였고 시간이 약간 걸렸던 문제는 ‘체스판 다시 칠하기’ 였다. 문제 이해하는데 좀 걸렸던 것 같다. 브루트 포스 브루트 포스는 모든 경우의 수를 무식하게 탐색 하는 것을 말한다. 데이터 전체를 탐색하기 때문에 완전 탐색, 전체 탐색이라고도 불린다. 장점 모든 경우의 수를 다 검색하기 때문에 (이론상)정확도 100%를 보장함. 알고리즘 구현하기 쉽다. 단점 문제의 복잡도(Complexity)에 매우 민감하다. 실행 시간이 오래 걸리고 메모리 효율면에서 비효율적이다. 브루트 포스 종류에는 순차탐색, 백트래킹, DFS, BFS가 있다. 백준 - 블랙잭 (2798번) 문제 요약 N장의 카드에 써져 있는 숫자가 주어졌을 때, M을 넘지 않으면서 M에 최대한 가까운 카드 3장의 합을 구해 출력하시오. 내 풀이 입력 받은 수를 반복문으로 수의 합을 M에서 빼고, 음수가 아닐 경우에는 리스트에 담고, 마지막에 MAX을 출력한다. Answer: k_cnt = list(map(int,input().split(' '))) k_num = list(map(int, input().split(' '))) temp = [] for i in range(0, len(k_num), 2): for j in range(1, len(k_num), 2): for k in range(len(k_num)): k_sum = k_cnt[1] if k_num[k] not in [k_num[i], k_num[j]]: k_sum-=(k_num[i]+k_num[j]+k_num[k]) if k_sum &gt;= 0 : temp.append(k_sum) k_sum = k_cnt[1] print(k_sum-min(temp)) 백준 - 분해합 (2231번) 문제 요약 자연수 N이 주어졌을 때, N의 가장 작은 생성자를 구해내는 프로그램을 작성하시오. 내 풀이 1부터 N까지 반복문을 통해 구한 분해값이 주어진 생성자랑 같으면 그 값을 출력한다. Answer: num = int(input()) ck = 0 for i in range(1,num+1): temp = 0 temp = sum([int(j) for j in str(i)]) if (temp+i) == num: print(i) ck+=1 break if ck == 0: print(0) 백준 - 덩치 (7568번) 문제 요약 각각 (x, y), (p, q)라고 할 때 x &gt; p 그리고 y &gt; q 이라면 우리는 A의 덩치가 B의 덩치보다 “더 크다”고 말한다. 학생 N명의 몸무게와 키가 담긴 입력을 읽어서 각 사람의 덩치 등수를 계산하여 출력해야 한다. 단, 서로 다른 덩치끼리 크기를 정할 수 없는 경우도 있다 내 풀이 모든 경우의 수로 덩치가 큰 사람을 세면 된다. Answer: num = int(input()) temp = [] for i in range(num): temp.append(list(map(int, input().split(' ')))) for i in temp: count = 1 for j in temp: if i[0] &lt; j[0] and i[1] &lt; j[1]: count += 1 print(count, end=\" \") 백준 - 체스판 다시 칠하기 (1018번) 문제 요약 MN개의 단위 정사각형으로 나누어져 있는 M×N 크기의 보드를 8×8 크기의 체스판으로 만들려고 한다. 체스판은 검은색, 흰색 번갈아 칠해졌다. 체스판을 색칠하는 경우는 두 가지인데, 하나는 맨 왼쪽 위 칸이 흰색인 경우, 하나는 검은색인 경우이다. 보드가 체스판처럼 칠해져 있다는 보장이 없어서, 아무대나 골라서 8x8로 잘랐을 경우, 다시 칠해야 하는 정사각형의 최소 개수를 구하는 프로그램을 작성하시오. 내 풀이 행렬의 값에 따라 경우의 수를 나누어 흰색이라면 검은색으로 바꿔야 하는 수를 증가시키고, 검은색이라면 흰색으로 바꿔야 하는 수를 증가 시키고 둘 중에 가장 작은 값을 출력 리스트에 담고 마지막에 출력함. Answer: N,M = map(int, input().split()) board, temp = [],[] for _ in range(N) : board.append(input()) for i in range(N-7): for k in range(M-7): b,w = 0,0 for i_a in range(i,i+8): for k_a in range(k,k+8): if (i_a+k_a)%2 == 0: if board[i_a][k_a] != 'W': b+=1 else: w+=1 else: if board[i_a][k_a] != 'B': b+=1 else: w+=1 temp.append(min(b,w)) print(min(temp)) 백준 - 영화 감독 숌 (1436번) 문제 요약 종말의 숫자란 어떤 수에 6이 적어도 3개이상 연속으로 들어가는 수를 말한다. 종말의 숫자란 어떤 수에 6이 적어도 3개이상 연속으로 들어가는 수를 말한다. 숌은 첫 번째 영화의 제목은 세상의 종말 666, 두 번째 영화의 제목은 세상의 종말 1666 이렇게 이름을 지을 것이다. 일반화해서 생각하면, N번째 영화의 제목은 세상의 종말 (N번째로 작은 종말의 숫자) 와 같다. 숌이 만든 N번째 영화의 제목에 들어간 숫자를 출력하는 프로그램을 작성하시오. 숌은 이 시리즈를 항상 차례대로 만들고, 다른 영화는 만들지 않는다. 내 풀이 반복문으로 1부터 수를 증가시키면서 666이 나올 때마다 +1 씩 증가 시킴. N번째와 증가 시킨 값이 같아지면 break 문으로 탈출함. Answer: num = int(input()) temp, cnt = 0,0 while cnt &lt;= num: temp+=1 if '666' in str(temp): cnt += 1 if cnt == num: print(temp) break",
    "tags": "Algorithm",
    "url": "/algorithm/2022/05/31/algorithm-bruteforce.html"
  },{
    "title": "논문 리뷰 - Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net) for Medical Image Segmentation",
    "text": "Paper URL : https://arxiv.org/pdf/1802.06955.pdf 이번 논문에서 제안하는 모델은 두가지이며, RU-Net과 R2U-Net이라는 모델이다. RU-Net은 U-Net 기반의 RCNN(Recurrent Convolutional Neural Network), R2U-Net은 U-Net 기반의 RRCNN(Recurrent Residual Convolution Neural Network)이다. 논문에서는 Recurrent와 RCNN 아키텍처를 사용하므로써, 보다 효율적인 모델이라고 말하며, U-Net과 동일한 수의 네트워크 매개변수를 갖지만 성능이 더 우수하다고 한다. 요약 기존의 U-Net구조에 Recurrent와 RCNN 아키텍처를 사용했다. RU-Net과 R2U-Net은 U-Net과 동일한 수의 네트워크 매개변수를 갖지만 성능이 더 우수하다. 망막 혈관 분할, 피부암, 폐 분할 데이터에서 민감도, 특이도, 정확도, AUC 의 결과를 보았을 때, 높은 성능을 보였다. RU-NET AND R2U-NET ARCHITECTURES 우선 네가지의 block 대해 알아보자. a : U-Net의 block b : RU-Net의 block c : Residual convolutional block b : R2U-Net의 block 모델에서 제안한 모델의 block은 b와 d 이며, a와 c는 기존 모델들의 block이다. (a) U-Net의 block 전체적인 U-Net 구조는 아래와 같고 구조를 뜯어서 살펴보면, conv Layer + ReLu가 두개가 한 block이고, 이것이 반복되는 것을 알 수 있다. 이것이 U-Net의 기본 block이다. (b) RU-Net의 block RU-Net의 block은 위에 보이는 U-Net block에 Recurrent를 합쳐서 만든 구조인데, 여러 conv 연산이 하나의 커널 가중치를 공유해 여러번 반복해서 연산하는 구조다. 현재 층의 출력을 다시 입력으로 받아 흐르는 구도이기 때문에, 적은 파라미터를 갖고도 더 좋은 성능을 낼 수 있다고 한다. (c) Residual convolutional block Residual Block은 정말 유명한데, 잔차 학습이라고도 불린다. 그림을 보면 알 수 있듯, weight layer들을 통과한 $F(x)$ 와 통과하지 않는 $x$를 합하는 구조이며, 이 block들이 쌓이면 Residual Network(ResNet) 이라는 모델이 만들어진다. (d) R2U-Net의 block R2U-Net는 b + c 의 block을 합친 구조이다. 논문 모델의 최종 아키텍처는 아래 그림과 같다. EXPERIMENTAL SETUP AND RESULTS 혈관 분할 망막 혈관 segmentation을 위해 DRIVE, STARE 및 CHASH_DB1 세 가지 데이터를 사용하였고, 높은 정확도를 보였다. 모델 별로 보자면 아래 그래프와 같다. 피부암 분할 폐 분할",
    "tags": "Paper",
    "url": "/paper/2022/02/28/R2U-Net.html"
  },{
    "title": "논문 리뷰 - 3D U-Net Learning Dense Volumetric Segmentation from Sparse Annotation",
    "text": "Paper URL : https://arxiv.org/pdf/1606.06650v1.pdf 보통 3D 데이터 셋을 이용해 모델을 만들고자 했을 때, 3D데이터를 잘게 슬라이스하여 2D 데이터로 만들어 사용한다고 한다. 대부분의 경우 잘 작동하겠지만, 많은 3D 데이터 세트에 대해서는 이상적인 선택은 아닐 것이다. 그래서 3D 데이터를 사용할 때의 모델 데이터 단위가 2D가 아닌 3D단위를 사용하는 것이 좋다. 그래서 기존의 U-Net(2D)의 구조에서 3D로 바꾼 것이 해당 논문의 핵심이다. 요약 이전에 제시된 U-Net 2D연산을 3D로 확장시켰다. 3D U-Net은 3D volumes을 input으로 받으며, 3D convolutions, 3D max pooling, batch normalization(BN) 을 사용한다. 3D U-Net Architecture Architecture 자체는 이전 U-Net에서 구조에서 크게 바뀌지 않았다. 기존의 2D U-Net 아키텍처 3D U-Net 아키텍처 왼쪽 Layer에는 각 2개의 3x3x3 convolutions, ReLU, strides, max pooling 2x2x2, 오른쪽 Layer에는 각 2개의 2x2x2 up-convolutio, 3x3x3 convolutions, ReLu 마지막 Layer에서는 1x1x1 convolutions은 출력 채널 수를 labels 수인 3으로 줄임. 참고로 각 ReLu전에 batch normalization(BN) 사용. 그리고 그림을 보면, input shape이 output shape보다 작은데, 그 이유는 이미지를 한번에 넣지 않고 patch 단위로 넣었기 때문이다. 결과 3D U-Net은 2가지 방법으로 사용 될 수 있다. a) Semi-automated segmentation: 몇 개의 단면만 annotate이 달린 데이터를 가지고 dense segmentation을 예측. b) Fullyautomated segmentation: annotate이 전부 달린 데이터를 학습 후, non-annotate 데이터 예측. 첫 번째 테이블을 보면 3개의 subset에 대해 3D+BN이 성능이 좋은 것을 알 수 있고, 두번째 테이블은 Semi-automated 학습에 사용되는 annotate 달린 단면(slices)의 개수가 많을수록 더 IoU가 높아진다. Fullyautomated segmentation에서도 2D+BN 보다 3D+BN를 사용하는 것이 좋다고 한다.",
    "tags": "Paper",
    "url": "/paper/2022/02/20/U-Net(3D).html"
  },{
    "title": "논문 리뷰 - Image-to-Image Translation with Conditional Adversarial Networks",
    "text": "Paper URL : https://arxiv.org/pdf/1611.07004.pdf Pix2Pix논문에 대해 리뷰 해볼까한다. 2018년도에 나온 논문이고 상당히 유명한 모델이다. 이번 논문 리뷰에서는 GAN에 대한 기본 개념은 어느정도 있다고 생각하고 글을 썼으니, 혹시나 헷갈리는 부분이 있다면 다시 한번 보고 오기를 바란다. 요약 다양한 image-to-image translation task를 수행하는 general-purpose GAN을 제안했다. 네트워크 구조나, 목적 함수에 변경 없이 다양한 이미지 변환 task를 수행할 수 있다. 저자들은 이를 ‘모델이 데이터에 적응하는 loss를 학습한다’ 라고 표현하고 있다. convolutional conditional GAN을 backbone으로 하였고, 목적함수에 L1 loss 추가하였다. 구체적으로 generator는 U-Net구조, discriminator는 patchGAN을 사용하였다. Introduction 이미지 변환에 대한 과거 연구들은 모두 task마다 별도로 세분화된 모델/기술을 사용하여 문제를 해결하였다. 논문의 저자들은 이미지 변환이라는게 결국 from pixels to pixels 라는 하나의 공통된 세팅하에 수행된다는 사실에 주목하였다. 그래서 모든 이미지 변환 task들을 해결할 수 있는 common framework를 제안하는 것을 목적으로 연구했다. automatic image-to-image translation : 어떤 한 장면에 대한 representation을 다른 representation으로 바꾸어 표현하는 task. Objective 저자들은 이전 연구인 GAN의 main objective에서 L2 distance등의 전통적인 loss를 추가하는 것이 이로움을 확인하였다. (아래 그림 수식(1)) 특히 generator가 ground truth output에 pixel단위로 더 비슷해지게 만드는 효과가 있는데, L2보다 L1이 결과가 덜 blurring해서 저자들은 L1 distance를 사용했다고 한다. 그 결과 생성 이미지의 결과가 완전 deterministic해졌다.(동일한 Input을 넣으면 항상 동일한 Output이 나옴). 저자들은 여기에 약간의 stochastic을 추가하기 위해 dropout을 train/test time 모두에 작동하도록 세팅하였다. 하지만 큰 랜덤성은 확보하지 못했다는 limitation이 남아있다. Model Generator 저자들은 Encoder-decoder 네트워크를 토대로 level별로 처리된 feature들의 정보를 효율적으로 이용할 수 있도록 connection을 추가한 U-Net구조를 generator의 아키텍쳐로 사용하였다. Discriminator L2와 L1 Loss는 계산될때, 모든 픽셀값들에 대해 averaging되기 때문에, image generation시 high-level 특성을 잘 살리지 못하고 blurry한 결과를 줄 수 있다.  그러나, L2, L1 손실들은 low-level 특성은 정확하게 잘 포착해낼 수 있다 따라서 discriminator의 구조를 고려할 때, 저자들은 high-level structure를 잘 살려낼 수 있도록 하는 모델을 선택할 필요가 있었고, PatchGAN의 discriminator를 사용하게 되었다. Optimization &amp; inference 최적화 시키기 위해 모델은 discriminator 한번, generator 한번 번갈아 학습하였고, $log(1-D(x,G(x,z))$를 최소화하는 대신 $logD(x,G(x,z))$를 최대화하는 방향으로 학습하였다. $D$를 최적화하는 동안 목적함수를 2로 나눠서 G와 비교한 상대적 학습률을 조금 늦췄으며, 파라미터는 Adam, lr=0.0002, beta1=0.5, beta2=0.999이다. 특이하게, inference시에도 train시와 동일하게 적용되도록 dropout이나 bn등이 train mode로 작동하게 하였다. Conclusion 결과물만 확인해보면 질적인 실험결과는 cGAN이 세그멘테이션을 잘 수행함을 보여주고 있다. 그러나 양적인 실험결과는 오히려 단순히 노말 GAN+L1 Loss를 사용하는 편이 정확도 등의 측면에서는 더 나음을 보여준다.",
    "tags": "Paper",
    "url": "/paper/2022/02/15/pix2pix.html"
  },{
    "title": "간단하게 오픈 API 사용하기",
    "text": "회사 그만두고 잠시, 다른 회사 노가다 업무 하나 맡아서 했었는데, 그에 대한 일이다. 내가 맡았던 업무는 각종 병원 관련 데이터를 긁어모으는 일이였는데, 생각보다 너무 노가다! 크롤링을 하면 직접하는 것보다 빠르겠지만, 한국인의 성격을 생각하면 느리게만 느껴진다! 그래서 오픈 API를 찾아보니 있어서 그걸로 대체해서 사용했다. 우선 오픈 API를 사용하려면 공공데이터포털에 로그인을 해야한다. 그리고 찾고자 하는 것을 검색해서 사용하고자 하는 API를 찾으면 된다. API를 사용하려면 활용 신청을 해야는데, 오픈 API는 따로 심사를 하지 않고, 활용 신청 후 바로 사용이 가능하다. 신청 후에 마이페이지 들어가서 확인해보면 아래와 같은 개발 계정을 볼 수 있다. 여기서 아래 인증키 두개 중에서 decoding 인증키만 복사한다. 그리고 아래로 내리면 인증키 설정하는 부분에 복사한 키를 넣는다. 설정 완료되면 바로 API 테스트를 할 수 있다. key - b_no에 검색하고자 하는 사업자 번호를 넣어서 테스트 해본다. 그럼 아래와 같이 리턴 값을 받을 수 있다. 이제 여기서 Python으로 실행해보기 위해 Curl을 복사한다. 복사 후 링크를 눌러 Json으로 변환 시키자! json으로 변환이 끝나면 파이참에 붙여넣기를 해보자. 그럼 아래와 같이 결과를 볼 수 있다. 그리고 검색하고자 하는 번호를 josn_data에 넣고 돌리면 끝난다. import requests import pprint headers = { 'accept': 'application/json', 'Authorization': [본인의 인증키], 'Content-Type': 'application/json', } params = ( ('serviceKey', [본인의 인증키]), ) json_data = { 'b_no': [ '1082122133', '1081296521', '1010158608', '1010201835' ], } response = requests.post('https://api.odcloud.kr/api/nts-businessman/v1/status', headers=headers, params=params, json=json_data) result = response.json()[\"data\"] for val in result: pprint.pprint(val['b_stt']) 아래와 같이 결과를 확인 할 수있다. 아무래도 크롤링 보다 훨씬 빠른데, 1회 호출시 최대 100개씩 받을 수 있기 때문에 빠르고 편리하다.",
    "tags": "Python",
    "url": "/python/2022/02/15/python-open-api.html"
  },{
    "title": "해시(Hash)에 대해 알아보자.",
    "text": "해시 테이블, 해시 맵, 해시함수등 … 많이 들어봤을텐데 이번에 간단하게 정리 해볼까한다. 📖 해시 테이블(Hash Table)을 특징을 정리하자면, 아래와 같이 정리 할 수 있다. 임의의 크기를 가진 데이터(Key)를 고정된 크기의 데이터(Value)로 변화시켜 저장. 키에 대한 해시값을 사용하여 값을 저장하고 Key-Value 쌍의 갯수에 따라 크기가 증가. 해시값 자체를 index로 사용하기 때문에 평균 시간복잡도가 O(1)로 매우 빠름. 🖍 해시 테이블과 비슷한 것으로 해시 맵이 있는데, 처음에는 같은 용어인줄 알고 헷갈렸는데, 동기화 지원 여부에 대해 차이가 있다는 걸 알게되었다. 해시 테이블은 동기화를 고려해야대는 상황에서(병렬 처리할 때) 쓰이고, 해시 맵의 경우는 동기화를 고려하지 않아도 되는 상황(병렬 처리 하지 않을 때)에서 주로 쓰인다. 📖 해시 함수(hash function)을 특징을 정리하자면, 아래와 같이 정리 할 수 있다. 입력값의 길이가 달라도 출력값은 언제나 고정된 길이로 반환. 동일한 값이 입력되면 언제나 동일한 출력값을 보장. 출력된 결과 값을 토대로 입력 값을 유추할 수 없음. ❗️해시는 언제 사용될까? 리스트를 사용할 수 없을 때 사용한다. 리스트는 숫자 인덱스를 이용하여 접근하는데, 숫자가 아닌 다른 값을 사용하려고 할 때 사용할 수 있다. 또한 빠른 접근과 집계가 필요할 때 사용한다. 해시는 간단하게 아래와 같이 만들어볼 수 있다. cat = { 'name':'뚜뚜', 'breed': '아메숏', 'age' : 2.5 } # {'name': '뚜뚜', 'breed': '아메숏', 'age': 2.5} 여기서 value에는 무엇이든 넣을 수 있다는 걸 기억하자. # 이렇게 함수를 만들어 함수를 넣을 수 있고, 어떤 객체든 전부 가능하다. def add(a,b): return a+b cat = { 'name':'뚜뚜', 'breed': '아메숏', 'age' : 2.5, 'fun' : add(1,3) } # {'name': '뚜뚜', 'breed': '아메숏', 'age': 2.5, 'fun': 4} 반대로 key에도 무엇이든 넣을 수 있다. ( 리스트 같은건 빼고 ) cat = { 'name':'뚜뚜', 'breed': '아메숏', 'age' : 2.5, 'fun' : add(1,3), 1 : add(1,23), add(1,234) : '덧셈' } # {'name': '뚜뚜', 'breed': '아메숏', 'age': 2.5, 'fun': 4, 1: 24, 235: '덧셈'}",
    "tags": "Algorithm",
    "url": "/algorithm/2021/12/18/algorithm-hash.html"
  },{
    "title": "논문 리뷰 - Conditional Generative Adversarial Nets",
    "text": "Paper URL: Conditional Generative Adversarial Nets 해당 논문은 Conditional GAN이라는 논문을 소개하고 있다. 간단하게 y라는 데이터를 추가하므로써 내가 원하는 데이터를 생성하는 것이 이 논문의 핵심이다. 기존의 GAN은 데이터가 생성되는데에 통제권이 없었다. 이뜻은 무엇일까? MNIST dataset을 기준으로 잡고 이야기 하자면, 내가 숫자 7 이미지를 원한다고 해서 모델이 7 이미지를 만들어주지 않는다는 이야기다. 그렇다면 왜, GAN은 왜 우리가 원하는데로 이미지를 생성해주지 않는걸까? 그 이유는 GAN은 라벨의 정보를 가지고 학습하는 것이라, 데이터 분포를 학습하기 때문이다. 즉, 라벨에 대해 잘 모르지만서도 1을 1답게 하는 데이터 분포를 학습하기 때문이다. 그렇다면, 우리가 원하는 데이터를 만들어주는 GAN을 만들려고 하면 어떻게 해야댈까? 라벨에 대해 잘 모르는 같으니, y라는 특정한 정보 y(라벨)을 넣어준다면, 분포와 함께 우리가 원하는데로 만들어 주지 않을까? 이렇게 데이터를 우리가 원하는데로 제어하는 모델을 conditional GAN 이라고 한다. 요점은 아래와 같다. Discriminator 조건을 토대로 생성된 이미지에 대해 진짜인가, 가짜인가를 판별하고 있다. Generator는 y에 의해 조건을 토대로 이미지를 생성하고 있다. 수식은 GAN이랑 크게 다르지 않다. 논문에서는 MNIST data에 대해 조건 y를 one-hot encoding 시킨 class label을 사용했으며 결과는 아래와 같다. 예쁘게 순서대로 나열된 결과를 볼 수 있다.",
    "tags": "Paper",
    "url": "/paper/2021/12/10/conditional-generative-adversarial-nets.html"
  },{
    "title": "프로그래머스 - 베스트 앨범",
    "text": "링크 : 프로그래머스 - 베스트 앨범 Answer: def solution(genres, plays): temp,temp_index = {},{} answer = [] for val, key in zip(plays, genres): if temp.get(key): temp[key].append(val) else: temp[key] = [val] sort_temp = dict(sorted(temp.items(), key = lambda item: sum(item[1]), reverse=True)) for k in sort_temp: df_val = sorted(sort_temp[k], reverse=True) if len(sort_temp[key]) &gt;= 2: sort_temp[k] = df_val[:2] for key in set(genres): temp_index[key] = [i for i, ele in enumerate(genres) if ele == key] for k in sort_temp: for v1, v2 in zip(sort_temp[k], temp_index[k]): if len(sort_temp[k]) != len(set(sort_temp[k])): df = [i for i, ele in enumerate(plays) if ele == v1] answer.extend(df) break else: answer.append(plays.index(v1)) return answer",
    "tags": "Algorithm",
    "url": "/algorithm/2021/11/18/algorithm-hash4.html"
  },{
    "title": "프로세스와 스레드",
    "text": "프로세스와 스레드를 완벽하게 이해하려면 동시성(Concurrency)과 병렬성(Parallelism)에 대해서도 알고 있어야 하는데, 오늘은 이 개념들까지 정리해볼까한다. 프로세스 프로세스는 프로그램을 실행해서 돌아가고 있는 상태, 즉 컴퓨터가 어떤 일을 하고 있는 상태를 프로세스라고 한다. 그런데 왜 동시성과 병렬성에 대해 알아야 하는 걸까? 그 이유는 운영체제가 여러개의 프로세스를 한번에 돌리고 있기 때문이다. 그렇다면 왜 여러 프로세스를 한번에 돌리는 것일까? 우리가 컴퓨터를 사용할떄, 보통은 한가지 작업만을 하지 않는다. 유튜브를 보며 이렇게 블로그 글을 쓴다거나, 인터넷 쇼핑하면서 카톡을 한다거나… 다양한 작업을 동시에 하게 되는데, 이렇게 동시에 할 수 있는 이유가 이 각각의 프로세스가 병렬성, 동시성 혹은 이 둘의 혼합으로 이루어져 있기 때문이다. 그렇다면 이제 동시성과 병렬성에 대해 알아보자. 동시성(Concurrency) : 프로세스 하나가 이거 조금, 저거 조금… 돌아가면서 일부분씩 진행하는 것을 말한다. 그리고 이렇게 진행 중인 작업을 바꾸는 걸 Context Switching이라고 한다. 이 과정이 너무 빨리 돌아가니까 사람들에게는 이 프로세스들이 동시에 진행되는 것처럼 느껴져서 동시성이라고 부르게 되었다. 병렬성(Parallelism) :프로세스 하나에 코어 여러개가 달려서 각각 동시에 작업들을 수행하는 것을 말한다. CPU의 속도가 발열과 같은 물리적 제약에 직면하여 예전만큼 빠르게 발전하지 못하자, 그 대안으로 여러 코어를 여러개 달려서 작업을 분담할 수 있도록 만든 것이다. 스레드 (Thread) 한 프로세스내에서도 여러 갈래의 작업들이 동시에 진행될 필요가 있다. 이 갈래를 스레드라고 부른다. 프로세스들은 컴퓨터 자원을 분할해서 쓰지만, 스레드는 프로세스마다 주어진 전체 자원을 함께 사용한다. 간단한 예를 보자. 김밥천국의 요리 메뉴 하나하나가 프로세스라고 보자. 프로세스(요리사)는 혼자 돌아댕기면서 이 메뉴들을 만든다. 프로세스안에는 계란부침을 만드는 스레드, 김에 밥을 펴 바르는 스레드, 참치마요를 만드는 스레드 등이 진행된다. 스레드 장점 : 동시에 많은 일을 처리할 수 있다. 때문에 같은 일을 더 빠른 시간안에 처리할 수 있다. 스레드 단점 : 스레드 두개가 동시에 손을 대면, 현실세계에서는 로맨스물이 되지만, 컴퓨터 세계에서는 에러물이 된다. 자원을 공유하기 때문에 동기화 문제가 발생할 수 밖에 없다. 간단하게 프로세스와 스레드에 대해 알아봤다. 멀티스레드나 멀티 프로세스와 같은 단어도 아마 들어봤을텐데, 스레드와 프로세스에 대해 이해를 잘했다면, 사실 바로 이해되었을꺼라 생각한다. 멀티스레드는 말 그대로 프로세스 안에 두개 이상의 스레드가 있는 것을 이야기한다. 멀티 프로세스도 마찬가지이다. 멀티스레드나 멀티 프로세스에 대한 더 자세한 이야기는 다음 포스팅때 다루도록 하겠다.",
    "tags": "ETC",
    "url": "/etc/2021/11/17/python-process-thread.html"
  },{
    "title": "재귀함수의 장단점과 꼬리 재귀",
    "text": "요즘 파이썬 기초부터 다시 공부 중인데, 우연히 재귀함수 관련 영상을 보게 되었고 그에 대에 대한 내용을 정리해볼까 한다. 재귀함수 재귀함수는 자신을 재참조하는 함수를 뜻한다. 쉬운 예를 들면 인셉션이랑 비슷하다고 보면 될것 같다. ( 꿈에서 꿈을 꾸는 것과 같은 .. ) 재귀함수 장점 재귀함수를 사용하게 되면, 코드를 효율적으로 짤 수 있다. [1,2,3,[4,5],6,7] 위와 같은 배열을 나열한다고 했을 때, 반복문을 두번 쓰면 전부 꺼낼 수 있지만, 아래와 같은 상황이라면 조금 이야기가 달라진다. [1,2,3,[4,5],6,7,[8,9,10],[[11,12]],[13,[14,[15]]]...] 이럴때 재귀함수를 쓰게 되면 효율적으로 코드를 짤 수 있다. 새로운 배열이 나올때마다 다시 함수를 불러오는 형식으로 말이다. 그렇다면 재귀함수의 장점으로 코드를 효율적으로 짠다고 한다면, 단점은 무엇을까 ? 재귀함수 단점 단점으로는 성능 문제가 있다. 재귀함수는 호출할때마다 메모리의 스택이 쌓인다. 메소드 위에 또 메소드가 올라가는 식으로 말이다. 결국에 메모리 부족으로 인한 에러가 발생하게 된다. 또한 속도면에서도 재귀함수는 jump가 잦아서 반복문보다 느리다. 단점 극복하기 그렇다면 재귀함수의 단점을 어떻게 커버할 수 있을까? 재귀함수의 단점을 극복하기 위해 “꼬리 재귀 최적화(Tail call optimization)”이라는 기능이 나오기 시작했다. 꼬리재귀는 재귀함수를 선형 알고리즘으로 만들어서 실행하면 스택이 넘치는 일이 일어나지 않게 해준다. 꼬리재귀는 아래와 같이 사용할 수 있다. def test(x): // ... return test 위와 같이 자신 자체를 return 하면 꼬리재귀이다. def test(x): // ... return n*test 그러나 위와 같이 return 할때 다른 것(?)들이 섞여 리턴하게 되면 꼬리 재귀가 불가하니, 이부분에 유의하자.",
    "tags": "Python",
    "url": "/python/2021/09/29/python-recursive.html"
  },{
    "title": "프로그래머스 - 완주하지 못한 선수",
    "text": "링크 : 프로그래머스 - 완주하지 못한 선수 Answer: def solution(participant, completion): find_val, find_val2 = {},{} for j,i in zip(participant,completion): count = find_val.get(j,0) find_val[j] = count + 1 count = find_val2.get(i,0) find_val2[i] = count + 1 for key in find_val: try: if find_val[key] != find_val2[key]: answer = key except KeyError: answer = key return answer",
    "tags": "Algorithm",
    "url": "/algorithm/2021/09/07/algorithm-hash3.html"
  },{
    "title": "프로그래머스 - 위장",
    "text": "내 방식대로 프로그래머스 - 위장을 풀었다. 풀고나면 항상 다른 사람 풀이를 꼼꼼히 보는데, 다들 해시에 대해 이해를 잘하신거 같다.. 대단해… Answer: import collections def solution(clothes): answer = 1 for i in collections.Counter(dict(clothes).values()).values(): answer *= (i+1) answer = answer-1 return answer",
    "tags": "Algorithm",
    "url": "/algorithm/2021/08/14/algorithm-hash2.html"
  },{
    "title": "프로그래머스 - 전화번호 목록",
    "text": "프로그래머스 - 전화번호 목록을 풀었는데, 다들 어쩜 그렇게 잘푸는걸까 …. 내가 푼 답이 부끄러워지는 순간..★ Answer: import collections def solution(phone_book): book = sorted(collections.Counter(phone_book).keys()) for i in range(len(book)-1): if book[i+1].startswith(book[i]): answer = False break else: answer = True return answer",
    "tags": "Algorithm",
    "url": "/algorithm/2021/07/04/algorithm-hash.html"
  },{
    "title": "논문 리뷰 - TadGAN:Time Series Anomaly Detection Using Generative Adversarial Networks",
    "text": "Paper URL: https://arxiv.org/abs/2009.07769 해당 논문은 오픈소스로 공개되어 있다. 이 논문에서는 새로운 GAN 아키텍처인 TadGAN을 소개하며, 새롭게 Anomaly Scores를 계산하는 방법을 제안하였다. 이상감지 논문에서 이상감지하는 방법에는 여러가지가 있다고 한다. 시계열 패턴을 학습. 특정 부분을 이상 감지하도록 요청. 사용자와 관련된 것을 학습했는지 확인하도록 모델을 훈련. 정상 데이터에 대해 정상 기준선을 제공하여 모델과의 편차가 비이상적으로 큰 데이터 패턴을 학습 시계열 패턴의 유사성을 판단 K-Nearest Neighbor(KNN) 으로 데이터를 묶어서 정위하고 이웃수를 이상점수를 결정하는데 사용. 그러나 시간적 상관 관계를 포착하기 힘들기 때문에 사용못함. 예측 모델을 사용하여 예측하는 방법. 도메인 지식 필요. 그러나 감지된 모든 이상이 문제가 되는 것은 아니다. 그렇기 때문에 모델에 의해 식별된 이상이 문제인지에 대한 평가는 도메인 전문가 또는 최종 사용자의 몫이라는 것을 기억해두자. TadGAN TadGAN의 아키텍처는 아래와 같다. TadGAN에서는 함수 2가지만 기억하면 되는데, Reconstruction Errors와 Critic outputs이다. 1. Reconstruction Errors Reconstruction Errors $𝑅𝐸(𝑥)$는 3가지 유형의 함수가 있다. 학습 시킬때 전부다 사용하지는 않고 해당 함수 중 하나를 선택한다. Point-wise difference 실제 데이터 $x^t$ 에서 예측 값 $\\hat{x} ^t$ 를 빼서 차이를 계산. $st = |{x}^t - \\hat{x}^t |$ Area difference 특정 구간의 유사성을 판별하기 위해 특정 길이에서 영역 비교 $st = \\frac{1}{2*l}\\left | \\int_{t-1}^{t+1} {x}^t - \\hat{x}^t dx \\right |$ Dynamic time warping (DTW) 다른 2개의 시계열 패턴의 유사성을 비교할 때 사용하는 알고리즘. ${W}^* = DTW(X, \\hat{X}) =\\underset{W}{min} \\left | \\frac{1}{K}\\sqrt{\\sum_{k=1}^{K}}w_k \\right |$ 예를 들면, 두 개의 목소리를 비교해서 동일인 인지 확인하는 것. 동일인이 말하더라도 천천히 말할 때와 빨리 말할 때가 다름. 그래서 DTW에서는 거리가 가장 짧은 시점(유사도가 높은 시작점)을 찾는다. 2. Critic Outputs 𝐶𝑥(𝑥) Critic $c_x$는 0에서 1사이의 확률 값을 가지며 진짜일 확률을 나타내며, Critic Outputs에 KDE를 취한 다음 maximum을 평활화 (smoothed value) 를 시킨 값이 Critic scores이다. 여기서 KDE(kernel density estimation)에 대해 잠깐 설명하자면, 데이터를 바탕으로 하는 밀도 추정으로 데이터마다 커널을 생성한 히스토그램이다. 최종적으로 만들어진 함수는 위 $𝑅𝐸(𝑥)$ 와 Critic outputs $𝐶𝑥$를 합친 것이다. 그러나 $RE(x)$와 Critic outputs $C_x$를 직접적으로 함께 사용할 수 없다. 왜냐하면, $RE(x)$는 클수록 anomaly scores가 높은 거고, Critic outputs $C_x$는 낮을수록 anomaly scores가 높기 때문이다. 그래서 $RE(x)$와 Critic outputs $C_x$에 평균과 표준편차를 취했고, 각각의 z-scores에 넣고 계산 한 다음 정규화를 취했다. 이렇게 되면 z-scores가 높게 나오게 될 경우 anomaly scores가 높다고 할 수 있기 때문이다. 최종적으로 만들어진 스코어 함수는 아래와 같다. $a(x) = αZ_{RE}(x) + (1 − α)Z_{C_{x}}(x)$ 모델애 대한 성능은 아래 표와 같다. 아래는 함수에 따른 성능 표이다. 결론 GAN은 데이터 분포를 학습하기 때문에 False 알람을 많이 유발 시킬 수 있다. 오픈소스를 이용하여 실 데이터에 적용해보았을 때, 성능이 나쁘지는 않았지만, 생각보다 수정해야 할 부분이 많았다… ㅋㅋㅋㅋㅋ 좋았던 부분이 하나 있었는데, GAN모델이기 때문에 generator값을 보며, score나 error값이 왜 이렇게 떨어지는지에 대한 설명력이 조금 생겨서 일반인들에게 설명할 때 편하다는 점이 좋았다.",
    "tags": "Paper",
    "url": "/paper/2021/07/03/TadGAN.html"
  },{
    "title": "머신러닝에서 미적분이 필요한 이유 2",
    "text": "예전 포스팅에 미분이 필요한 이유에 대해서 정말 잠깐 이야기 했었는데, 블로그 정리하면서 보니 설명이 부족한거 같아 좀 더 정리할 겸 이렇게 글을 쓰게 되었다. 다항식 머신러닝에서 비선형 함수를 어떻게 추측해낼까? 우선 비선형 함수를 가장 간단하게 표현하면 아래와 같이 다항식으로 표현할 수 있다. $h_{\\theta }\\left ( x \\right )= \\theta _{0} + \\theta _{1}x + \\theta _{2}x^{2} + \\theta _{3}x^{3} + \\theta _{4}x^{4} + \\theta _{5}x^{5}$ 0차나 1차식을 제외한 모든 다항식은 전부 비선형으로 표현 할 수 있다. 이런 다항식들로 다항식 형태가 아닌 함수들도 근사치를 표현할 수 있다. 그리고 이때, 근사치의 n차 다항식을 찾아내는 작업을 미분방정식에서 Taylor expansion으로 배운다. Taylor expansion 그래서 오늘은 이 Taylor expansion에 대해 이야기 해볼까 한다. Taylor expansion를 이용한 비선형을 다항식화 한다고 하면 아래 식과 같이 근사 다항 함수로 표현하면 아래와 같이 표현 할 수 있다. $f(x+h) = f(x)+ {f}'(x)h + {f}''(x)\\frac{h^{2}}{2!}+{f}'''(x)\\frac{h^{3}}{3!} + ... $ 식을 정말 간단하게 설명하자면 x가 h값만큼 이동 했을때의 기울기와 값을 구하려고 한다. 그래서 f(x+h) 즉, x가 h만큼 이동 했을 때의 값 {f}’(x)h를 알 수 있게 된다. 오차항 그러나 위 그림의 오른쪽 그림과 같이 오차가 발생할 수 밖에 없다. 이론적으로 n이 무한대가 되면 오차는 0의 완벽한 근사치 다항식을 만들 수 있지만, 그렇게 되면 계산이 끝도 없기 때문에 보통은 n=2 , n=3정도로 타협하고 나머지 항목을 오차항이라고 부른다. 위 그래프처럼 다차항이 커짐에 따라 실제 데이터와의 fitting이 점점 증가한 모습을 볼 수 있으며, 만약 실제 데이터에 적용한다면 아래의 그림과 같다. 이렇게 미적분은 기본적으로 많이 사용되니 왜 사용하는지 잘 알아두는 것이 좋다.",
    "tags": "MachineLearning",
    "url": "/machinelearning/2021/02/11/non-linearity.html"
  },{
    "title": "샘플링과 리샘플링의 차이는 무엇일까?",
    "text": "샘플링과 리샘플링은 여러곳에 다양하게 사용되기 때문에 알아둬야 한다. 샘플링과 리샘플링에 대해 간단하게 알아보자. 샘플링(Sampling) 샘플링은 모집단에서 임의의 Sampling을 뽑아내는 것으로, 쉽게 말해 표본 추출을 의미한다. 샘플링을 하는 가장 큰 이유는 모집단 전체에 대한 조사는 사실상 불가능하기 때문에 Sampling을 이용하여 모집단에 대한 추론을 하게 되는 것이다. 주로 머신러닝과 통계분야에서 흔히 볼 수 있으며 신뢰구간, 오버피팅, 분산 등 밀접한 관련이 있다. 여기서 알아둬야 할 것은 표본은 모집단을 닮은 모집단 mirror 같은 존재이지만, 모집단 그 자체는 아니다. 그렇기 때문에 표본에는 반드시 모집단의 원래 패턴에서 놓치는 부분이 존재 할 수 밖에 없다. 리샘플링(Resampling) 리샘플링은 내가 가지고 있는 샘플에서 다시 샘플 부분집합을 뽑아서 통계량의 변동성을 확인하는 것을 이야기한다. 즉, 같은 샘플을 여러 번 사용해서 성능 측정하는 방식이다. 가장 많이 사용되는 방법이며 종류로는 k겹 교차 검증, 부트스트래핑이 있다. k-fold k-fold는 k-1개의 부분집합들을 훈련 세트로 사용하고 남어지 하나의 부분집합을 테스트 세트로 사용하는 것을 말한다. 이렇게 하면 아래의 그림처럼 k개의 훈련/테스트 세트가 만들어지고 k번의 훈련과 테스트를 거쳐 k개의 테스트 결괏값의 평균을 얻을 수 있다. 당연한 이야기이지만, 평가 결과와 안정성과 정확도는 k값에 따라 달라진다. 일반적으로 k는 10으로 두고 검증하며, 10외에 5, 20이란 값도 자주 사용된다. 그리고 샘플을 나누는 과정에서 생길 수 있는 차별을 최소화 하기 위해 일반적으로 p번을 랜덤하게 반복하여 나누어 진행한다. 즉 k-fold는 최종적으로 p번의 k겹 교차 검증을 실행한 값의 평균을 말한다. 주로 10차 10겹 교차검증을 사용한다. 부트스트래핑 부트스트래핑(bootstrapping)은 m개의 샘플이 있는 데이터 세트 D를 가정한다면, 우리는 샘플링을 통해 데이터 세트 D’를 만든다. 매번 D에서 샘플 하나를 꺼내 D’에 복사하여 넣는다. 그리고 다시 원래의 데이터 세트 D로 돌려보낸다. 이러한 과정을 m번 반복한 후 우리는 m개의 샘플이 들어있는 데이터 세트 D’를 얻는다. 이것이 부트스트래핑의 결과이다. 그러나 어떤 샘플은 아예 뽑히지 않을 수도 있는데, 수학적으로 계산하면 m번의 채집 과정 중 샘플이 한 번도 뽑히지 않을 확률은 (1 - 1)m/m이다. 극한값을 계산해본다면 아래와 같이 할 수 있다. $\\lim_{x \\to \\infty }\\left ( 1-\\frac{1}{m} \\right )^m = \\frac{1}{e} \\approx 0.368$ 즉, 부트스트래핑을 사용하면 데이터 세트 D중 36.8%의 샘플은 D’에 들어기지 못하기 때문에 D’를 훈련 데이터로 D-D’를 데이터 세트로 사용할 수 있다. 이러한 테스트를 Out-of-Bag이라고 한다. 아무튼 데이터를 어떻게 나누어 검증하냐에 따라 모델 성능이 천차만별이나 주의하자..",
    "tags": "MachineLearning",
    "url": "/machinelearning/2021/01/25/sampling-resampling.html"
  },{
    "title": "고유값(eigen value)과 고유벡터(eigen vector), 왜 중요한가?",
    "text": "고유값과 고유벡터에 대해서 많이 들어봤을 것이다. 그만큼 무척 중요하다고 모두들 이야기한다. 이번에 새롭게 공부하기 시작하면서 고유값과 고유벡터가 무엇인지, 왜 중요한지에 대해 정리해보려고 한다. 고유값, 고유벡터란? 대부분 벡터 x에 어떤 행렬 A를 곱하게 되면 벡터의 크기와 방향이 바뀌게 된다. 그러나 정방행렬에 정방행렬의 고유벡터를 곱하면 고유벡터의 방향이 바뀌지 않는다 더 간단히 말하면 벡터x가 행렬 A의 고유벡터라면 바뀌지 않는다. 월래 정방행렬 A는 임의의 벡터 x과의 행렬연산으로 x의 위치나 방향을 전환 시키는 역활을 하는데, 이때 어떤 특정 벡터들은 A에 곱해져도 위치나 방향이 바뀌어도 월래 자신과 평행하거나 동일한 값을 갖는다. 이러한 벡터들을 고유벡터라고 이야기 한다. 변화된 벡터 x의 크기를 나타내는 특정 상수는 람다(λ)로 표현하며 고유값이라고 이야기 한다. 왜 중요한가 ? 고유값과 고유벡터는 어떤 행렬의 가장 굉장히 중요한 정보를 담고 있다. 임의의 벡터를 어느 방향으로 변화시켰는지, 변환 과정에서 변화 없이 유지는 되는 부분은 어느 부분인지를 말한다. 어떠한 물체나 영상 등 이들은 무수히 많은 벡터들의 뭉치라고 이야기 할 수 있는데, 고유값과 고유벡터로 인해 영상이나 물체가 어떤식으로 변화되고 중심축은 어디인지에 관한 중요한 정보들을 파악할 수 있다. 응용분야에는 PCA, EigenFace 가 있다.",
    "tags": "MachineLearning",
    "url": "/machinelearning/2021/01/04/eigen-value-eigen-vecotor.html"
  },{
    "title": "2021 AI 개발자 로드맵",
    "text": "이제 3년차 머신러닝 개발자로 들어서고 있는 시점에 나는 내가 가지고 있는 역량이나 방향이 제대로 가고 있는지에 대해 또는 무엇을 더 공부해야 되는지에 대해 의문을 가졌다. 로드맵은 언제든 바뀔 수 있으므로 최신버전을 보고 싶다면 링크를 타고 확인하기를 바란다. Introduction Fundamentals Data Science Roadmap Machine Learning Roadmap Deep Learning Roadmap Data Engineer Roadmap Big Data Engineer Roadmap 정답이라는 보장은 못하지만 그래도 방향 잡는데에 도움이 되기를 바란다.",
    "tags": "ETC",
    "url": "/etc/2021/01/03/AI-Roadmap.html"
  },{
    "title": "3년차 머신러닝 개발자 나의 2020년 정리하며..",
    "text": "이제 2021년 된 새해 기념으로 나의 2020년 정리해볼까 한다. (일기처럼 줄줄 썻기 때문에 그냥 술술 읽고 가볍게 넘기면 된다) 2020년 3줄 요약 팀 이동 팀원들 절반이 퇴사하는 일 발생하며, 팀 이동됨 머신러닝 개발 1도 안함 급하게 REST API 만드는 일이 생겨 올해 절반을 API개발만 함. 대학원 합격 나 자신을 공부시키기 위해 스스로 대학원에 들어감. 누군가 2020년도는 나에게 어떤 해였는지 물어본다면, 정신적으로 정말 힘들었던 해라고 이야기 할 수 있다. 나는 3년동안 현재 회사를 다니며 정말 우리팀 사람들과 가족같이 지냈고 행복했었다. 내 첫 팀이였고, 내 첫 회사여서 더욱더 깊게 추억이 자리 잡았던 것 같다. 그러나 올해 우리팀의 절반정도가 퇴사했고 그로인해 정신적으로 힘들었다. 처음 겪는 이별(?)이라 더 충격을 받은게 아닌가 싶다. 그리고 나는 내가 그분들께 많이 의지하고 있는 줄 몰랐는데 생각보다 많이 의지했었나보다ㅋㅋㅋ… 아무튼 팀의 절반이 퇴사했고 이제 우리팀에 머신러닝을 하는 사람은 나밖에 없어 다른 팀으로 반강제로 팀이 이동되었고, 현재 적응중에 있다. 주변사람들의 퇴사로 인해 힘들 것보다 더 힘든 것이 있는데, 그건 바로 내 커리어 문제이다. 주변 사람들이 하나 둘 떠나자 문뜩 나도 퇴사와 이직에 대한 생각을 하게 되었다. 예전부터 이직에 대한 생각은 있었지만, 경력이 부족했기 때문에 이직은 하지 않았는데, 이제 3년차가 되니 슬슬 이직에 대한 생각이 들기 시작했다. 그러나 이직에 관한 생각을 할 때마다 숨이 턱 막혀온다. Aㅏ.. 이직에 대해 생각하다보니, 너무 준비할 것도 많고 뭐부터 해야댈지 모르겠다는 생각과 함께 3년차에 필요한 역량 에 대해 생각은 안할 수 없게 되더라. 그렇게 하반기부터 계속 스트레스를 받으며 지내왔던 것 같다. 특히 날 더 초조하게 만든 것이 하나 더 있는데, 그건 바로… 올해 한 일이 없다 라는 사실때문이다.. ㅋㅋㅋㅋ 너무나 부끄럽지만 사실이다. 올해(2020년) 머신러닝 관련해서 서비스하는 것이 좀 있어서 API를 만들게 되는데, 한달이면 끝날 줄 알았던 REST API개발은 약 5~6개월정도 공수를 잡아먹었고 결국 난 대략 40~50개의 REST API와 본의아니게 자바 컨트롤러 등.. 여러 업무를 받으며 한해를 보내게 되었다. 그렇게 논문도, 머신러닝 관련 코드도, 공부도 1도 안하다가 급 신경망 하나를 짜게 되는 일을 맡게 되었는데 내가 이렇게까지 못했던가 ? 싶었을 정도로 아무 생각 안나더라. 그렇게 크게 현타가 오기 시작했다. 흐릅.... 9월말쯤 온전히 내 룰인, 머신러닝 관련해서 데이터 분석이나 알고리즘을 겨우 다시 짤 수 있게 되었지만 일을 제대로 하지도 못하는 내가 너무 싫더라. 그렇게 공부를 처음부터 다시하자고 마음 먹게 되는 계기가 되었다. 그와 동시에 이직을 위한 채용공고를 많이 봤는데, 생각보다 요구하는 스킬이 많은 것을 보고 막막했다. 내가 할수 있는 언어는 아직 R, Python 뿐인데, 다들 기본 C와 JAVA를 원하는 곳이 너무 많았고, 특히나 하둡이나 스파크를 많이 요구하는데………. 우리 회사는 쓰지 않는단말이다ㅠㅠㅠㅠㅠㅠ 안써봐서 ㅠㅠ 모른다고ㅠㅠ 주변 개발자들이 그러다라. N년차인데, 이런것도 모르냐라는 소리가 가장 무섭다고… 나도 무섭다. 아무튼 아직 한번도 해보지 않은 이직에 대해서, 미래에 대해서 생각하면 할수록 무섭고 내가 잘 할 수 있을지에 대한 생각으로 2020년으로 보낸 것 같다. 물론! 나쁜 일만 있는 것은 아니고 좋은 일도 하나 있는데, 대학원에 합격했다는 사실… 합격통지서 아직 갈지 말지에 대해 신중히 생각하고 있는 단계라 고민중이지만, 2020년 가장 좋은 일중 하나 아닐까한다. 21년도는 정말 알차게 보내고 싶다는 생각이 너무 강하다. 대학원은 무조건 가겠지만 서강대가 아닌 다른 대학원을 선택해 갈 수도 있다. 역시 학비 생각을 안할 수 없기 때문.. 아무튼 코로나도 물러가고 퇴사와 이직 둘다 잘 됬으면 좋겠다.",
    "tags": "ETC",
    "url": "/etc/2021/01/02/log-2020.html"
  },{
    "title": "파이썬 컨벤션에 대해 알아보자",
    "text": "회사에서 일하다보면 간혹 파이참에서 내 코드에 노란줄 표시하는 것을 볼 수 있다. 이 노란줄은 “컨벤션 좀 지켜주세요” 라는 뜻이다. Go 같은 경우는 무조건 지켜야 실행이 가능하지만, 다른 언어들은 지키지 않아도 문제 없기 때문에 사실 안지키는 사람이 많다. 나도 지키지 않는 경우가 많은데, 이제는 깔끔하게 코드를 짜고 싶어 이렇게 한번 되돌아보는 시간을 가져볼까 한다. 사실 컨벤션은 필수가 아니고 “코드를 이렇게 작성하는게 좋을 것 같습니다.” 하는 스타일 가이드이다. 스타일 가이드가 중요한 이유는 혼자 보는 코드가 아니기 때문이다. 다른 사람들과 함께 협업해서 일을 하는 것이기 때문에 어느정도 코드에 대한 일관성이 있어야 보기 편하다. PEP8 – tyle Guide for Python Code 파이썬 창시자가 만든 파이썬 스타일 가이드이다. 중요하기 때문에 무조건 읽어봐야 한다. 하지만 난 읽지 않았기 때문에 같이 함께 보도록 하자. 1 . 들여쓰기 들여쓰기는 공백 4개를 쓰도록 하며, 첫번째 줄에는 인수가 오면 안된다. 또한 fun name과 인수를 구별을 명확하게 하기 위해 들여쓰기(공백 4개)를 한번 더 해야 한다. # 올바른 예 # Aligned with opening delimiter. foo = long_function_name(var_one, var_two, var_three, var_four) # Add 4 spaces (an extra level of indentation) to distinguish arguments from the rest. def long_function_name( var_one, var_two, var_three, var_four): print(var_one) # Hanging indents should add a level. foo = long_function_name( var_one, var_two, var_three, var_four) # 잘못된 예 # Arguments on first line forbidden when not using vertical alignment. foo = long_function_name(var_one, var_two, var_three, var_four) # Further indentation required as indentation is not distinguishable. def long_function_name( var_one, var_two, var_three, var_four): print(var_one) 또한 들여쓰기 할 때, 탭은 탭으로, 스페이스는 스페이스로 명확하게 일관성을 유지해야 한다. Python3 같은 경우는 혼합하는 것을 허용하지 않으므로 들여쓰기 하는 방법은 통일해야 한다. 통상적으로 탭을 사용한다. 2. Blank Lines(빈줄) 2개의 빈줄로 함수와 클래스 정의를 구분한다. 클래스 내의 함수는 하나의 빈줄로 구분한다. 3. Import (임포트) import는 반드시 행을 분리해서 선언한다. 또한 모듈의 이름은 소문자로 한다. # 올바른 예 import os import sys # 잘못된 예 import sys, os # 아래의 경우는 허용 from subprocess import Popen, PIPE import는 절대경로를 이용한 절대 import가 권장된다. import mypkg.sibling from mypkg import sibling from mypkg.sibling import example 그리고 import한 클래스의 이름이 지역 변수와 충돌나면 아래처럼 사용해도 된다. from myclass import MyClass from foo.bar.yourclass import YourClass # 충돌 나면 이렇게 사용 import myclass import foo.bar.yourclass 4. String Quotes(따움표) 파이썬에서는 문자열을 표현할 때, 큰 따옴표와 작은 따옴표 구분 없이 편하게 사용하면 된다. 5. Pet Peeves 무의미한 공백은 자제하자. # 올바른 예 spam(ham[1], {eggs: 2}) # 잘못된 예 spam( ham[ 1 ], { eggs: 2 } ) 쉼표와 닫는 괄호도 아래와 같다. # 올바른 예 foo = (0,) # 잘못된 예 bar = (0, ) 6. 주석 주석은 영어로 쓰도록 하자. 또한 마침표 뒤에는 두 칸의 공백을 주고, 반드시 문장 형태여야 한다. 인라인 주석의 경우( 한 구문과 같은 줄에 쓰는 주석 ) 구문과 2칸 이상의 공백을 두고 # 기호와는 한칸의 공백을 주고 사용한다. 그러나 인라인 주석의 경우 잘 사용되지는 않는다. 7. Docstrings public 모듈, 함수, 클래스, 함수에 대해서는 Docstrings을 사용하도록 하자. 무슨 역활을 하는지에 대해 작성하는 것이 좋으며 def문장 바로 아래줄에 작성하자. # 한줄 Docstrings def kos_root(): \"\"\"Return the pathname of the KOS root directory.\"\"\" global _kos_root if _kos_root: return _kos_root ... # 여러 줄 Docstrings def complex(real=0.0, imag=0.0): \"\"\"Form a complex number. Keyword arguments: real -- the real part (default 0.0) imag -- the imaginary part (default 0.0) \"\"\" if imag == 0.0 and real == 0.0: return complex_zero ... 8. Descriptive Naming Styles 변수명에서 _(밑줄)은 위치에 따라 여러 의미가 있다. _single_leading_underscore : 내부에서 사용하는 변수이다. 앞에 언더스코어를 사용한 객체는 import 되지 않는다. single_trailing_underscore_ : 파이썬 기본 키워드와 충돌을 피하려고 사용한다. __double_leading_underscore : 클래스 속성으로 사용되면 그 이름을 변경한다. (ex. FooBar에 정의된 __boo는 _FooBar__boo로 바뀝니다.) double_leading_and_trailing_underscore: magic 객체’, 사용불가, 미리 정의되어 있는 이름만 사용 가능하다. 9. Names 소문자 l, 대문자 O, 대문자 I는 변수명으로 사용하지 말자. 숫자 1과 0은 구별하기 힘들기 때문이다. 클래스 이름은 일반적으로 capWords(CamelCase)규칙을 사용한다. 함수명은 소문자를 사용하자. 서브 클래스의 경우 이름의 충돌을 막기 위해 밑줄 2개를 붙이자.",
    "tags": "Python",
    "url": "/python/2020/12/12/python-convention.html"
  },{
    "title": "P-value란? (with Python)",
    "text": "P-value에 대해 알기전에 귀무가설, 대립가설에 대해 알아야한다. 귀무가설은 표본 데이터를 사용하여 모집단에 대한 주장(귀무가설)의 타당성을 검정한다. 간단하게 말하면 어떠한 두 가지 현상 사이에 관계가 없다라고 이야기하는 일반적인 가설이다. 대립가설은 귀무가설과 반대로 내가 믿는 것, 실험이나 연구를 통해 입증하려고 하는 주장을 말한다. 이제 귀무가설, 대립가설에 대해 알았으니 P-value에 대해 알아보자. What is P-value? 아래와 같은 정규 분포가 있다고 하자. P-value는 귀무 가설(null hypothesis)이 맞다는 전제 하에, 표본에서 실제로 관측된 통계치와 ‘같거나 더 극단적인’ 통계치가 관측될 확률을 말한다. 예시로 아래의 그림처럼 빨간색 점 오른쪽에 있는 값(극단)을 가져올 확률이라고 생각하면 편하다. 귀무가설, 대립가설과 P-value와의 관계 내가 여기서 빨간점보다 더 오른쪽 값을 가져오게 되었다고 해보자. 모집단 평균이 100이라고 했을때, 내가 가져온 데이터의 평균은 100보다 클 것이다. 아래의 그림처럼 다른 정규분포가 있다면 다른 분포에 속할 가능성이 높다고 이야기 할 수 있다. 조금 더 쉽게 예를 들어 보자. 피자 배달 시간의 평균이 30분 이내이다. 위와 같은 귀무가설이 참이라는 가정을 해보자. 총 1000개의 데이터에서 내가 100개의 데이터를 샘플링했을 때, 이론적으로 나올 수 있는 평균의 분포에서 지금 가지고 있는 값이 30보다 큰 값이 나올 확률! 이것을 P-value이다. P-value가 높다면 귀무가설을 잘 따르고 있다 P-value가 작다면 귀무가설을 잘 따르고 있지 않다. P-value에 대한 잘못된 해석 가끔 P-value에 대한 해석을 잘못하는 사람들이 있다. 일반적으로 많이 하는 실수에 대한 일부를 이야기 해볼까한다. P-value는 결과가 통계적으로 유의할 때, 전체 의사 결정과정의 작은 부분에만 도움이 되는 것이지, 실질적인 의사결정을 내려서는 안된다. 또한 절대적인 지표가 아님을 명심해야 한다. 특히! P-value와 대립가설은 별로 관련이 없다. P-value는 순전히 귀무가설이 맞다라는 전제하에 나온 P-value이기 때문이다. 또한 귀무가설이 참일 확률이 아님이 기억하자. P-value는 두 집단간에 차이가 있는지 없는지 정도로만 알려주기 때문에 주의 해야한다. Python code 보스턴 데이터를 사용해서 P-value를 구현해볼까한다. import statsmodels.formula.api as smf from sklearn.datasets import load_boston boston = load_boston() boston_data = pd.DataFrame(boston.data, columns=boston.feature_names) boston_data['MEDV'] = boston.target res = smf.ols(formula='MEDV ~CRIM + ZN + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT', data=boston_data).fit() print(res.summary()) statsmodels 라이브러리를 이용해서 쉽게 P-value를 구할 수 있다. 이 테이블은 독립 변수에 대한 모든 통계를 표시한다. 일단 중요한 건 아니니 전부 넘기고 P-value만 보도록 하자. 위에서도 귀무가설과 대립가설에 대해 이야기 했지만, 회귀모델에서 P-value 값이 무엇을 의미하는지를 정리하면 아래와 같이 정리할 수 있다. 귀무가설 : 독립변수 종속변수에 대해 유의한 영향을 미치지 않는다. 대립가설 : 독립변수는 종속변수에 중요한 영향을 미친다. 통상적으로 0.05이하인 경우 통계적으로 유의하다라고 판단한다. 그런데, 표에서 AGE 값이 0.95다. 이뜻은 AGE 변수는 Target에 영향을 주는 변수라고 볼 수 없다. 여기서 해당 변수를 제거하고 다시 OLS모델을 만들면 R-squared가 올라가는 것을 알 수 있는데, 더 성능이 좋은 모델을 만들어짐을 볼 수 있다. 이렇게 P-value의 도움으로 더 적은 변수로 간단하게 모델을 만들 수 있다.",
    "tags": "MachineLearning",
    "url": "/machinelearning/2020/12/04/statistics-p-value.html"
  },{
    "title": "도커로 오라클을 사용해보자",
    "text": "어쩌다보니, 집에서 SQL쿼리문을 간단하게 짜야하는 일이 생겨버렸다. 그러나 집에서 오라클을 설치해서 사용해야하는데, 그러기에는 너무 번거롭다.. 어떻게 할까? 생각하다가 왠지 도커로 오라클 설치해서 사용할 수 있을 것 같아 시도해봤는데, 역시나 도커!기대를 저버리지 않는다. Docker Oracle Docker Hub에 가면 Oracle 11g 이미지를 다운받아 사용할 수 있다. docker pull jaspeen/oracle-11g 이미지를 다운 받았으면, 실행을 해보자. docker run --name oracle11g -d -p 1521:1521 jaspeen/oracle-xe-11g 간단하게 SQL문을 쓰는게 목적이라면, 아래와 같이 적자. docker exec -it oracle11g sqlplus 참고로 user name과 password는 system / oracle 이다. 그런데, 이렇게 쓸수는 없기에… SQL Developer를 다운받아 사용하였다. 접속 정보는 위와 같다. 이제 집에서도 편하게 SQL공부 할 수 있다.",
    "tags": "Docker",
    "url": "/docker/2020/12/04/docker-sql-install.html"
  },{
    "title": "파이썬 REST API을 만들어보자",
    "text": "회사에서 머신러닝 개발자로 일하고 있지만, 1년 내내 머신러닝 모델을 만들거나 논문만 읽거나 그러지는 않는다. 나름대로 Python API 나, 자바 컨트롤러 정도는 만들면서 다른 업무도 도와주거나, 데이터 분석도 하는 편이다. 그중에서 입사 초에 Python API 하나를 못 만들어서 정말 고생했던 것을 이번 포스팅에서 풀어볼까 한다. 이번 포스팅에서는 Python API를 만들면서 테스트 하는 것을 중점으로 포스팅 할 것이다. REST API 우선 API를 만들기전에 API로 만들어야 할 함수를 간단하게 두개 만들것이다. 나는 Study.py 라는 파일을 생성하여 아래와 같은 함수를 만들었다. # Study.py def add(a,b): return a+b def same(a,b): if a == b: return True else: return False add함수는 덧셈 함수이고 , same함수는 a와 b를 입력 받았을 때, True 인지 False 인지를 반환해주는 함수이다. 이제 REST API 함수를 만들텐데, 일단 간단하게 app.py 파일을 생성하고 TEST 가능한지 TEST API를 먼저 만들어보겠다. # app.py from flask import Flask app = Flask(__name__) @app.route('/') def greeting(): return \"This is Test API ! \" API 띄울때는 app.py 파일이 있는 곳에서 아래와 같이 두 줄만 치만 된다. export FLASK_APP=\"app.py\" # 리눅스 일때 &lt;- set FLASK_APP=app.py # 윈도우 일때 &lt;- flask run --host=0.0.0.0 테스트용으로 만드는 것이기 때문에 flask run –host를 0.0.0.0이라고 했지만, 실제 현업이나 중요한 곳에서 사용해야 한다면, 실제 IP와 포트로 바꿔서 사용해야한다. 이제 “http://127.0.0.1:5000/” 주소로 들어가서 확인 하면 This is Test API ! 라는 문구를 볼 수 있다. 여기서 아까 만들었던 add 함수와 same 함수를 API로 만들어보자. from flask import Flask, request from flask_api import status import json from Study import * app = Flask(__name__) @app.route('/') def greeting(): return \"This is Test API ! \" @app.route('/add', methods=['POST']) def get_add(): if request.method == 'POST': a = request.json[\"a\"] b = request.json[\"b\"] c = add(a,b) result = json.dumps(c) return result, status.HTTP_200_OK, {\"Content-Type\": \"application/json; charset=utf-8\", \"Access-Control-Allow-Origin\": \"*\"} @app.route('/same', methods=['POST']) def get_same(): if request.method == 'POST': a = request.json[\"a\"] b = request.json[\"b\"] c = same(a,b) result = json.dumps(c) return result, status.HTTP_200_OK, {\"Content-Type\": \"application/json; charset=utf-8\", \"Access-Control-Allow-Origin\": \"*\"} 조금 더 알아보기 편하게 그림으로 준비했다. 사실 Flask로 구현되어 있기 때문에 구조가 간단하다. @app.route에 주소를 적고 아래 함수를 적으면 끝이다. 근데… add 함수에 넣어야 할 매개변수는 어디있을까? Study.py에 있는 add 함수에는 a,b라는 매개변수를 받아 서로 더하고 나서 리턴하는 구조인데, get_add() 함수는 그렇지 않다. 왜 그런걸까? 그 이유는 함수 안에서 데이터를 처리하기 때문에 함수 생성시, 직접 매개변수를 작성하지 않기 때문이다. 이제 생성된 API를 테스트하려고 하는데, 어떻게 해야할까? 당연히 아까와 마찬가지로 http://127.0.0.1:5000/add 를 치고 들어가면 “Method Not Allowed The method is not allowed for the requested URL.” 를 볼 수 있을것이다. 전달할 데이터도 뭣도 없으니 저런 문구가 뜨는 것이 당연하다. 여기서부터 실제 만든 API를 테스트하고 확인하려면 간단하게 Postman이라는 툴을 이용하면 된다. 설치도 사용법도 무척이나 간단하다. 위처럼 똑같이 세팅해주면 postman으로 API결과를 확인 할 수 있다. add와 마찬가지로 same도 테스트 하면 결과를 확인 할 수있다.",
    "tags": "Python",
    "url": "/python/2020/10/19/python-rest-api.html"
  },{
    "title": "스마트폰 만족감에 영향을 주는 것이 무엇일까?(다중회귀)",
    "text": "단순 선형 회귀분석의 목적이 하나의 독립변수만을 가지고 종속 변수를 예측하기 위한 회귀 모형을 만들기 위한 것이었다면, 다중 회귀 분석의 목적은 여러 개의 독립변수(x)들을 가지고 종속변수(y)를 예측하기 위한 회귀 모형을 만드는 것이다. 예를 들면 집값에 영향을 미치는 요소가 단순히 평수만 있는 것이 아니다. 요소 중에는 평수, 교통, 집방향, 층수, 풍수지리, 범죄율, 학군 등등이 있다. 스마트폰 만족감에 영향을 주는 것 이번에 스마트폰 만족감에 영향을 주는 것이라는 주제로 다중 회귀 분석을 하려고 합니다. 위에서 말했듯이 집값에 영향을 주는 요소가 여러개인데, 마찬가지로 스마트폰 만족감을 주는 것도 여러가지 이기 때문에 다중 회귀 분석을 하는 것이다. 데이터를 불러와서 확인하면 아래와 같다. reg(y=multi_hg$만족감, x=multi_hg[1:3]) 컬럼은 아래와 같이 외관, 편의성, 유용성으로 나눌 수 있다. 여기서 interceot는 절편이다. 스마트폰 만족감에 가장 큰 영향을 미치는 것은 가장 기울기가 큰 것을 구하면 알 수 있다. 그렇기 때문에 위의 결과를 보면 기울기가 가장 높은 것은 외관이라는 것을 알 수 있다. 사실 더 간단한 방법은 아래와 같이 코드를 작성하여 확인 할 수 있다. attach(multi_hg) lm(만족감~외관+편의성+유용성, data=multi_hg) 위 코드를 작성하면, 아래와 같이 결과가 똑같이 나온다.",
    "tags": "MachineLearning",
    "url": "/machinelearning/2020/09/17/r-linear-regression3.html"
  },{
    "title": "표준화(Standardization)와 정규화(Normalization)의 차이",
    "text": "예전에 부장님이 나에게 퀴즈로 내신 문제가 생각나서 글을 올리게 되었다. 어느날 문득 “표준화와 정규화의 차이점에 대해 말해봐” 라는 질문을 받게 되었는데, 그 당시 정규화에 대해서만 애기하고 표준화에 대해서는 대충 대답했던 것이 떠올랐다. 공부도 할 겸 적어서 우선 올려본다. 표준화 (Standardization) 평균을 기준으로 얼마나 떨어져 있는지를 나타낼 때 사용하는 값으로 2개 이상의 대상의 단위가 다를 때 대상 데이터를 같은 기준으로 보게 하기 위해서 사용한다. 정규화(Normalization) 정규화는 전체 구간을 0 ~ 100으로 설정하여 데이터를 관찰하는 방법으로 데이터에서 특정 데이터가 가지는 위치를 볼 때 사용한다.",
    "tags": "MachineLearning",
    "url": "/machinelearning/2020/09/14/ml-standardization-normalization.html"
  },{
    "title": "강화학습으로 tic tac toe 학습 시키기전 필요 요소",
    "text": "강화학습을 이용해서 tic tac toe을 만들어볼까한다. 강화는 동물이 시행착오를 통해 학습하는 방법 중 하나로 강화라는 개념을 처음 제시한 스키너라는 행동 심리학자이다. 강화라는 것은 동물이 이전에 배우지 않았지만 직접 시도하면서 행동과 그 결과를 나타나는 좋은 보상 사이에 상관관계를 학습하는 것이다. 핵심은 바로 보상을 얻게해주는 행동의 빈도의 증가이다. 다른말로 애기하면 보상을 얻게 하는 행동을 점점 더 많이 하도록 학습하는 것을 말한다. 지도학습과 강화학습의 차이 강화학습으로 tic tac toe를 만들기 전에 우선 지도학습과 강화학습의 차이 정도는 알고 가자. 사실 위에서 TMI로 많이 이야기 했지만.. 어쨋든 한번은 짚고 넘어가야 댈듯하다. 지도학습은 직접적인 정답을 통해 오차를 계산해서 학습하는 것이고, 강화학습은 자신의 행동의 결과로 나타나는 보상을 통해 학습하는 것을 이야기 한다. 서로 다른 방식으로 학습을 하기 때문에 이 점은 꼭 기억하자. 강화학습에 필요한 요소 강화학습에 필요한 요소를 정리하려고 한다. TMI로 순차적으로 행동을 계속 결정해야하는 문제를 수학적으로 표현한 것을 MDP( Markov Dicision Process )라고 한다. 아무튼 강화학습에 필요한 요소는 다음과 같다. 상태 행동 보상함수 상태변환확률 감가율 정말 필요한 기본 요소들만 적었기 때문에 다른 강화학습 모델을 만든다고 하면 저 필요 요소보다 더 필요할 것이다. 틱텍토 코드의 중요한 특징 2가지가 있는데, 아래와 같다. 각 수에 대한 가중치를 점점 갱신해 나간다. 수를 둘 때마다 랜덤수를 던진다. (10번중 1번으로) 위의 1번의 경우 금방 납득을 할 수 있겠지만, 2번의 경우는 이해가 안될 수 있다. 랜덤수를 던지는 이유는 랜덤 수가 없다면 학습된 데이터로만 계속 둘 것이다. 그렇다면 더 좋은 수가 있는데, 더 좋은 수를 발견 못하고 계속 같은 게임만 하게 될 수도 있다. 그렇기 때문에 중간중간 랜덤의 수를 던져야 한다. 다음 포스팅 때는 필요 요소를 기반으로 기본 코드를 짜보겠다.",
    "tags": "MachineLearning",
    "url": "/machinelearning/2020/08/20/tic-tac-toe.html"
  },{
    "title": "정규분포에 대해 알아보자",
    "text": "정규분포는 많은 경영, 경제, 사회현상, 자연 현상들이 정규분포의 형태를 띄고 있다. 예를 들면 한국 성인 남자의 평균 키가 173cm라고 하면 키가 173cm에서 크게 벗어나지 않는 사람들이 많고, 상대적으로 이 수치에서 벗어난 150cm, 190cm인 사람들은 별로 없다는 의미이다. 정규분포 정규분포는 평균에서 멀어질수록 데이터 분포가 감소하여 종모양의 형태를 띈다. 그리고 정규분포 그래프를 3등분하면, 평균 근처의 비율이 68%정도 된다. ## -3 ~3까지의 데이터를 200개 만든다. x&lt;-seq(-3, 3, length=200) plot(x, dnorm(x, mean=0, sd=1), type='l', main=\"정규분포 그래프\") dnorm은 y축이며, mean 평균값, sd는 표준편차이다. type = ‘l’은 직선을 뜻한다. 정규분포 그래프를 보면 데이터가 오른쪽으로 비대칭인지, 왼쪽으로 비대칭인지를 확인할 수 있다. 여기서 좌우의 기울어짐의 정도를 뜻하는 용어는 왜도(skewness)라고 하며 (x&gt;0이면, 오른쪽으로 꼬리가 길며, x&lt;0이면 왼쪽으로 꼬리가 길다.) 위아래 뾰족한 정도는 첨도(kurtosis)(3에 가까울수록 정규분포, 3보다 작을수록 완만하다)라고 한다. 아래의 데이터를 넣고 어느쪽으로 편향되어 있는지 정규분포를 그려서 확인해볼까한다. x175 &lt;-c(rep(1, 4), rep(2,6), rep(3,4), rep(4,4), rep(5,3), rep(6, 2), rep(7,1), rep(8,1)) # or x175&lt;-c(rep(c(1, 2, 3, 4, 5, 6, 7, 8), c(4, 6, 4, 4, 3, 2, 1, 1))) # 그래프 그리기 plot(x175, dnorm(x175, mean=mean(x175), sd=sd(x175)), ylab='y', type='l', main=\"그래프\") 여기서 왜도와 첨도를 구하면 아래와 같이 구할 수 있다. # 왜도(skewness) 구하기 install.packages(\"fBasics\") library(fBasics) skewness(x175) # 첨도(kurtosis) 구하기 kurtosis(x175)",
    "tags": "R",
    "url": "/r/2020/08/09/r-normal-distribution.html"
  },{
    "title": "Go에서 Pointer 사용하기",
    "text": "GO는 설치되어 있지 않지만 실행을 해보고 싶다면, 여기에서 간단하게 돌려 볼 수 있다. Go Pointers Go에는 포인터가 있지만 포인터 연산은 불가능하다. 구조체 변수는 구조체 포인터를 이용해 접근 할 수 있다. 포인터를 이용하는 간접적인 접근은 실제 구조체에도 영향을 미친다. package main import ( \"fmt\" ) type Vertex struct { x int y int } func main() { p := Vertex{1, 2} q := &amp;p q.x = 1e9 fmt.Println(p) } 결과 :",
    "tags": "Go",
    "url": "/go/2020/07/30/go-pointers.html"
  },{
    "title": "백준 10951번 while문으로 A+B를 출력하자(EOF)",
    "text": "이번 문제는 간단하게 예외처리만 하면 되는 문제인데, 문제에 EOF라고 명시가 되어 있지 않아서 조금 해맸다. 나중에 EOF라는 것를 밖에 링크 누를때 확인하고 나서야 풀 수 있었다…ㅠㅠ 백준 10951번 문제는 아래와 같다. Answer: while(True): try : a, b = list(map(int, input().split(' '))) print(a + b) except EOFError: break",
    "tags": "Algorithm",
    "url": "/algorithm/2020/07/19/algorithm-baekjoon7.html"
  },{
    "title": "백준 10171번 고양이를 출력하자",
    "text": "백준 10171번의 고양이를 출력하자 문제는 너무나 귀여운 것 같다.. 백준 10171번 문제는 아래와 같다. Answer: print(\"\\ /\\\\\\n ) ( ')\\n( / )\\n \\(__)|\")",
    "tags": "Algorithm",
    "url": "/algorithm/2020/07/16/algorithm-baekjoon6.html"
  },{
    "title": "백준 2588번, 빈 칸에 들어갈 수는? (곱셈)",
    "text": "BAEKJOON 2588번 문제는 빈 칸에 들어갈 수를 출력하는 문제이다. 사실 혼자서 어렵게 풀고 있었다가 나중에서야 “아..” 하고 나의 바보스러움에 감탄하며 문제를 풀었다. 백준 2588번 문제는 아래와 같다. Answer: a = int(input()) b = list(input()) b.reverse() for i in range(3): num = str(a*int(b[i])) print(num) print(a*int(b[2]+b[1]+b[0]))",
    "tags": "Algorithm",
    "url": "/algorithm/2020/07/15/algorithm-baekjoon5.html"
  },{
    "title": "백준 10869번, 사칙연산",
    "text": "BAEKJOON 10869번 문제를 풀어보았다. 간단하게 print에 사칙연산 출력만 하면 되기 때문에 금방 할 수 있다. 백준 10869번 문제는 아래와 같다. Answer: a,b = map(int , input().split(' ')) print(a+b) print(a-b) print(a*b) print(int(a/b)) print(a%b)",
    "tags": "Algorithm",
    "url": "/algorithm/2020/07/11/algorithm-baekjoon4.html"
  },{
    "title": "백준 10871번, x보다 작은 수 출력하기",
    "text": "BAEKJOON 10871번 문제를 풀어보았다. for문만 사용할 줄 안다면, 푸는데는 2분도 채 걸리지 않을것이다. 백준 10871번 문제는 아래와 같다. Answer: a = list(map(int,input().split(' '))) b = list(map(int,input().split(' '))) num = [] for i in b: if a[1] &gt; i: num.append(i) else: pass print(*num, sep = \" \")",
    "tags": "Algorithm",
    "url": "/algorithm/2020/07/03/algorithm-baekjoon3.html"
  },{
    "title": "백준 1330번, 두 수 비교하기(if)",
    "text": "간단하게 두 수 비교하는 백준 알고리즘을 풀어보았다. 사실 if문 배우면서 누구나 한번쯤은 해본거라 사실 고민할거리도 되지 않았다 ….. 백준 1330번 문제는 아래와 같이 간단하게 두 수를 비교하는 것이다. Answer: a,b = map(int, input().split()) if a &gt; b : print(\"&gt;\") elif a &lt; b : print(\"&lt;\") else: print(\"==\") 그런데 문제 다 풀고 나서 숏코딩 코드를 보았는데, 정말 많이 놀랬다. 숏코딩: a,b=map(int,input().split()) print(['&gt;&lt;'[a&lt;b],'=='][a==b]) 내가 진짜 공부를 안하고 놀기만 했구나라는 사실에 대해 알게 되었다.",
    "tags": "Algorithm",
    "url": "/algorithm/2020/07/03/algorithm-baekjoon2.html"
  },{
    "title": "백준 2557번, Hello World!를 출력하자",
    "text": "아마 2년만에 알고리즘에 손대는 것 같다. 취업하고 나서는 알고리즘에 손도 안대서 그런지 오랜만에 문제 풀려고 하니까 안풀어지더라. 그래서 백준 알고리즘을 하나씩 풀어볼까 한다. 백준 2557번 사실 문제가 너무 간단해서 포스팅 안하려고 했다. 근데 처음 시도했을때, 틀렸습니다와 컴파일 에러 뜨길래 보니까 언어 선택을 잘못하거나”Hello World!”에서 ! 안붙여서 에러가 났다… 문제를 잘 읽고 풀어야 하는 알고리즘에서 그냥 Hello World 출력하는거네~ 하고 문제 제대로 안보고 그냥 코드를 돌린 나의 한심함을 표하며 글을 올린다. Answer: print(\"Hello World!\")",
    "tags": "Algorithm",
    "url": "/algorithm/2020/07/03/algorithm-baekjoon1.html"
  },{
    "title": "이름없는 한줄짜리 함수(Lambda)",
    "text": "함수나, 클래스를 만들때 보통은 함수에 이름을 붙여서 사용한다. 그러나 함수를 재사용하지 않으려고 하는 경우에 Lambda 함수를 사용한다. Lambda 람다식은 아래와 같다. 그래서 위의 식을 이용해서 간단하게 덧셈 함수를 만들면 이렇게 할 수 있다. add = lambda x, y: x+y #매개변수 : #실행문 ret = add(1, 3) print(ret) # 4가 출력됨 람다는 간단하지만, 처음 사용할땐 어려울 수 있다. funcs = [lambda x: x+'.pptx', lambda x: x+'.docx'] ret1 = funcs[0]('Intro') ret2 = funcs[1]('Report') print(ret1) # intro.pptx가 출력됨 print(ret2) # Report.docx가 출력됨 이번엔 람다식을 이용해 key를 부르면 아래와 같이 간단하게 작성할 수 있다. names = {'Mary':10999, 'Sams':2111, 'Aimy':9778, 'Tom':20245, 'Michale':27115, 'Bob':5887, 'Kelly':7855} ret3 = sorted(names.items(), key=lambda x: x[0]) #0번째 요소(key)를 불러오는 함수식 print(ret3)",
    "tags": "Python",
    "url": "/python/2020/07/02/python-lambad.html"
  },{
    "title": "R에서 병렬처리하는 방법",
    "text": "우선은 병렬처리를 왜하는걸까? 병렬처리가 중요한가 ?와 함께 하둡에 대해 살짝 이야기 해볼까한다. 하둡(hadoop)이라고 아마 많이 들어봤을꺼다. 구글에서 쌓여지는 수많은 빅데이터들을 구글에서도 처음에는 RDBMS(오라클)에 입력하고 데이터를 저장하고 처리하려고 시도를 했지만, 데이터가 너무 많아서 실패하고 자체적으로 빅데이터를 저장할 기술을 개발했다. 그리고 대외적으로 논문 하나를 발표했는데, 그 논문을 더그커팅이라는 사람이 읽고 자바로 구현한게 하둡(hadoop)이다. 여기서 TMI는 이름을 뭘로 할까 고민하다가 더그커팅의 애기가 노란 코키리 장난감을 가지고 그냥 Hadoop이라고 하는걸 듣고 하둡이라고 이름을 지었다고 한다. 아무튼 병렬처리는 여러대의 노드를 묶어서 마치 하나의 서버처럼 보이게하고 여러 노드의 자원을 이용해서 데이터를 처리하기 때문에 처리하는 속도가 아주 빠르다는 장점이 있다. Hadoop 여기서 TMI는 이름을 뭘로 할까 고민하다가 더그커팅의 아이가 노란 코키리 장난감을 가지고 놀면서 그냥 Hadoop이라고 하는걸 듣고 하둡이라고 이름을 지었다고 한다. 아무튼 병렬처리는 여러대의 노드를 묶어서 마치 하나의 서버처럼 보이게하고 여러 노드의 자원을 이용해서 데이터를 처리하기 때문에 처리하는 속도가 아주 빠르다는 장점이 있다. 현대의 서버로 1테라 바이트의 데이터를 처리하는데 걸리는 시간은 2시간 반이 걸린다고 하면 병렬처리로 작업을 하면 2분내에 데이터를 읽을 수 있다. 사실 하둡이 사용하는 것이 좋지만 우선 R에서 사용할 수 있는 간단한 병렬처리를 코드를 작성해볼까한다. snow함수를 이용한 병렬처리 병렬처리는 여러가지 방법으로 할 수 있는데, 아래는 snow를 이용한 병렬처리 예제이다. install.packages(\"snow\") library(snow) ex.df &lt;- data.frame(a=seq(1,100000,1),b=seq(10,1000000,10),c=runif(10000)) custom.function &lt;- function(a,b,c){ result &lt;- (a+b)*c return(result) } system.time(apply(ex.df,1, function(x) custom.function(x[1],x[2],x[3]))) clus &lt;- makeCluster(3) clusterEvalQ(clus, custom.function &lt;- function(a,b,c){ result &lt;- (a+b)*c return(result)}) clusterExport(clus,\"custom.function\") system.time(parRapply(clus,ex.df, function(x) custom.function(x[1],x[2],x[3]))) ClusterEval는 snow 에서 함수 선언할때 사용하는 함수이다. foreach함수를 이용한 병렬처리 또 다른 패키지인 foreach를 사용하여 병렬처리를 해볼까한다. install.packages(\"foreach\") library(foreach) system.time(l1 &lt;- rnorm(100000000)) system.time(l4 &lt;- foreach(i = 1:4, .combine = 'c') %do% rnorm(25000000)) 먼가 코드가 점점 짧아지는거 같다. doParallel함수를 이용한 병렬처리 마지막으로 doParallel를 사용하여 병렬처리 하는 방법은 아래와 같다. install.packages(\"doRarallel\") library(doParallel) registerDoParallel(cores=4) system.time(l4p &lt;- foreach(i = 1:4, .combine = 'c') %dopar% rnorm(25000000))",
    "tags": "R",
    "url": "/r/2020/07/01/r-doparallel.html"
  },{
    "title": "k-means? 유사한 데이터끼리 묶을 수 있을까?",
    "text": "k-means는 우선 비지도 학습에 속한다. 비지도 학습은 대체로 라벨이 없기 때문에 입력 데이터의 특징을 가지고 자동으로 분류한다. k-means는 그중에서 간단하고 자주 쓰이는 편에 속한다. k-means 위에서도 말했듯, 요약하면 k-means는 각 문서들 속에 들어있는 데이터 분석을 통해 유사하거나 관계가 높은 항목들끼리 집합을 만들고 싶을 때, 사용하는 알고리즘이다. 그림을 설명해보자면 아래와 같다. 처음에 중심값을 선택. (랜덤하게 중심값을 선택) 클러스터 할당: k개 (그림 속에서는 2개)의 중심값과 각 개별 거리를 측정한다. 가장 가까운 클러스터에 해당 데이터를 분류한다. 새 중심값 선택 : 클러스터마다 새로운 중심값을 계산한다. 범위 확인 : 선택된 중심값이 변화가 어느정도 없다면 멈춘다. -&gt; 만약 있다면 (1)번부터 반복 k-means 단점 k-means의 단점으로는 이상치(노이즈)에 민감하다는 점이 있다. 노이즈 데이터를 잘 처리해주지 않으면 이상치 값과 그렇지 않은 값으로 뷴류한다. 정리하면, k-means의 한계점을 뽑자면 아래와 같다. k값 입력 파라미터를 직접 지정해야한다. 이상치에 민감하다. k-means 예제 기본 데이터 셋을 간단하게 만든다. c &lt;- c(3,4,1,5,7,9,5,4,6,8,4,5,9,8,7,8,6,7,2,1) row &lt;- c(\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\") col &lt;- c(\"X\",\"Y\") data &lt;- matrix( c, nrow= 10, ncol=2, byrow=TRUE, dimnames=list(row,col)) 위에서 만든 데이터 셋을 plot 그래프로 그려서 확인하면 아래와 같다. k-mean 패키지 설치 후, 분류하면 아래와 같다. 원래 데이터와 겹쳐서 함께 시각화하면 아래와 같다. plot(round(km$centers), col=km$centers, pch=22, bg=km$centers, xlim=range(0:10), ylim=range(0:10)) par(new=T) plot(data, col=km$cluster+1, xlim=range(0:10), ylim=range(0:10)) 그렇게 어렵지 않게 데이터를 군집화 시킬 수 있기 때문에 간단하게 많이들 돌려보는 편이다. 또한 간단한거 치고는 유용하기 때문에 데이터 분석할때 사용하기 용이하다.",
    "tags": "R",
    "url": "/r/2020/06/30/r-k-means.html"
  },{
    "title": "회귀로 삼성전자와 현대자동차 주식 수익률 분석하기",
    "text": "단순 선형회귀를 이용해서 이번에 코스피 지수 수익율과 삼성, 현대자동차 주식 수익율의 상관 괸계를 간단하게 분석해볼까 한다. 데이터는 한국 거래소에서 다운 받을 수 있다. plot 우선 그레프를 그릴껀데, plot 그래프를 그리기 쉽게 모든 데이터를 merge를 한다. k_index &lt;- read.csv(\"K_index.csv\", header=T, stringsAsFactors = F) s_stock &lt;- read.csv(\"S_stock.csv\", header=T, stringsAsFactors = F) h_stock &lt;- read.csv(\"H_stock.csv\", header=T, stringsAsFactors = F) all_data &lt;- merge(merge(k_index, s_stock), h_stock) head로 데이터를 보면 아래와 같이 볼 수 있다. head(all_data) 그래프는 아래와 같고, x축은 코스피 등락 비율, y축은 삼성 수익율 등락 비율이다. attach(all_data) plot(k_rate, s_rate, col='blue') 여기서 회귀 직선을 그으면 아래와 같은 그림을 볼 수 있다. s_model &lt;- lm(s_rate~k_rate, all_data) abline(s_model, col='red 현대도 삼성과 마찬가지로 그래프를 그리면 아래와 같다. plot(k_rate, h_rate, col='purple') h_model &lt;- lm(h_rate~k_rate, all_data) abline(h_model, col='brown') 데이터 분석 10년치 데이터를 가지고 그래프를 그리면 아래와 같이 그릴 수 있고, 기울기를 보면 현대는 1.06, 삼성은 1.02임을 알 수 있다. 전체적으로 봣을때는 비슷비슷 한거 같다. 그렇다면 상관 계수를 한번 살펴보자. 처음 데이터를 가지고 상관 계수를 구하면 아래와 같다. 그리고 10년치 데이터를 사용하여 상관 계수를 구하면 아래와 같다. 그렇다면 상관계수와 기울기를 가지고 어떤 결론을 낼 수 있을까? 이 작업에 베타라는 것을 구하는데 베타는 시장과의 상관성을 말한다. 시장과의 상관성이란 시장을 움직이는 것에 따라 얼마나 탄력적으로 움직이는지를 말한다. 기울기가 낮을수록 뚝심이 강해 시장과 상관없이 움직이는 주식이고, 기울기가 높을수록 탄력적으로 움직이는 주식이다. 수익률이 낮더라도 안전하게 벌고 있다면 기울기가 낮은 것을 선택해야 하며, 위험성이 높아도 수익률이 높은 것을 하려면 기울기가 높은 것을 선택해야 한다. 상관계수는 시장과 얼마나 비슷하게 움직이냐는 것을 찾는 것이다. 현업에서는 0.65에서 0.70위부터 가치 있다고 판단하고 투자 분석을 한다고 한다. 현재 주식 투자를 하지는 않겠지만, 아무튼 재밌는 데이터였다.",
    "tags": "R",
    "url": "/r/2020/06/14/r-linear-regression2.html"
  },{
    "title": "단순 선형회귀? 회귀란 ?",
    "text": "회귀는 하나의 변수가 나머지 다른 변수들과의 선형적 관계를 갖는가의 여부를 분석하는 하나의 방법이디. 즉 하나의 종속변수와 독립변수 사이의 관계를 명시하는 것을 우리는 회귀라고 한다. 오늘은 회귀가 무엇인지와 구하는 방법에 대해 알아보겠다. 회귀 독립변수와 종속변수는 아래와 같다. 독립 변수: 종속 변수의 영향을 주는 변수(평수, 학군) 종속 변수: 서로 관계를 가지고 있는 변수들 중에서 다른 변수의 영향을 받는 변수 (집값) 회귀식은 $y = ax + b$ 이며, 정말 간단하다. 회귀식을 이용하여 간단하게 예제 문제를 풀어보겠다. 1986년 1월 28일 미국의 스페이서 셔틀 챌린저호의 승무원 7명이 사망했다. 우주 왕복선이 발사 도중에 폭파해서 사망을 했기 때문이다. 폭파에 대한 원인 분석을 했는데 그 원인이 발사 온도에 대한 o형 링의 파손 때문이라고 한다. 그렇다면 식을 대입해 풀어본다면 아래와 같다. 여기서 a와 b를 구해야는데, 최적의 a(기울기)와 b(절편)을 결정하기 위해 정규 최소 제곱으로 알려진 추정 기법을 사용할 수 있다. 실제 값와 예측값 사이의 수직 직선인 오차(잔차)를 제곱해서 구한 총합을 알아야 한다. 사실 식을 깔끔하게 정리하면 아래와 같다. 결국 기울기와 절편을 R을 이용해서 구하면 아래와 같다. a &lt;- cov(challenger$temperature, challenger$distress_ct)/var(challenger$temperature) b &lt;- mean(challenger$distress_ct)-(a*mean(challenger$temperature)) # a (기울기) = -0.057 # b (절편) = 4.3 만약 온도가 31도고 a가 -0.057이고 b가 4.3임을 알아냈다면 식은 $y = -0.057*31+4.3$이 된다. 약 3개의 링이 파손 되었을거라고 추정할 수 있다.",
    "tags": "R",
    "url": "/r/2020/06/13/r-linear-regression.html"
  },{
    "title": "파이썬에서의 클래스 생성자와 소멸자",
    "text": "파이썬에서의 클래스 생성자와 소멸자에 관해 알아보겠습니다. 생성자는 이름에서 알 수 있듯이 객체가 만들어질 때 호출되는 함수를 생성자라고 이야기 하며, 객체가 사라질 때 호출되는 함수를 소멸자라고 이야기합니다. init 위에서 생성자가 무엇인지 이야기를 했는데, 생성자는 왜 사용하는 걸까요? 생성자는 객체를 초기화 할 때, 자주 사용합니다. class MyClass: def __init__(self): self.var = '안녕하세요!' print('MyClass 인스턴스 객체가 생성되었습니다') obj = MyClass() # ‘MyClass 인스턴스 객체가 생성되었습니다’가 출력됨 print(obj.var) # ‘안녕하세요’가 출력됨 del def __init__으로 객체를 생성했으므로 이번엔 객체를 __del__를 사용하여 소멸시켜보겠습니다. class MyClass: def __init__(self): self.var = '안녕하세요!' print('MyClass 인스턴스 객체가 생성되었습니다') def __del__(self): print('MyClass 인스턴스 객체가 메모리에서 제거됩니다') obj = MyClass() del obj",
    "tags": "Python",
    "url": "/python/2020/05/20/python-init-del.html"
  },{
    "title": "헤드라인 뉴스 키워드으로 바라본 한국 경제 흐름",
    "text": "저번 포스팅때는 한국 사회의 흐름을 알아보았는데요. 지난 19년을 되돌아보니 사건, 사고가 많았으며, 다시는 일어나면 안되는 사고들과 잊으면 안되는 일들이 많았습니다. 그럼 이번엔 경제를 되돌아보며 한국 경제에는 어떤 일들이 있었는지 알아보겠습니다. 그리고 크롤링은 저번에 했기 때문에 따로 다루지는 않겠습니다. Keyword 사회와 마찬가지로 경제도 표를 만들면 아래와 같습니다. 우선 경제하면 가장 많이 떠오르는 것이 부동산이 아닐까 싶습니다. 역시나 부동산이나 재건축이 가장 많이 눈에 띄는 것이 보입니다. 2005년 국세청 키워드가 보이는데, 아마 종부제 시행으로 인해 많은 헤드라인을 장식했던 키워드 인것 같습니다. 경제하면 기업들이 빼놓을 수 없는데, 아래와 같이 예전에 비해서는 갈수록 기업의 이름이 언급되는 걸로 보아 이제는 한국을 대표하는 기업이라고도 볼 수 있겠네요. 이번에는 직접적으로 한국에 영향을 끼쳤던 것이 무엇인지 알아보겠습니다. 아래와 같이 FTA, 광우병, 갤럭시 등을 볼 수가 있습니다. FTA나 광우병때는 제가 어려서(?) 뭔지 잘 몰랐었는데, 지금 보니 참 아이러니 했네요. 금융이나 기름값으로도 많은 문제가 있었던 해가 보이기도 하고 갤럭시 휴대폰이 터져서 문제가 되었던 2016년도 보입니다. 경제 불황은 국제 부분과 함께 보면 좋겠지만, 우선 우리나라만 놓고 보면 2009년 이후로 경제 불황 키워드가 보이기 시작합니다. 실제 통계청에서도 경기 성장률 관련 지표를 찾아서 볼 수 있으며 아래와 같습니다. 2008년 이후로 글로벌 금융 위기와 유럽 재정위기의 여파로 인해 세계 경기가 침체됨에 따라 한국의 성장률도 3%내외로 하락했고 장기적으로 성장률이 하락했다고 합니다. 일반적으로 보이는 현상이라고는 하는데, 그래도 이런 경제 침체는 없으면 좋겠습니다. 이렇게 보니 나중에 2020년도때는 더 많은 경기 불황 키워드가 보이지 않을까 합니다. 코로나로 인해 많은 사람들이 피해를 보고 있는 이때, 모두들 잘 이겨냈으면 좋겠습니다.",
    "tags": "R",
    "url": "/r/2020/05/13/r-keyword-analysis2.html"
  },{
    "title": "미디어 트렌드 분석?헤드라인 뉴스 키워드 분석(사회)",
    "text": "미디어는 한국 사회의 흐름을 압축적으로 담고 있는 Snapshot입니다. 지난 19년, 가장 대표적이고 강력한 미디어, 헤드라인 뉴스로 한국 사회를 되돌아 볼까합니다. 이번 포스팅은 사회 부분을 다뤄보겠습니다. SBS뉴스 사회 부분의 헤드라인 부분을 다 크롤링하여 문장을 단어별로 나눈 후 count 하였습니다. Web Crawling R을 이용하여 크롤링을 하려고 할때 아래와 같은 라이브러리가 필요합니다. # 필요 패키지 설치 library(gsubfn) library(stringr) library(XML) 크롤링 함수는 아래와 같이 URL을 입력받아 긁어오도록 하였습니다. SBSnews &lt;- function(url){ doc &lt;- htmlTreeParse(url, useInternalNodes = T, trim = T, encoding=\"utf-8\") rootNode &lt;- xmlRoot(doc) - xmlName(rootNode) names(rootNode) result &lt;- xpathSApply(rootNode, \"//strong[@class='spml_tit']\", xmlValue) print(idx) return(result) } 크롤링 기간은 아래와 같이 정하겠습니다. 저는 1년씩 하나씩 긁어서 1년 단위로 정리하겠습니다. urlbase &lt;- \"http://news.sbs.co.kr/news/programMain.do?prog_cd=R1&amp;broad_date=\" Start &lt;- as.Date(\"2018/01/01\") yyyy &lt;- substr(Start, 1, 4) End &lt;- as.Date(\"2018/12/31\") list &lt;- seq(from = Start, to = End, by=1) list2 &lt;- format(list, format=\"%Y%m%d\") urlfinal&lt;- str_c(urlbase,list2) 그리고 for문으로 Crawling 해줍니다. idx &lt;- 1 ## 초기화 result &lt;- as.character() for(url in urlfinal){ print(idx) result &lt;- c(result,SBSnews(url)) idx &lt;- idx + 1 } str(result) 크롤링한 데이터를 저장하기 전에 1차적으로 간단하게 전처리 해주고 파일을 저장합니다. fulltitle &lt;- gsub(\"[\\r\\t\\n]\", \"\", result) news_df &lt;- data.frame(category = substr(fulltitle,1,2), fulltitle = fulltitle, stringsAsFactors = F) str(news_df) idx &lt;- as.numeric(which(news_df$category==\"스포\")) # Category가 \"스포\" -&gt; \"스포츠\"로 변경 for(n in idx){ news_df$category[n] &lt;- \"스포츠\" } news_df$title &lt;- substring(news_df$fulltitle,3) for(n in idx){ news_df$title[n] &lt;- substring(news_df$fulltitle[n],4) } news_df2 &lt;- subset(news_df, !grepl(\"*클로징*\", news_df$title)) news_df2 &lt;- subset(news_df2,news_df2$title != \"오늘의 주요뉴스\") rownames(news_df2) &lt;- 1:nrow(news_df2) nrow(news_df)-nrow(news_df2) Keyword 파일을 저장 후 데이터를 따로 전처리를 끝내고 나면 아래와 같이 사회관련 뉴스 부분을 정리 할 수 있는데, 여기서 가장 많은 키워드는 아무래도 ….. 검찰과 경찰이겠죠? 그리고 주기적으로 보이는 키워드는 아무래도 날씨와 관련 있는 재해 부분 키워드 입니다. 저 부분을 제외하면 중요해 보이는 키워드는 아래와 같습니다. 잠깐만 봐도 해당 키워드들은 한 해를 떠들썩하게 했던 사건사고 키워드라는 것을 알 수 있습니다. 잊으면 안되는 2003년도 대구지하철 참사부터 2009년 신종플루 2010년 김길태, 천안함 사건, 2014년 세월호 등 많은 중요 키워드를 보고 한국 사회에서 일어난 모든 흐름을 알 수 있습니다. 다음에는 경제 관련해서 얼마나 많은 일이 있었는지에 대해 알아보겠습니다.",
    "tags": "R",
    "url": "/r/2020/05/10/r-keyword-analysis.html"
  },{
    "title": "의사 결정트리로 독일은행의 대출 채무 이행 여부 분석하기(C5.0 Model)",
    "text": "의사 결정트리로 독일 은행의 대출 채무 이행 여부를 분석해볼까합니다. 데이터는 여기에서 다운 받을 수 있습니다. 그리고 코드는 여기 GitHub에서 확인 할 수 있습니다. DataSet 데이터 라벨은 default 변수이며, yes는 대출금 미상환, no는 대출금을 상환했다는 뜻입니다. 대부분의 컬럼은 이름에서부터 알 수 있기 때문에 다루진 않고, 알기 어려운 컬럼에 대해 설명을 조금 설명을 하자면 아래와 같습니다. checking_balance 예금 계좌 savings_balance 적금 계좌 amount 대출 금액 데이터에서 저 컬럼이 왜 있는지에 대해 분석을 조금 하게 되면, 데이터 분석시에 조금 수월할 수 있습니다. 예금계좌와 적금계좌가 있는 이유는 대출 신청자의 예금계좌와 적금계좌의 예금 정도를 확인해서 예금액이 많을수록 대출이 안전하다고 가정을 지을 수 있기 때문에 존재하는 컬럼입니다. summary(credit$amount) 위와 같이 summary를 하게 되면 대출 금액의 구성도를 대충 파악 할 수 있습니다. 대출금액이 250 마르크에서 18424마르크로 구성되어 있음을 알 수 있습니다. 여기서 조금 더 나아가 대출금 상환을 한 사람과 안한사람의 비율을 잠깐 보면 아래와 같습니다. prop.table(table(credit$default)) 70%정도가 대출금을 상환했다는 것을 알 수 있습니다. 과거 데이터를 분석해보니 대출금 상환 불이행자가 30%나 되니 앞으로 30%이내로 떨어트리는게 은행의 목표가 되겠금 model를 생성해보겠습니다. 우선 데이터를 shuffle 시키고 훈련데이터와 테스트 데이터를 9:1 비율로 나누겠습니다. 보통은 7:3 비율로 데이터를 나누지만, 어떤 비율을 할지는 자신의 선택이므로 다른 비율로 하셔도 괜찮습니다. set.seed(1) train_cnt &lt;- round(0.9*dim(credit)[1]) train_index &lt;- sample(1:dim(credit)[1], train_cnt, replace=F) credit_train &lt;- credit[train_index, ] credit_test &lt;- credit[-train_index, ] 또는 # train_cnt를 쓰지 않고 train_index &lt;- sample(2, nrow(credit), prob=c(0.9, 0.1), replace=T) credit_train &lt;- credit[train_index==1, ] credit_test &lt;- credit[train_index==2, ] C5.0 Model 의사 결정 알고리즘 중 C5.0 이라는 알고리즘을 적용하여 예측 모델을 생성해보겠습니다. # 패키지 설치 install.packages(\"C50\") library(C50) # 의사결정트리 C5.0 알고리즘을 적용한 모델 만들기 credit_model &lt;- C50(credit_train[-17], credit_train[17]) # 라벨을 뺀 전체 컬럼 라벨 컬럼 그리고 모델을 적용하여 예측합니다. credit_result &lt;- predict(credit_model, credit_test) 이번엔 위에서 만든 결과를 사용하여 이원 교차표를 작성하겠습니다. library(gmodels) CrossTable(credit_result, credit_test[, 17]) 표를 보면 알 수 있듯이 채무 이행을 안했는데, 채무 이행을 했다고 나온 경우를 볼 수 있습니다. 채무 이행으로 나왔으나 불이행한 사람이 14명이나 있었으므로 정확도를 올리고 오류율 14명을 줄일 수 있도록 모델의 성능을 올려보겠습니다. 앙상블 credit_model2 &lt;- C5.0(credit_train[,-17], credit_train[,17], trials=10) credit_result2 &lt;- predict(credit_model2, credit_test[,-17]) CrossTable(credit_result2, credit_test[, 17]) 성능이 월등하게 개선되진 않았지만, 결정 트리 하나의 모델로만으로도 성능을 개선할 수 있음을 알 수 있었습니다. 여기서 만약 성능 개선을 더 해보고 싶다면, 아래와 같이 앙상블을 이용하여 모델을 개선할 수 있습니다. (앙상블에 대한 자세한 설명은 다음에..) install.packages(\"caret\") library(caret) set.seed(300) m &lt;- train(default ~., data = credit , method = \"C5.0\" ) p&lt;-predict(m, credit) table(p, credit$default) 정확도 99.8%까지 올릴 수 있음을 알 수 있습니다.",
    "tags": "R",
    "url": "/r/2020/04/24/R-Decision-Tree-credit.html"
  },{
    "title": "Python 정수 리스트에서 소수만 걸러내기(filter)",
    "text": "파이썬에서 간혹 리스트에 소수만을 걸러 내고 싶을때, 사용되는 filter 함수입니다. 이름에서부터 느껴지듯 뭔가 걸러내기 위해 사용되는 함수라는 것을 알 수 있습니다. filter 기본적으로 내장되어 있는 모듈이기 때문에 따로 불러 올 필요는 없습니다. filter를 이용하면 소수를 걸러낼 수 있다고 했는데, 소수는 두개의 수로 못나눈 것을 이야기합니다. 대표적으로 7이 소수가 될 수 있습니다. 소수의 반대는 합성수라고 이야기 할 수 있는데, 합성수의 예로는 6이 있습니다. (2*3=6) def getPrime(x): for i in range(2, x-1): if x%i == 0: break else: return x listdata = [117, 119, 1113, 11113, 11119] ret = filter(getPrime, listdata) print(list(ret)) # [11113, 11119] 가 출력됨",
    "tags": "Python",
    "url": "/python/2020/04/24/python-filter.html"
  },{
    "title": "파이썬에서 에러가 발생해도 특정 코드는 무조건 실행되게 하기 (try ~ except ~ finally)",
    "text": "에외처리에도 여러가지 방법이 있는데, 이번에는 실행한 코드가 정상적으로 작동을 하든, 에러가 나던 무조건 실행되는 블록을 추가해보는 것을 해보겠습니다. 예외처리 (try ~ except ~ finally) try ~ except ~ finally문법은 아래와 같습니다. try ~ except과 별반 다를 것이 없고 그냥 한줄만 추가하면 되는거라 복잡하지는 않습니다. def my_power(): try: x = input('분자 숫자를 입력하세요 ~ ') y = input('분모 숫자를 입력하세요 ~ ') z = int(x) / int(y) except: print(' 0 으로 나눌 수 없습니다. ') finally: print('저는 무조건 실행할꺼라구요! 아.시.겠.어.요.?') print(my_power())",
    "tags": "Python",
    "url": "/python/2020/04/19/try-except-finally.html"
  },{
    "title": "파이썬에서 간단하게 예외처리하기(try ~ except ~ else)",
    "text": "파이썬에서 예외처리는 프로그램에서 에러가 발생 했을 때, 에러를 핸들링하는 기능을 이야기합니다. 예외처리 (try ~ except ~ else) 에러가 나면 보통 프로그램이 종료되기 때문에 큰 문제가 발생하게 됩니다. 예를들면 API를 구현했는데, 어떠한 이유로 에러가 나서 종료가 된다면 큰 문제가 될 수 밖에 없습니다. 그래서 보통 예외처리를 하게 되는데, 예외처리를 하게 된다면 에러가 나는 부분을 제외하고 나머지 프로그램은 정상 작동하기 때문에 문제가 발생되지 않습니다. def my_power(): x =input(\" 분자 숫자를 입력하세요 \" ) y =input(\" 분모 숫자를 입력하세요 \") return int(x)/int(y) print(my_power()) 위와 같이 코드를 작성하고 에러를 출력해보겠습니다. 여기서 try ~ except를 사용하겠습니다. def my_power(): try: x = input('분자 숫자를 입력하세요 ~ ') y = input('분모 숫자를 입력하세요 ~ ') z = int(x) / int(y) except: print(' 0 으로 나눌 수 없습니다. ') else: print('나눈 값을 잘 추출했습니다.') return z print(my_power()) 여기서 복수개의 except절을 사용하여 예외처리를 여러 개 나열해보겠습니다. def my_power(): try: x = input('분자 숫자를 입력하세요 ~ ') y = input('분모 숫자를 입력하세요 ~ ') z = int(x) / int(y) return z except ZeroDivisionError as err: print(' 0 으로 나눌 수 없습니다.') except: print(' 다른 예외 상황입니다. ') print(my_power()) 이렇게 되면 에러로 인한 비정상 종료를 막을 수 있기 때문에 코드를 작성할때, 꼭 사용해야 합니다.",
    "tags": "Python",
    "url": "/python/2020/04/19/try-except-else.html"
  },{
    "title": "min/max 알고리즘이란?",
    "text": "강화학습에서의 자주 거론되는 min/max 알고리즘에 대해 자세히 알아보겠습니다. min/max 알고리즘 min/max 알고리즘에서 max는 나를 뜻하는 것이고 min는 적을 이야기 합니다. 탐색전에 트리는 맨 아래 단계만 의미있는 수를 가지며, 탐색이 진행되면서 하위에서 상위로 값을 찾아 올라가게 되는데 이때 상위 노드에서 선택되는 값은 현 단계가 max이면 아래에서 최대값을, 현 단계에서 min이면 아래에서 최소값을 선택하게 됩니다. 즉, 나(컴퓨터)는 가장 큰 수를 취해야 하며, 상대(사람)은 가장 작은 수를 취해야 합니다. 양수(+ ∞)는 내가 이긴 것(컴퓨터)이고, 음수(- ∞)는 내가 진 것(사람)을 이야기합니다. 강화학습으로 Tic Tac Toe을 학습한다고 했을 때, 평가 함수 e와 상태 함수 p가 있다고 가정하고 이야기 하면 아래와 같습니다. 상태 함수 p가 양쪽 모두에게 이기는 상황이 아니라면 e(p) = (Max에게 가능한 행, 열, 대각선 수) - (Min에게 가능한 행, 열, 대각선 수) 상태 함수 p가 Max에게 이기는 상황 (컴퓨터가 이기는 상황) e(p) = + ∞ 상태 함수 p가 Min에게 이기는 상황(사람이 이기는 상황) e(p)= - ∞ 정리하자면, 숫자는 평가 함수 e에 의해서 결정이 되는데, Max는 숫자가 큰 값을 선택해야 자신이 이길 경우의 수가 더 많은 경우로 나아갈 수 있습니다. Min은 숫자가 적은 값을 선택해야 자신이 이길 수 있는 경우의 수가 많은 값으로 나아갈 수 있습니다. Tic Tac Toe으로 결정 과정을 보면 아래 그림과 같습니다. 상대방 수는 가장 작은 수를 올리고 내 수는 가장 큰 수를 올립니다. 상대가 두는 턴(사람)↑ 내가 두는 턴(컴퓨터)↑ 그러나 오목이나 바둑 같은 경우는 트리가 굉장히 크기 때문에 이 문제를 해결하기 위해 나온 알고리즘이 알파베타프루니입니다. 이 부분에서는 다음에 다시 다루겠습니다.",
    "tags": "MachineLearning",
    "url": "/machinelearning/2020/03/20/python-minmax.html"
  },{
    "title": "파이썬으로 원주율 구하기(몬테카를로)",
    "text": "오늘은 몬테카를로 알고리즘으로 파이썬 원주율을 구해보려고 합니다. 원주율 가로 길이가 2인 사각형에 내접하는 반지름 길이가 1인 원의 넓이를 몬테카를로 알고리즘으로 구해보겠습니다. 원에 무작위로 점을 찍어서 원 안에 들어가는 점의 개수와 사각형 안에 찍히는 전체 점의 개수를 비교합니다. 예를 들면 10개의 점을 무작위로 찍었을 때, 원 안에 찍히는 점의 개수가 7개 라면? 이런식으로 비교하는 거죠. 그래서 100개의 점을 무작위로 찍었을 때, 1000, 10000, … 1억개의 점을 무작위로 찍었을 때, 찍는 점의 수가 많아 질수록 실제 원의 넓이에 가까워 집니다. 만약에 100개의 점을 찍었을 때, 이 중 80개의 점이 부채꼴의 내부에 찍혔다면, 아래와 같이 계산 할 수 있습니다. 코드로 구현하면 아래와 같이 구현 할 수 있습니다. num=int(input('랜덤을 반복할 횟수는 ? ')) cnt=0 for i in range(num): x=random.uniform(0,1) y=random.uniform(0,1) if x*x + y*y &lt;= 1: cnt+=1 print((cnt/num)*4) 몬테카를로 원주율을 구하기 위해선 랜덤으로 점을 찍어야 하기 때문에 랜덤으로 점을 막 찍고 부채꼴 안에 있는 점의 갯수가 몇개인지 count만 하면 되기 때문에 코드가 간단합니다.",
    "tags": "Python",
    "url": "/python/2020/03/20/python-circumference.html"
  },{
    "title": "강화학습에서의 몬테카를로 알고리즘이란?",
    "text": "강화 학습에서 빠질 수 없는 몬테카를로 알고리즘에 대해 자세히 알아보겠습니다. 몬테카를로 알고리즘 몬테카를로 알고리즘의 원리는 멘땅의 해딩을 여러번 반복하다 보니, 답을 알게 되었다 입니다. 확률의 종류는 3가지로 볼 수 있습니다. 1. 수학적 확률: 경험을 하지 않고도 미리 알 수 있는 확률로 동전을 던질 때, 앞면이 나올 확률은 굳이 경험하지 않아도 수학적으로 구할 수 있는 확률 2. 통계적 확률: 똑같은 조건에서 아주 오랫동안 과거의 경험을 바탕으로 확률을 구하는 방법. 예를 들어 비가 올 확률과 같이 과거의 데이터를 바탕으로 얻는 경험적 확률입니다. 3. 기하학적 확률: 1777년 프랑스 수학적인 조르주주이르클레이드뷔퐁이 제시한 ‘바늘 문제’에서 유명해졌습니다. 바늘 문제란 바늘이 여러개의 평행선을 그은 평면 위에 던질 때, 바늘이 평행선 가운데 하나와 만날 확률이 얼마인지를 따지는 문제인데, 이 문제로 원주율 값을 알아냈다. 몬테카를로 트리 탐색 “수를 두기 전에 미리 수를 다 넣어보고 가장 좋은 결과 값으로 수를 둔다” 또는 “랜덤을 발생시켜서 해답을 찾아가는 과정”이라고 볼 수 있습니다. 한수를 평가하기 위해서 그것에서 비롯되는 모든 가능성을 탐색하는게 아니라 무작위로 선택한 샘플만을 검토합니다. 샘플 크기를 늘리고 시물레이션 횟수를 증가시키면 모든 가능성을 검토한 것과 같은 결과가 나옵니다. 몬테카를로 알고리즘이 필요한 이유 많은 요소들이 작용하는 불확실한 상황에서 의사결정을 하기 위해서입니다. 랜덤 값을 발생시켜 시물레이션을 함으로써 직접 계산하기 어려운 복잡한 수치들을 구할 필요가 있을 때 유용합니다. 실제 세상에서 발생되는 거의 모든 일들은 확률로 설명될 수 있습니다. 이러한 확률적 현상들을 수학적으로 풀고 싶은데, 그것이 수학적으로 정밀하게 풀어낼 수 없을 때 랜덤 값을 발생시켜 시물레이션함으로서 정답에 근접한 해를 구할 수 있습니다.",
    "tags": "MachineLearning",
    "url": "/machinelearning/2020/03/20/python-reinforcement.html"
  },{
    "title": "PLSQL에서 예외처리(exception)하기",
    "text": "예외처리는 모든 프로그래밍에서 사용되며, PLSQL도 여김 없이 예외처리가 있습니다. 예외처리가 무엇인지와 종류에 대해서 이야기 해보겠습니다. 예외(exception) PLSQL에서 예외는 오라클에서 에러나 갔을 때, 엔드유저(프로그램 사용자)의 눈높이를 맞추기 위해 사용하는 문법입니다. 예를 들어 홈플러스 계산원이 컴퓨터 프로그램 화면에 고객 번호를 입력하고 해당 고객 정보를 보려고 하는데, 오라클 에러 메시지가 화면에 ORA-0001 Data not found …. 이런식으로 나온다면 사용자는 알아보기 힘드니, 해당 고객은 존재하지 않습니다등의 문구를 띄웁니다. 이 경우 외에도 비정상적으로 프로그램이 종료되지 않기 위해서 사용되는데, data가 잘못되어서 프로그램이 종료되버리는 현상을 막기 위해 프로그램이 종료되는게 아니라 정상적으로 처리가 되고 data가 잘못되어서 발생하는 에러 메세지만 따로 출력해주겠금 하려고 예외를 사용합니다. 예외(exception)의 종류 3가지 예외처리는 총 3가지 종류로 나눌 수 있습니다. 1. 오라클에서 미리 정의한 예외처리 예를 들면 no data found , too many rows 같은 일반적인 예외에 대한 예외처리가 있습니다. 2. 오라클에서 미리 정의하지 않는 예외처리 오라클에서 미리 정의 하지 않는 예외를 따로 예외처리하는 방법이 있습니다. 3. 사용자 정의 예외처리 오라클에서 미리 정의한 예외나 오라클에서 미리 정의하지 않는 예외는 둘다 오라클 에러 메세지가 출력될 때, 발생하는 예외인데, 그에 반해 사용자 정의 예외는 오라클에서 에러가 발생하지 않았지만 이것은 예외다 라고 사용자가 정의하는 것을 말합니다. 사원번호를 물어봤는데, 만약 없는 사원번호를 입력할 경우 아래와 같이 쓸 수 있습니다. 아래의 오류는 프로그램 문법상의 오류가 아니라 data가 없어서 발생하는 오류이므로 예외처리를 해줘야 합니다. accept p_empno prompt ' 사원번호? : ' declare v_empno emp.empno%type := &amp;p_empno; v_sal emp.sal%type; begin select sal into v_sal from emp where empno = v_empno; dbms_output.put_line(v_sal); exception ------ 예외처리 when no_data_found then dbms_output.put_line('없는 사원입니다.'); end; / 중복으로 예외처리를 걸 경우 아래와 같이 쓸 수 있습니다. accept p_empno prompt ' 사원번호? : ' declare v_empno emp.empno%type := &amp;p_empno; v_sal emp.sal%type; begin select sal into v_sal from emp where empno = v_empno; dbms_output.put_line(v_sal); exception when no_data_found then ------ 예외처리 1 dbms_output.put_line('없는 사원입니다.'); when too_many_rows then ------ 예외처리 2 dbms_output.put_line('이 사원의 경우는 데이터가 많습니다.'); end; /",
    "tags": "DB",
    "url": "/db/2020/03/20/plsql-exception.html"
  },{
    "title": "PLSQL에서 커서(cursor)가 필요한 이유.(메모리에 데이터 올리기)",
    "text": "오랜만에 PL/SQL에 관한 포스팅을 쓰네요. 오늘은 PLSQL에서 커서(cursor)라는 것에 대해 알아보겠습니다. 커서(cursor) 커서는 4단계로 볼 수 있습니다. 커서선언, 커서오픈, 커서패치, 커서닫기가 있습니다. 커서는 메모리에 올릴 데이터를 결정하고 메모리에 올린 데이터를 한건씩 가져올때 사용합니다. 커서의 진행 단계는 아래와 같습니다. 1. 커서선언: 메모리에 올랄 데이터를 결정 2. 커서오픈: 메모리에 올린 데이터를 쓰기 위해 메모리를 여는 단계 3. 커서패치: 메모리에 올린 데이터를 한건씩 가져오는 작업 4. 커서닫기: 메모리 닫기 Close 위 절차대로 사용해야하며 아래와 같이 사용 하실 수 있습니다. accept p_ename prompt ' 이름을 입력하세요 ~ ' declare v_ename emp.ename%type := upper('&amp;p_ename'); v_sal emp.sal%type; v_job emp.job%type; cursor emp_cursor is select ename ,sal ,job from emp where job = v_job; ---- 커서 선언 begin open emp_cursor; ----- 커서 열기 loop fetch emp_cursor into v_ename , v_sal, v_job ; ----- 커서 패치 exit when emp_cursor%notfound; dbms_output.put_line(v_ename ||' ' ||v_sal || ' '|| v_job); end loop; ---------- 커서 문 닫음 Close emp_cursor; end; /",
    "tags": "DB",
    "url": "/db/2020/03/20/plsql-cursor.html"
  },{
    "title": "강화학습에서 사용하는 용어와 역사",
    "text": "강화학습은 시행착오를 통해서 환경에 적응하는 학습 방법을 이야기합니다. 지도학습과는 다르게 이 행동이 맞다는 것이 명시적으로 주어지는 것이 아니라, 보상을 통해 행동의 바람직함을 스스로 알아가는 학습 방법입니다. 강화학습에서 사용하는 용어 강화학습에서는 크게 6가지의 용어가 있습니다. 1. 에이전트: 강화학습을 통해 학습하는 컴퓨터. 학습의 주체 2. 환경: 에이전트가 행동하는 곳 3. 상태: 에이전트가 처한 특정한 상황에 대해 관찰 4. 행동: 에이전트가 어떠한 상태에서 취할 수 있는 행동 5. 보상: 행동에 대한 보상 6. 정책: 에이전트가 보상을 얻으려면 행동을 해야한다. 특정 상태가 아닌 모든 상태에 대해 어떤 행동을 해야하는지 알아야 하는데 이렇게 모든 상태에 대해 에이전트가 어떤 행동을 해야하는지 정해 놓은 것 여기서 보상은 강화 학습이 다른 머신러닝 기법과 다르게 만들어주는 가장 핵심적인 요소이며 보상에는 즉각적 보상과 최종 보상 등이 있습니다. 바둑이 왜 인공지능의 위대한 도전인가? 바둑은 경우의 수가 매우 많습니다. 한 게임에서 평균적으로 바둑판에 둘 수 있는 점은 250개이며 게임이 평균 150수까지 진행된다고 할 때, 총 경우의 수느느 250의 150승입니다. 즉, 10의 360승개의 경우의 수가 있습니다. 이 뜻은 우주의 원자의 개수인 10의 80승보다도 많고 체스 10의 123승보다 많으며 비교도 할 수 없을 정도로 큰 경우의 수입니다. 아무리 슈퍼 컴퓨터를 수만대 동원하더라도 모든 경우의 수를 따져서 승리를 보장하는 최적의 수는 찾기 불가능합니다. 그렇기 때문에 강화학습으로 인공지능을 알린 것이 대단하다고 하는 것입니다. 바둑 프로그램은 사실 1960년대부터 연구를 했으나, 위의 경우의 수 때문에 5급 정도의 아마추어 수준에 머물었다고 합니다. 여기서 min/max 트리 알고리즘이라는 것이 나온 후 업그레이드가 되었지만 min/max 트리를 바둑의 트리로 표현하면 250의 150승 노드를 갖게 되므로 모든 노드를 탐색하는 것은 사실상 불가능합니다. 그리고 2008년에 몬테카를로 트리 탐색기법이 적용되면서 바둑 5단으로 비약적인 수준으로 상승했다고 합니다. 이세돌과 대결한 알파고의 원리 어떻게 알파고는 바둑을 학습했을까요 ? 알파고는 총 3단계에 거쳐서 학습을 했다고 알려져 있습니다. 바둑은 19x19이며, 흑백의 착수 순서만 입력해서 신경망을 만들었는데, 다음 수를 예측하는 이 신경망의 정확도가 55.7%가 나왔다고 합니다. 여기서 2단계에 강화학습을 입혔다고 합니다. 알파고는 강화학습을 통해 스스로 공부하며 깨우치는 학습 능력을 갖추게 되었습니다. 사람이 100만번의 대국을 치를려면 1000년 이상 걸린다고 하는데, 4주동안 한거면 정말 대단한것을 알 수 있습니다. 마지막으로 3단계는 몬테카를로 트리 탐색와 가치망입니다. 이 두가지 기법의 장단점을 보완하기 위해 위의 2가지 결과를 50%씩 반영하여 최종 착수 위치를 결정한다고 합니다. 이렇게 보니 강화학습이 정말 대단하고 알파고가 얼마나 똑똑한지 조금은 와닿는 것 같습니다.",
    "tags": "MachineLearning",
    "url": "/machinelearning/2020/03/16/python-minmax-dqn.html"
  },{
    "title": "네이버에서 이미지 크롤링하기",
    "text": "저번 포스팅에서는 신문사를 크롤링하는 것을 해봤는데, 이번에는 네이버에서 이미지를 크롤링하는 것을 해볼까합니다. 이미지 크롤링 이미지 크롤링할 때 필요한 모듈은 아래와 같습니다. import urllib.request from bs4 import BeautifulSoup from selenium import webdriver from selenium.webdriver.common.keys import Keys # 웹 애플리케이션의 테스트를 자동화하기 위한 프레임 워크 # 손으로 마우스 클릭해서 데이터를 검색하고 스크롤링 할 수 있다 import time # 중간마다 sleep를 걸어야 한다. 그리고 저번 신문사 크롤링과는 다르게 이번에 chromedriver가 필요하기 때문에 다운해야합니다. 여기에서 OS와 버전에 맞게 다운로드 해주세요. chromedriver는 크롬 웹 브라우저를 열기 위한 것이기 때문에 필요합니다. 그리고 네이버 이미지에서 검색창 id를 확인해주세요. id는 위 그림과 같이 nx_query임을 알 수 있으며, 이제 아래와 같이 코드를 구현할 수 있습니다. binary = 'chromedriver.exe' # 크롬 웹 브라우저를 열기 위한 크롬 드라이버 # 팬텀 js를 이용하면 백그라운드로 실행 할 수 있음. browser = webdriver.Chrome(binary) # 브라우저를 인스턴스화 browser.get(\"https://search.naver.com/search.naver?where=image&amp;amp;sm=stb_nmr&amp;amp;\") # 네이버의 이미지 검색 url elem = browser.find_element_by_id(\"nx_query\") # nx_query는 네이버 이미지 검색에 해당하는 input창 id 그리고 검색어를 입력과 웹에서 submit = enter의 역활을 하는 코드를 아래와 같이 작성합니다. elem.send_keys(\"아이언맨\") elem.submit() 이제 이미지 저장과 반복할 횟수 그리고 time sleep를 걸어놓습니다. time sleep을 거는 이유는 네트워크의 속도를 위해 5초의 sleep을 걸어놓습니다. 꼭 필요한 것은 아니지만 안정성이나 네트워크가 느려질 수 있으므로 걸어두는 것이 좋습니다. 또한 이미지 크롤링 하면서 페이지를 내리기 위해 body를 활성화하여 페이지를 내리겠습니다. # 스크롤링( 스크롤을 내리는 동작)을 반복할 횟수 for i in range(1, 2): browser.find_element_by_xpath(\"//body\").send_keys(Keys.END) # 웹창을 클릭 후 END키를 누르는 동작 # 브라우저 아무데서나 END키 누른다고 페이지가 내려가지 않음 # body를 활성화한 후 스크롤 동작 time.sleep(5) # 이미지가 로드 되는 시간 5초 # 로드가 되지 않은 상태에서 자장하기 되면 No image로 뜸. time.sleep(5) # 네크워크의 속도를 위해 걸어둔 sleep 이제 이미지를 다운 받아 로컬에 저장 하겠습니다. html = browser.page_source # 크롬 브라우저에서 현재 불러온 소스 코드를 가져옴 soup = BeautifulSoup(html, \"lxml\") # beautiful soup을 사용해서 html 코드를 검색할 수 있도록 설정 def fetch_list_url(): # 이미지를 url이 있는 img 태그의 img클래스로 감 params = [] imgList = soup.find_all(\"img\", class_=\"_img\") for im in imgList: # params 리스트 변수에 images url을 담음 params.append(im[\"src\"]) return params def fetch_detail_url(): params = fetch_list_url() a = 1 for p in params: # 다운받을 폴더경로 입력 urllib.request.urlretrieve(p, str(a) + \".jpg\") a = a + 1 fetch_detail_url() # 브라우저 창 닫기 browser.quit()",
    "tags": "Python",
    "url": "/python/2020/03/15/python-web6.html"
  },{
    "title": "파이썬으로 모든 신문사 웹스크롤링 스크립트 만들기",
    "text": "저번에 중앙일보와 한계례 신문사를 저번에 웹 스크롤링하는 것을 했습니다. 두번정도 해보니까 이제 슬슬 감이 잡히지 않던가요 ? 링크랑 기사내용 태그만 확인하면 스크롤링 되는 것을 확인 했으니, 이번엔 그냥 전체 신문사에서 선택해서 스크롤링하는 스크립트를 짜볼까합니다. 웹 스크롤링 함수 구현 우선 자신의 컴퓨터 user-agent를 확인해야합니다. 여기를 눌러 자신의 agent를 꼭 확인합니다. 우리는 메인 함수와 서브 함수 두가지를 우선 만들어야 합니다. 메인 함수는 스크롤링한 text를 리턴하는 함수를 만들고, 서브 함수는 두 가지 정도를 만들려고 합니다. 서브 함수는 기사 상세 url과 기사 text를 리스트를 append시키는 함수, 그리고 url를 입력받아 html로 변환하고 beautiful soup에서 사용할수 있도록 설정하는 함수를 만들겠습니다. 우선 서브 함수로 url를 받아서 html로 변화하는 함수를 만들겠습니다. import urllib.request as rq from bs4 import BeautifulSoup def start(url): headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36',} url = rq.Request(url, headers = headers) res = rq.urlopen(url).read() return BeautifulSoup(res, \"html.parser\") 그리고 기사 url과 기사 text를 리스트에 담는 함수를 작성합니다. def news_fetch(url, tag): soup = start(url) for link in soup.select(tag[1]): result.append(link.get_text()) 그리고 메인 함수인 스크롤링한 text를 리턴하는 함수를 작성합니다. def fetch_list_url(url, tag): global result soup = start(url) for link in soup.select(tag[0]): result.append(link.get_text()) print('link =' ,link['href'], link.get_text()) news_fetch(link['href'], tag) 이제 가장 중요한 url링크와 기사 내용을 스크롤링해야 할 태그 부분을 가져옵니다. result = [] url_list = ['http://search.daum.net/search?w=news&amp;q={search}&amp;spacing=0&amp;p={page}&amp;cp=16ZHEMAarrmZlVrZG3&amp;cpname=%EB%94%94%EC%A7%80%ED%84%B8%ED%83%80%EC%9E%84%EC%8A%A4', [\"#clusterResultUL &gt; li &gt; div.wrap_cont &gt; div &gt; div &gt; a\", \"#resizeContents &gt; div\"]] 여기서 이제 주소 끝 부분에 ‘디지털 타임즈’가 아니라 각종 신문사를 넣어 링크를 여러개 만들어 전체 신문사를 스크롤링 스크립트를 작성하려고 합니다. 전체 신문사 기사 스크롤링 신문사를 선택해서 스크롤링 하는 최종 코드는 아래와 같습니다. import urllib.request as rq from bs4 import BeautifulSoup from wordcloud import WordCloud, STOPWORDS import matplotlib.pyplot as plt from os import path import re def start(url): headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36',} url = rq.Request(url, headers = headers) res = rq.urlopen(url).read() return BeautifulSoup(res, \"html.parser\") def fetch_list_url(url, tag): global result soup = start(url) for link in soup.select(tag[0]): result.append(link.get_text()) print('link =', link['href'], link.get_text()) news_fetch(link['href'], tag) def news_fetch(url, tag): soup = start(url) for link in soup.select(tag[1]): result.append(link.get_text()) result = [] search_text = str(input(\"검색어를 입력하세요 : \").encode(\"utf-8\"))[2:-1].replace('\\\\x', '%') def numbers_to_strings(): num = input('1 : 전자신문 \\n2 : 디지털 타임즈 \\n3 : 경향신문 \\n4 : 중앙일보 \\n5 : 동아일보 \\n6 : 조선일보\\n') switcher = { 1: ['http://search.daum.net/search?w=news&amp;q={search}&amp;spacing=0&amp;p={page}&amp;cp=16yGc-mR1Rz5JT4-UZ&amp;cpname=%EC%A0%84%EC%9E%90%EC%8B%A0%EB%AC%B8&amp;DA=PGD', [\"#clusterResultUL &gt; li &gt; div.wrap_cont &gt; div &gt; div &gt; a\", \"#articleBody &gt; p\"]], 2: ['http://search.daum.net/search?w=news&amp;q={search}&amp;spacing=0&amp;p={page}&amp;cp=16ZHEMAarrmZlVrZG3&amp;cpname=%EB%94%94%EC%A7%80%ED%84%B8%ED%83%80%EC%9E%84%EC%8A%A4', [\"#clusterResultUL &gt; li &gt; div.wrap_cont &gt; div &gt; div &gt; a\", \"#resizeContents &gt; div\"]], 3: ['http://search.daum.net/search?w=news&amp;q={search}&amp;spacing=0&amp;p={page}&amp;cp=16bfGN9mQcFhOx4F5l&amp;cpname=%EA%B2%BD%ED%96%A5%EC%8B%A0%EB%AC%B8', [\"#clusterResultUL &gt; li &gt; div.wrap_cont &gt; div &gt; div &gt; a\", \"#container &gt; div.main_container &gt; div.art_cont &gt; div.art_body &gt; p\"]], 4: ['http://search.daum.net/search?nil_suggest=btn&amp;w=news&amp;cluster=y&amp;q={search}&amp;cp=16nfco03BTHhdjCcTS&amp;cpname=%EC%A4%91%EC%95%99%EC%9D%BC%EB%B3%B4&amp;p={page}',[\"#clusterResultUL &gt; li &gt; div.wrap_cont &gt; div.cont_inner &gt; div &gt; a \" ,\"#article_body\"]], 5: ['http://search.daum.net/search?w=news&amp;nil_search=btn&amp;enc=utf8&amp;cluster=y&amp;cluster_page=1&amp;q=AI&amp;cp=16Et2OLVVtHab8gcjE&amp;cpname={search}&amp;DA=PGD&amp;p={page}',[\"#clusterResultUL &gt; li &gt; div.wrap_cont &gt; div.cont_inner &gt; div &gt; a \" , \"div.article_txt \"]], 6: ['http://search.daum.net/search?w=news&amp;nil_search=btn&amp;enc=utf8&amp;cluster=y&amp;cluster_page=1&amp;q=AI&amp;cp=16EeZKAuilXKH5dzIt&amp;cpname={search}&amp;p={page}',[\"#clusterResultUL &gt; li &gt; div.wrap_cont &gt; div.cont_inner &gt; div &gt; a \",\"div.par\"]] } return switcher.get(int(num), \"nothing\") url_list = numbers_to_strings() for i in range(1, 2): url, tag = url_list[0].format(search=search_text, page=i), url_list[1] fetch_list_url(url, tag) print(result) f = open('data3.txt', 'w', encoding='UTF-8') f.writelines(result) f.close() 여기서 알아둬야 할 점은, 신문사 같은 경우는 웹 스크롤링하는 사람이 꽤 있다고 들어서 신문사들도 태그나 URL를 바꾸는 경우도 있다고 합니다. 그래서 현재 작성된 코드가 지금은 돌아가더라도 시간이 지난 뒤에는 안돌아갈 수 있으니 이 부분을 꼭 참고하셔서 작성하셔야합니다.",
    "tags": "Python",
    "url": "/python/2020/03/15/python-web5.html"
  },{
    "title": "J사 신문사 웹스크롤링 하기",
    "text": "저번에 H사 신문사를 웹 스크롤링해봤습니다. 신문사마다 주소가 다르기 때문에 이번에는 J사 주소를 가지고 와서 저번이랑 똑같이 웹 스크롤링을 해보도록 하겠습니다. J사 웹스크롤링 우선 중앙일보 신문사 링크를 가져와 보겠습니다. 저번과 마찬가지로 메모장에 복붙하면 암호화 된 주소 를 확인할 수있습니다. import urllib.request from bs4 import BeautifulSoup search_text = input(\"검색어를 입력하세요 : \").encode(\"utf-8\") search_text = str(search_text)[2:-1].replace('\\\\x', '%') list_url = \"hhttp://search.joins.com/JoongangNews?page=2&amp;Keyword=\" + search_text + \"&amp;SortType=New&amp;SearchCategoryType=JoongangNews\" 여기서 저번과 마찬가지로 페이지 번호를 확인하여 for을 통해 스크롤링하겠습니다. def fetch_list_url(): params = [] for i in range(2): list_url = \"https://news.joins.com/Search/JoongangNews?page=\" + str(i+1) + \"&amp;Keyword=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&amp;SortType=New&amp;SearchCategoryType=JoongangNews\" url = urllib.request.Request(list_url) res = urllib.request.urlopen(url).read().decode(\"utf-8\") soup = BeautifulSoup(res, \"html.parser\") for link in soup.find_all(class_=\"headline mg\"): for i in link: params.append(i.get('href')) # 신문자 URL return params 최종적으로 작성하면 아래와 같이 작성 할 수 있겠습니다. import urllib.request from bs4 import BeautifulSoup search_text = input(\"검색어를 입력하세요 : \").encode(\"utf-8\") search_text = str(search_text)[2:-1].replace('\\\\x', '%') list_url = \"hhttp://search.joins.com/JoongangNews?page=2&amp;Keyword=\" + search_text + \"&amp;SortType=New&amp;SearchCategoryType=JoongangNews\" def fetch_list_url(): params = [] for i in range(2): list_url = \"https://news.joins.com/Search/JoongangNews?page=\" + str(i+1) + \"&amp;Keyword=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&amp;SortType=New&amp;SearchCategoryType=JoongangNews\" url = urllib.request.Request(list_url) res = urllib.request.urlopen(url).read().decode(\"utf-8\") soup = BeautifulSoup(res, \"html.parser\") for link in soup.find_all(class_=\"headline mg\"): for i in link: params.append(i.get('href')) # 신문자 URL return params def fetch_list_url2(): params3 = [] list_url = fetch_list_url() for i in range(len(list_url)): url = urllib.request.Request(list_url[i]) res = urllib.request.urlopen(url).read().decode(\"utf-8\") soup = BeautifulSoup(res, \"html.parser\") params1 = [] params2 = [] for link1, link2 in zip(soup.find_all('div', class_=\"byline\"), soup.find_all('div', id=\"article_body\")): params1.append(link1.get_text()) params2.append(link2.get_text()) for i1, i2 in zip(params1, params2): params3.append(i1.strip()) params3.append(i2.strip()) return params3 f = open('ydata3.txt', 'w', encoding='UTF-8') f.writelines(fetch_list_url2()) f.close()",
    "tags": "Python",
    "url": "/python/2020/03/15/python-web4.html"
  },{
    "title": "H사 신문사 웹스크롤링 하기-3(검색어 입력받기)",
    "text": "저번 포스팅에서는 기사 제목과 내용을 스크롤링하는 것을 했는데, 이번에는 스크롤링 하려고 검색어을 입력받아 자동으로 스크롤링하는 것을 짜보겠습니다. H사 웹 스크롤링 저번에 가져온 암호화 된 주소를 살펴보면 아래와 같은데, 여기서 검색어를 삽입해야합니다. def han_article(): # 모듈 임포트 import urllib.request from bs4 import BeautifulSoup import re import os # 검색어 입력 search_text = input(\"검색어를 입력하세요 : \").encode(\"utf-8\") search_text = str(search_text)[2:-1].replace('\\\\x', '%') ##상세 기사 url url_list = [] for i in range(30): list_url = \"http://search.hani.co.kr/Search?command=query&amp;keyword=\"+search_text+\"&amp;media=news&amp;submedia=&amp;sort=d&amp;period=all&amp;datefrom=2020.01.01&amp;dateto=2020.03.22&amp;pageseq=\"+str(i) url = urllib.request.Request(list_url) # url 요청 res = urllib.request.urlopen(url).read().decode(\"utf-8\") # utf 파일로 decoding soup = BeautifulSoup(res, \"html.parser\") # 전체 html ##url_list에 url 전체 담기 for link in soup.find_all('dt'): # dt 태그에 해당하는 모든 부분 for i in link: # resultset를 태그로 만드는 작업 #dt 태그 밑에 있는 a 태그를 가져오기 위해 for loop url_list.append(i.get('href')) # 태그가 되어야 get, get_text()를 쓸 수 있다. ##상세 기사 full_article = [] # url 하나씩 불러오기 for i in range(len(url_list)): url = urllib.request.Request(url_list[i]) res = urllib.request.urlopen(url).read().decode(\"utf-8\") # print(res) # 위의 두가지 작업을 거치면 # 위의 url 의 html 문서를 res 변수에 담을수 있게 된다. soup = BeautifulSoup(res, \"html.parser\") day = [] article = [] for link1, link2 in zip(soup.find_all('p', class_=\"date-time\"), soup.find_all('div', class_=\"text\")): day.append(link1.get_text()) article.append(link2.get_text()) for i, j in zip(day, article): full_article.append(i.strip()) full_article.append(j.strip()) f = open('han_article.txt', 'w', encoding='UTF-8') f.writelines(full_article) f.close() han_article() 스크롤링이 잘되는 것을 확인 할 수 있는데, 여기서 특정 기간만 선택해서 스크롤링하고 싶다면 위 그림을 참고해서 수정하면 될 것 같네요.",
    "tags": "Python",
    "url": "/python/2020/03/15/python-web3.html"
  },{
    "title": "H사 신문사 웹스크롤링 하기-2(기사 내용)",
    "text": "지난 포스팅에서 기사 제목만을 스크롤링하는 것을 했는데, 이번에는 조금 더 나아가서 기사 내용까지 스크롤링해보도록 하겠습니다. H사 웹 스크롤링 (기사 내용 스크롤링) 기사 내용을 스크롤링하기 위해 기사를 클릭 후 저번과 마찬가지로 F12를 눌러 기사 내용을 클릭합니다. 그럼 기사 내용이 div 태그에 text 클래스에 기사 내용이 있음을 확인할 수 있습니다. 여기서 기사 내용만을 클릭하면 언제 올라온 기사인지 모르니 기사 날짜도 함께 스크롤링합니다. 기사 날짜는 p 태그에 data-time 클래스가 있습니다. import urllib.request from bs4 import BeautifulSoup def fetch_list_url(): # 현재 기사 URL list_url = \"http://www.hani.co.kr/arti/economy/marketing/933198.html url = urllib.request.Request(list_url) res = urllib.request.urlopen(url).read().decode(\"utf-8\") #print(res) # 위의 두가지 작업을 거치면 위의 url 의 html 문서를 res 변수에 담을수 있게 된다. soup = BeautifulSoup(res, \"html.parser\") params1 =[] params2 =[] for link1,link2 in zip(soup.find_all('p', class_=\"date-time\"),soup.find_all('div', class_=\"text\")): params1.append( link1.get_text() ) params2.append( link2.get_text() ) for i1,i2 in zip(params1,params2): print(i1.strip(),end=' ') print(i2.strip()) fetch_list_url() 그럼 여기서 현재 기사 URL만 스크롤링하지 않고 아래의 두가지 조건을 만족하는 웹 스크롤링을 작성해보겠습니다. 1. 인공지능으로 검색한 페이지의 기사 URL을 가져오는 코드 2. 그 상세 URL로 기사 날짜와 기사 내용을 검색하는 코드 저번 포스팅에서 사용한 코드와 오늘 작성한 코드를 합치면 간단하게 만들 수 있습니다. import urllib.request from bs4 import BeautifulSoup def fetch_list_url(): params=[] for i in range(50): list_url = \"http://search.hani.co.kr/Search?command=query&amp;keyword=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&amp;media=news&amp;submedia=&amp;sort=d&amp;period=all&amp;datefrom=2000.01.01&amp;dateto=2020.03.22&amp;pageseq=\"+str(i) url = urllib.request.Request(list_url) res = urllib.request.urlopen(url).read().decode(\"utf-8\") soup = BeautifulSoup(res,\"html.parser\") for link in soup.find_all('dt'): for i in link: params.append(i.get('href')) return params def fetch_list_url2(): list_url = fetch_list_url() # 리스트를 한번에 받아오기 for i in range(len(list_url)): url = urllib.request.Request(list_url[i]) res = urllib.request.urlopen(url).read().decode(\"utf-8\") soup = BeautifulSoup(res, \"html.parser\") params1 =[] params2 =[] for link1,link2 in zip(soup.find_all('p', class_=\"date-time\"),soup.find_all('div', class_=\"text\")): params1.append( link1.get_text() ) params2.append( link2.get_text() ) for i1,i2 in zip(params1,params2): print(i1.strip(),end=' ') print(i2.strip()) print(fetch_list_url2()) 추가로 여기서 스크롤링한 기사를 메모장에 저장하고 난 최종 코드는 아래와 같이 작성할 수 있습니다. import urllib.request from bs4 import BeautifulSoup def fetch_list_url(): txt_list = [] for i in range(20): list_url = \"http://search.hani.co.kr/Search?command=query&amp;keyword=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&amp;media=news&amp;submedia=&amp;sort=d&amp;period=all&amp;datefrom=2000.01.01&amp;dateto=2020.03.22&amp;pageseq=\"+str(i) url = urllib.request.Request(list_url) res = urllib.request.urlopen(url).read().decode(\"utf-8\") soup = BeautifulSoup(res, \"html.parser\") article_title_url = soup.find_all('dt') for link in article_title_url: for i in link: article_list_url = i.get('href') article_url = urllib.request.Request(article_list_url) article_res = urllib.request.urlopen(article_url).read().decode(\"utf-8\") soup = BeautifulSoup(article_res, \"html.parser\") article_content = soup.find_all('div', class_=\"text\") article_date = soup.find_all('p', class_=\"date-time\") for i, j in zip(article_date,article_content): txt_list.append(i.get_text(' ', strip = True)+' ') txt_list.append(j.get_text(' ', strip = True).replace('\\n',' ')+'\\n') return txt_list f = open('data.txt','w',encoding=\"UTF-8\") f.writelines(fetch_list_url()) f.close() 다음 포스팅에서는 검색어를 입력하면 자동으로 스크롤링하게 해보겠습니다.",
    "tags": "Python",
    "url": "/python/2020/03/15/python-web2.html"
  },{
    "title": "H사 신문사 웹스크롤링 하기-1(기사 제목)",
    "text": "워드 클라우드나 특정 데이터를 수집하기 위해 자주 사용되는 웹 스크롤링에 대해 한번 알아보겠습니다. 보통 웹 스크롤링 할때는 a태그에 있는 것을 긁어서 사용합니다. 저는 오늘 특정 신문사에서 기사를 긁어오는 것을 한번 해보겠습니다. H사 웹 스크롤링 (기사 제목만 스크롤링) 신문사 홈페이지로 들어가서 찾고자 하는 것을 검색한 후 url을 가져오겠습니다. 여기서 알아야 할 점은 주소가 위 그림처럼 뜨지만, 주소를 메모장에 복붙하면 아래처럼 암호화 된 주소가 보여집니다. 그렇기 때문에 직접 URL를 건들어서 웹 스크롤링을 하기는 힘들다는 점을 알아두셔야합니다. 이제 여기서 우리가 해야 댈것은 페이지 번호를 확인하고 나서 for문을 돌려서 웹 스크롤링 할 예정입니다. 우선 F12를 눌러 기사 제목을 클릭해서 기사 제목을 확인해보겠습니다. F12를 누른 후 ctrl+ shift+c를 누른 후 기사 제목을 클릭하고 dt 태그 밑에 있는 a태그에서 링크와 기사 제목이 있는지를 확인합니다. 우리는 기사 제목을 스크롤링 해야하므로 a태그를 스크롤링해야합니다. 확인했으면 이제 페이지 번호를 확인하기 위해서 다음 페이지로 넘어갑니다. 주소를 보면 pageseq 라는 것이 생기면서 URL주소가 바뀌는 것을 확인할 수 있습니다. 그럼 이제 준비는 끝났습니다. 페이지 번호가 들어간 URL과 기사 제목이 어디에 위치 했는지를 확인했으니 웹 스크롤링을 할수 있겠네요. import urllib.request from bs4 import BeautifulSoup # 웹 스크롤링을 위한 모듈 def fetch_list_url(): for i in range(100): list_url = \"http://search.hani.co.kr/Search?command=query&amp;keyword=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&amp;media=news&amp;submedia=&amp;sort=d&amp;period=all&amp;datefrom=2000.01.01&amp;dateto=2020.03.22&amp;pageseq=\"+str(i) url = urllib.request.Request(list_url) res = urllib.request.urlopen(url).read().decode(\"utf-8\") soup = BeautifulSoup(res, \"html.parser\") for link in soup.find_all('dt'): # dt 태그 밑에 있는 for i in link: # a 태그를 가져오기 print(i.get('href')) # 기사 URL print(i.get_text('href')) # 기사 제목 fetch_list_url() 웹 스크롤링이 잘되는 것을 확인할 수있습니다. 다음 포스팅에서는 신문사 기사 내용도 스크롤링하고 저장하는 것을 해보겠습니다.",
    "tags": "Python",
    "url": "/python/2020/03/15/python-web1.html"
  },{
    "title": "2017 카카오 신입 공채 블라인드 시험문제(LRU알고리즘)",
    "text": "두번째 카카오 알고리즘을 풀어보겠습니다. LRU알고리즘(난이도 하) 지도개발팀에서 근무하는 제이지는 지도에서 도시 이름을 검색하면 해당 도시와 관련된 맛집 게시물들을 데이터베이스에서 읽어 보여주는 서비스를 개발하고 있다. 이 프로그램의 테스팅 업무를 담당하고 있는 어피치는 서비스를 오픈하기 전 각 로직에 대한 성능 측정을 수행하였는데, 제이지가 작성한 부분 중 데이터베이스에서 게시물을 가져오는 부분의 실행시간이 너무 오래 걸린다는 것을 알게 되었다. 어피치는 제이지에게 해당 로직을 개선하라고 닦달하기 시작하였고, 제이지는 DB 캐시를 적용하여 성능 개선을 시도하고 있지만 캐시 크기를 얼마로 해야 효율적인지 몰라 난감한 상황이다. 어피치에게 시달리는 제이지를 도와, DB 캐시를 적용할 때 캐시 크기에 따른 실행시간 측정 프로그램을 작성하시오. 입력 형식 캐시 크기(cacheSize)와 도시이름 배열(cities)을 입력받는다. cacheSize는 정수이며, 범위는 0 ≦ cacheSize ≦ 30 이다. cities는 도시 이름으로 이뤄진 문자열 배열로, 최대 도시 수는 100,000개이다. 각 도시 이름은 공백, 숫자, 특수문자 등이 없는 영문자로 구성되며, 대소문자 구분을 하지 않는다. 도시 이름은 최대 20자로 이루어져 있다. 출력 형식 입력된 도시이름 배열을 순서대로 처리할 때, “총 실행시간”을 출력한다. 조건 캐시 교체 알고리즘은 LRU(Least Recently Used)를 사용한다. cache hit일 경우 실행시간은 1이다. cache miss일 경우 실행시간은 5이다. 입출력 예제 size=int(input('크기를 입력하세요 ')) put = [y.lower() for y in input('도시이름을 입력하세요 ').split(',')] my_cache=[] cnt=0 for i in put: if i in my_cache: my_cache.append(i) cnt+=1 else: my_cache.append(i) cnt+=5 if size &lt; len(my_cache): del my_cache[0] elif size == 0: my_cache.append(i)",
    "tags": "Algorithm",
    "url": "/algorithm/2020/03/15/algorithm-kakao2.html"
  },{
    "title": "2017 카카오 신입 공채 블라인드 시험문제(비밀지도)",
    "text": "옛날 카카오 블라인드 알고리즘 문제를 한번 오랜만에 풀어볼까합니다. 비밀지도(난이도 하) 네오는 평소 프로도가 비상금을 숨겨놓는 장소를 알려줄 비밀지도를 손에 넣었다. 그런데 이 비밀지도는 숫자로 암호화되어 있어 위치를 확인하기 위해서는 암호를 해독해야 한다. 다행히 지도 암호를 해독할 방법을 적어놓은 메모도 함께 발견했다. 지도는 한 변의 길이가 n인 정사각형 배열 형태로, 각 칸은 “공백”(“ “) 또는 “벽”(“#”) 두 종류로 이루어져 있다. 전체 지도는 두 장의 지도를 겹쳐서 얻을 수 있다. 각각 “지도 1”과 “지도 2”라고 하자. 지도 1 또는 지도 2 중 어느 하나라도 벽인 부분은 전체 지도에서도 벽이다. 지도 1과 지도 2에서 모두 공백인 부분은 전체 지도에서도 공백이다. “지도 1”과 “지도 2”는 각각 정수 배열로 암호화되어 있다. 암호화된 배열은 지도의 각 가로줄에서 벽 부분을 1, 공백 부분을 0으로 부호화했을 때 얻어지는 이진수에 해당하는 값의 배열이다. 네오가 프로도의 비상금을 손에 넣을 수 있도록, 비밀지도의 암호를 해독하는 작업을 도와줄 프로그램을 작성하라. 입력 형식 입력으로 지도의 한 변 크기 n 과 2개의 정수 배열 arr1, arr2가 들어온다. 1 ≦ n ≦ 16 arr1, arr2는 길이 n인 정수 배열로 주어진다.정수 배열의 각 원소 x를 이진수로 변환했을 때의 길이는 n 이하이다. 즉, 0 ≦ x ≦ 2^n - 1을 만족한다. 출력 형식 원래의 비밀지도를 해독하여 “#”, 공백으로 구성된 문자열 배열로 출력하라. num = int(input('n 값을 입력하세요 ')) arr1 = [int(x) for x in input('1번 지도를 입력하세요 ').split(',')] arr2 = [int(y) for y in input('2번 지도를 입력하세요 ').split(',')] total=[] for i in arr1: for j in arr2: a=bin(i | j) a = a.replace('0b','') if num &gt; len(a): a = (num-len(a))*str('0')+str(a) a = a.replace('1','#') a = a.replace('0',' ') total.append(a) else: a = a.replace('1','#') a = a.replace('0',' ') total.append(a) if num != 0: print(total[::num+1]) 조금 더 정성스럽게 풀엇다면 간단하고 이쁜 코드가 나왔겠지만….. 일단 대충합시다..",
    "tags": "Algorithm",
    "url": "/algorithm/2020/03/15/algorithm-kakao1.html"
  },{
    "title": "파이썬에서 언더바(_)를 사용하는 경우",
    "text": "파이썬 코드를 가끔 보다보면 _ 같은것을 볼 수 있습니다. 이 언더바(_)는 언제 사용하는 걸까요? 언더바(_)를 사용하는 경우 언더바를 사용하는 경우는 주로 아래의 4가지 경우입니다. 인터프리터에서 마지막 값을 저장할 때 값을 무시하고 싶을 때 변수나 함수명에 특별한 의미를 부여하고 싶을 때 숫자 또는 문자값의 자릿수 구분을 위한 구분자로써 사용할 때 우선 첫번째, 인터프리터에서 마지막 값을 저장 할때 사용하는데, 아래와 같이 사용할 수 있습니다. 그래서 이런 출력도 가능합니다. 두번째는 값을 무시하고 싶을 때 사용하는데, for 문을 사용할 때 가장 많이 사용됩니다. 2를 무시하고 값이 출력되는 것을 알 수 있으며, for 문을 사용하면 아래와 같이 사용할 수있습니다. 세번짼 변수나 함수명에 특별한 의미를 부여하고 싶을 때 사용합니다. 패키지를 만들기 위해 폴더 안에 init 파일 만들때 사용합니다. 또한 함수명에 언더바를 붙이는 것에 따라 두가지 의미를 가지 수 있습니다. class Message: def __init__(self, msg): self.msg=msg def __repr__(self): return \"Message: %s\" %self.msg ‘_‘이 앞에 붙으면 외부 사용자는 사용하지말라는 권유의 문법입니다. ‘__‘이 앞에 붙으면 private이 되어 외부에서 사용할 수 없고, 다른 클래스에서 사용하거나 override 할 수 없습니다. __를 붙인 함수와 안붙인 함수는 두가지 차이점을 가지고 있습니다. 이 클래스 파일을 다른 파일에 import 할 때, __가 붙은 함수는 import가 되지 않습니다. 클래스를 인스턴스화 하고 함수를 호출할때, 객체명만 사용하고 함수이름을 쓰지 않아도 실행을 할 수 있습니다. 마지막으로 언더바는 숙자 또는 문자값의 자릿수 구분을 위한 구분자로써 사용할 수 있는데, 아래와 같이 사용할 수 있습니다. dec_base= 1_000_000 print(dec_base)",
    "tags": "Python",
    "url": "/python/2020/03/14/python_val_pass.html"
  },{
    "title": "파이썬의 클래스 상속과 super함수",
    "text": "다른 언어와 마찬가지로 파이썬에도 상속 개념이 존재합니다. 클래스 상속을 사용하게 되면 반복적으로 코드를 짜지 않어도 되기 때문에 많이 사용합니다. 상속 상속은 클래스들끼리 기능을 서로에게 물려주는 것이기 때문에 같은 코드를 반복해서 짜지 않아도 됩니다. 상속을 받기 위해 클래스 두개를 작성하고 자식 클래스에 상속시켜보겠습니다. class father: def base_method(self): print(\"hello~\") class child(father): pass father=father() father.base_method() child=child() child.base_method() 코드를 보면 자식 클래스가 아빠 클래스를 상속을 받았는데, 이상태로 출력을 하면 아래와 같은 결과를 얻을 수 있습니다. 결과를 보면 알수 있듯이 child class에는 함수를 쓰지 않았지만 클래스를 상속 받았으므로 father의 함수를 사용할 수 있습니다. 그러면 여기서 father에 속성이 하나 더 생긴다면 child는 어떻게 될까요 ? class father: def __init__(self): print(\"hello~\") self.message=\"Good Morning\" class child(father): def __init__(self): father print (\"hello~~ I am Child\") father=father() father.message father는 출력이 잘 되지만, child를 출력하면 에러가 출력됩니다. 이 오류를 해결하기 위해선 우선 class와 변수명 대소문자를 구분하여 작성해야 합니다. class Father: def __init__(self): print(\"hello~\") self.message=\"Good Morning\" class Child(Father): def __init__(self): Father.__init__(self) # 추가된 부분 # Father의 __init__을 실행하겠다는 명령어 print (\"hello~~ I am Child\") father1=Father() print(father1.message) child1=Child() print(child1.message) 그러나 파이썬은 암묵적인것을 싫어하고 명시적인 것을 좋아하는 성격이 있는데 위와 같이 father.init(self)라고 해도 되지만 파이썬의 내장 함수 중 super()를 사용하여 아래와 같이 사용할 수 있습니다. class Father: def __init__(self): print(\"hello~\") self.message=\"Good Morning\" class Child(Father): def __init__(self): #Father.__init__(self) #추가된 부분 super().__init__() #super() print (\"hello~~ I am Child\") 사실 super를 사용하는 것을 권장하는데, 그 이유는 상속시에 나타나는 가장 큰 문제점을 해결할 수 있기 때문입니다. 다중 상속, 죽음의 다이아몬드 상속에 가장 큰 문제점은 다중 상속 시에 나타나는 죽음의 다이아몬드 때문입니다. 다중 상속 다중 상속이란, 두 개 이상의 클래스를 상속 받는 것을 이야기 합니다. 이 경우에는 두 클래스의 모든 속성을 물려받게 됩니다. 이는 하나의 자식 클래스가 두 개 이상의 부모 클래스를 가지는 경우를 이야기 할 수 있습니다. 예를 들면 아래와 같습니다. class Grandfather: def __init__(self): print('튼튼한 두팔') class Father1(Grandfather): def __init__(self): Grandfather.__init__(self) print('지식') class Father2(Grandfather): def __init__(self): Grandfather.__init__(self) print('지혜') class Grandchild(Father1, Father2): def __init__(self): Father1.__init__(self) Father2.__init__(self) print('자기 만족도가 높은 삶') grandchild=Grandchild() 만약 튼튼한 두팔, 지혜, 지식, 자기 만족도가 높은 삶을 출력한다고 했을 때, 실제로 출력 되는 것을 확인해보면 튼튼한 두팔이 두번 나오는 경우를 확인 할 수 있습니다. 이것이 다중 상속시 볼 수 있는 죽음의 다이아몬드 상속입니다. 이를 해결하기 위해서 super 함수를 사용해야 이 문제를 피할 수 있습니다. class Grandfather: def __init__(self): print('튼튼한 두팔') class Father1(Grandfather): def __init__(self): super().__init__() print('지식') class Father2(Grandfather): def __init__(self): super().__init__() print('지혜') class Grandchild(Father1, Father2): def __init__(self): super().__init__() print('자기 만족도가 높은 삶')",
    "tags": "Python",
    "url": "/python/2020/03/14/python-father-child.html"
  },{
    "title": "파이썬 static method (정적 메소드)",
    "text": "정적 메소드를 사용하여 클래스가 같은 메모리를 바라보는 것을 해보도록 하겠습니다. 정적 메소드 (static method) 정적 메소드는 self를 매개변수로 받지 않는 메소드를 말하며 여러 인스턴스가 공유해서 사용하는 메소드입니다. 저번에 만든 코드를 이용하여 예를 들어보겠습니다. class gun(): # 클래스가 실체화 될때, 바로 작동하는 메소드(함수) def __init__(self): # 총을 만든다. (실체화) # 총알을 0으로 설정 self.bullet = 0 # class에는 꼭 self가 들어간다. def charge(self, num):#충전하는 기능 self.bullet=num def shoot(self, num): #쏘는 기능 for i in range(num): if self.bullet&gt;0: print('탕 !') self.bullet-= 1 elif self.bullet == 0: print('총알이 없습니다.') break def print(self): #출력하는 기능 print('{}발 남았습니다.'.format(self.bullet)) 이 클래스에서 총을 두개를 만들면 아래와 같습니다. #gun_1 gun_1=gun() gun_1.charge(10) gun_1.shoot(3) gun_1.print() #gun_2 gun_2=gun() gun_2.charge(10) gun_2.shoot(6) gun_2.print() 여기서 gun_1과 gun_2는 서로 별개의 총입니다. 같은 클래스로 만들었지만 별개의 총이죠. 실제로 print를 찍어 확인해보면 메모리 주소가 다른 것을 확인 할 수 있습니다. 그런데, 여기서 static method를 사용해서 클래스를 다시 만들면 아래와 같습니다. class Gun(): bullet=0 #class 변수 선언 @staticmethod #static method 사용 선언 def charge(num): #충전 Gun.bullet=num @staticmethod def shoot(num): #발포 for i in range(num): if Gun.bullet&gt;0: print('탕 !') Gun.bullet-= 1 elif Gun.bullet == 0: print('총알이 없습니다.') break @staticmethod def print(): #출력하는 기능 print('{}발 남았습니다.'.format(Gun.bullet)) 그리고 아까와 똑같이 만드는데, gun_2에는 총알을 장전하지 않고 총을 쏘게 되면 아래와 같은 결과를 얻을 수 있습니다. 결과를 보면 다른 총이지만 장전된 총알을 공유하는 것을 알 수 있습니다. 즉 , gun_2는 장전은 하지 않았지만 발포가 가능합니다. 월래라면 에러가 나야 하지만 메모리를 공유하기 때문인데 총알을 공유하는 것입니다. print을 찍어보면 메모리를 같음을 알 수 있습니다.",
    "tags": "Python",
    "url": "/python/2020/03/14/python-class-static.html"
  },{
    "title": "파이썬으로 탐욕 알고리즘 구현하기(동전)",
    "text": "탐욕 알고리즘은 당장 눈앞의 이익만 추구하는 것을 이야기 하며, 먼 미래를 내다 보지 않고 지금 당장의 최선이 무엇인가를 판단하는 알고리즘입니다. 그럼 탐욕 알고리즘을 이용하여 금액과 화폐가 주어졌을 때 가장 적은 화폐로 지불하는 것을 구현해도록 하겠습니다. 예를 들어 액수를 362 라고 입력하고 화폐단위를 1,50,100 이라고 입력했을 때 결과 값이 100원 3개, 50원 1개, 1원 12개의 값이 나와야 합니다. def coinGreedy(): money = int(input('액수입력 : ')) cash_type = [int(x) for x in input('화폐단위를 입력하세요 : ').split(' ')] cash_type=sorted(cash_type, reverse=True) coin={} for i in cash_type: cnt=0 while money &gt;= i: money=money-i cnt += 1 coin[i]=cnt for key in coin: print('{0}원 : {1}개'.format(key,coin[key])) coinGreedy() 구현하면 위와 같이 구현할 수 있고 탐욕스럽게 잘 나오고 있는 것을 알 수 있습니다.",
    "tags": "Algorithm",
    "url": "/algorithm/2020/03/14/algorithm-coin-greedy.html"
  },{
    "title": "파이썬으로 버블정렬 알고리즘 구현하기",
    "text": "버블정렬은 두 인접한 원소를 검사하여 정렬하는 방법입니다. 파이썬으로 구현하자면 아래와 같이 구현할 수 있습니다. def bubble_search(data): for i in range(len(data)): for j in range(len(data)-1): if data[j] &gt; data[j+1]: (data[j],data[j+1]) = (data[j+1],data[j]) print('i=',i,'j=',j,data) print(data) data = [5,4,3,2,1,8,7,10] bubble_search(data) 여기서 버블 정렬을 재귀 함수로 구현한다면 아래와 같이 구현할 수 있습니다. def bubble_search(data): for i in range(len(data)-1): if data[i] &gt; data[i+1]: a = data[i] data[i] = data[i+1] data[i+1] = a bubble_search(data) return data data = [5,4,3,2,1,8,7,10] print(data) print(bubble_search(data))",
    "tags": "Algorithm",
    "url": "/algorithm/2020/03/14/algorithm-bubble_search.html"
  },{
    "title": "검색 알고리즘? 파이썬으로 이진탐색 구현하기",
    "text": "이진 검색 알고리즘(binary search algorithm)은 오름차순으로 정렬된 리스트에서 특정한 값의 위치를 찾는 알고리즘입니다. 처음 중간의 값을 임의의 값으로 선택하여, 그 값과 찾고자 하는 값의 크고 작음을 비교하는 방식입니다. data = [ 1, 7, 11 , 12, 14 , 23, 33, 47, 51, 64, 67, 77, 130, 672, 871 위와 같은 데이터가 있을 때, 이진 탐색을 구현하면 아래와 같습니다. def binary_search(in_data, input_num): in_data = sorted(in_data) start_num = 0 end_num = len(in_data) - 1 while start_num &lt;= end_num: mid_num = int((start_num + end_num) / 2) if in_data[mid_num] == input_num: return print('입력하신 {0}이(가) 있습니다.'.format(input_num)) elif in_data[mid_num] &gt; input_num: end_num = mid_num - 1 elif in_data[mid_num] &lt; input_num: start_num = mid_num + 1 return print('입력하신 {0}이(가) 없습니다.'.format(input_num)) 데이터의 중간의 값을 선택하고 값의 크고 작음을 비교하는 방식이기 때문에 if문을 사용하면 간단히 구현할 수 있습니다.",
    "tags": "Algorithm",
    "url": "/algorithm/2020/03/14/algorithm-binary_search.html"
  },{
    "title": "파이썬 모듈(import)사용하기",
    "text": "파이썬에서 모듈(import)을 이해하고 사용해보도록 하겠습니다. 모듈(import) 이란? 이미 만들어져 있고 안정성이 검증된 함수들을 성격에 맞게 하나의 파일로 묶어놓은 것들을 모듈이라고 합니다. 외부의 모듈을 사용하려면 이 모듈을 먼저 코드로 가져와서 자유롭게 사용할 수 있도록 해야하는데 이런일을 파이썬에서는 모듈을 import라고 합니다. import time print('5초간 프로그램을 정지합니다 ') time.sleep(5) 또는 아래와 같이 사용할 수 있습니다. 같은 위치에 test3.py 라는 이름으로 아래의 코드를 생성하면 사용할 수 있습니다. 메인 모듈과 하위 모듈 name(내장 전역변수)를 이용하면 지금 쓰고 있는 모듈이 하위 모듈인지를 확인 할 수 있습니다. 메인 모듈의 경우 __name__를 사용하면 main이라고 나오며 하위모듈의 경우 __name__을 이용하여 출력을 하면 모듈명이 출력됩니다.",
    "tags": "Python",
    "url": "/python/2020/03/13/python-import.html"
  },{
    "title": "파이썬 클래스(class)란?",
    "text": "파이썬에서의 클래스 정의와 구현하는 법에 대해 알아보겠습니다. 클래스(class) 사실 클래스(class)하면 가장 많이 비유로 사용되는 붕어빵 틀과 붕어빵이 있는데, 이 부분은 그냥 패스하고 직접 구현하면서 알아보도록 하겠습니다. 우선 총이라는 클래스를 만들어 보겠습니다. class gun(): # 클래스가 실체화 될때, 바로 작동하는 메소드(함수) def __init__(self): # 총을 만든다. (실체화) # 총알을 0으로 설정 self.bullet = 0 위에 있는 class가 기본 구조 입니다. 여기서 총알을 충전하는 기능을 추가해보겠습니다. 총알 충전해야 하니까 숫자를 입력 받아야겠죠? 그리고 class에서는 꼭 self가 들어가기 때문에 클래스내에 함수를 사용할 경우 앞에 self를 붙여줘야 합니다. class gun(): # 클래스가 실체화 될때, 바로 작동하는 메소드(함수) def __init__(self): # 총을 만든다. (실체화) # 총알을 0으로 설정 self.bullet = 0 # class에는 꼭 self가 들어간다. def charge(self, num):#충전하는 기능 self.bullet=num 총알을 장전했으면 쏘는 기능을 만들어야 합니다. for문을 이용하여 총알을 탕탕! 쏘는 기능을 구현해보도록 하겠습니다. class gun(): # 클래스가 실체화 될때, 바로 작동하는 메소드(함수) def __init__(self): # 총을 만든다. (실체화) # 총알을 0으로 설정 self.bullet = 0 # class에는 꼭 self가 들어간다. def charge(self, num):#충전하는 기능 self.bullet=num def shoot(self, num): #쏘는 기능 for i in range(num): if self.bullet&gt;0: print('탕 !') self.bullet-= 1 elif self.bullet == 0: print('총알이 없습니다.') break 마지막으로 총알이 몇발 남앗는지 출력하는 간단한 print 함수를 만들어보도록 하겠습니다. class gun(): # 클래스가 실체화 될때, 바로 작동하는 메소드(함수) def __init__(self): # 총을 만든다. (실체화) # 총알을 0으로 설정 self.bullet = 0 # class에는 꼭 self가 들어간다. def charge(self, num):#충전하는 기능 self.bullet=num def shoot(self, num): #쏘는 기능 for i in range(num): if self.bullet&gt;0: print('탕 !') self.bullet-= 1 elif self.bullet == 0: print('총알이 없습니다.') break def print(self): #출력하는 기능 print('{}발 남았습니다.'.format(self.bullet)) 클래스를 만들었으니 이제 클래스를 실행해봐야겠죠 ? # 총을 실체화시킨다. gun=gun() # 총알을 장전한다. gun.charge(10) # 총을 쏜다. gun.shoot(3) # 몇 발 남았는지 확인한다. gun.print()",
    "tags": "Python",
    "url": "/python/2020/03/13/python-class.html"
  },{
    "title": "파이썬 패키지(package)란?",
    "text": "파이썬에서 패키지 사용하는 법에 대해 알아보겠습니다. 패키지(package) 우리가 음악 파일을 저장할때도 장르별로 폴더를 만들어서 별도로 저장하듯이 파이썬 모둘도 음악처럼 갯수가 많아지면 폴더(모듈 꾸러미) 별로 관리를 해야 관리가 편해지는데 이 폴더(디렉토리)가 패키지입니다. 평범한 폴더가 패키지로 인정을 받으려면 반드시 갖고 있어야 하는 파일은 __init__파일입니다. __init__파일은 대게 비워두는 것이 보통인데 이 파일을 손데는 경우는 __all__변수를 조정할 때 사용합니다. 즉, 패키지로부터 반입할 목록을 정의할 때 사용합니다. 그래서 실제로 적용해서 사용하자면 아래와 같이 사용할 수 있습니다. import my_loc.cal_test3 #패키지 안의 모듈을 불러온 뒤 print(my_loc.cal_test3.plus(10,7)) #함수를 실행 site-package site-packages란, 파이썬의 기본 라이브러리 패키지외에 추가적인 패키지를 설치하는 디렉토리입니다. 이 디렉토리에 여러가지 소프트웨어가 사용할 공통 모듈을 넣어두면 물리적인 장소에 구애받지 않고 모듈이 접근하여 반입할 수 있습니다. 즉, 모듈을 site-packages에 넣어두면 무조건 실행할 수 있습니다.",
    "tags": "Python",
    "url": "/python/2020/03/13/python-package.html"
  },{
    "title": "파이썬 멤버체크(in)으로 특정 단어 개수 세기",
    "text": "파이썬에서 특정 단어가 있는지 확인하기 위해 자주 쓰이는 함수 in입니다. 아마 for 사용할때 더 많이 사용해본 적 있을꺼예요. 이 맴버 함수에 대해 자세히 알아보겠습니다. list안에 있는지 없는지 확인하기 in을 사용하여 list안에 특정 숫자나 문자열이 있는지 체크해보겠습니다. listdata =[1, 2, 3, 4] ret1 = 5 in listdata # False ret2 = 4 in listdata # True print(ret1); print(ret2) strdata = 'abcde' ret3 = 'c' in strdata # True ret4 = '1' in strdata # False print(ret3); print(ret4) 텍스트(.txt)에서 특정 단어 및 라인 출력하기 겨울왕국 데이터를 사용하여 겨울왕국에서 elsa가 몇번 나오는지 확인하기 위해 우선 한라인씩 list변수에 담아보겠습니다. 겨울왕국 데이터는 여기에서 다운 받으실 수 있습니다. file=open(\"winter.txt\",\"r\") for winter_list in file: a = winter_list.split(' ') print(a) 이번엔 라인이 아니라 for문을 중첩시켜 겨울왕국의 단어들만 출력해보겠습니다. file=open(\"winter.txt\",\"r\") for winter_list in file: a = winter_list.split(' ') for b in a: print(b) 여기서 count 함수를 사용해서 elsa가 나오면 1씩 카운트 해보면 아래와 같이 쓸수 있습니다. file=open(\"winter.txt\",\"r\") sum=0 for winter_list in file: a = winter_list.split(' ') for b in a: c = b.lower().count('elsa') sum += c print(sum)",
    "tags": "Python",
    "url": "/python/2020/03/12/python-in-winter.html"
  },{
    "title": "파이썬에서 지역변수와 전역변수 사용하기",
    "text": "이번에 파이썬에서 지역변수와 전역변수를 사용하는 것에 대해 배워보겠습니다. 지역변수와 전역변수 변수는 자신이 생성된 범위(코드 블럭)안에서만 유효합니다. 함수안에서 만든 변수는 함수 안에서만 살아 있다가 함수 코드의 실행이 종료되면 그 생명을 다합니다. 이것을 지역변수라고 합니다. 이와는 반대로 함수 외부에서 만든 변수는 프로그램이 살아 있는 동안에 함께 살아 있다가 프로그램이 종료가 되면 같이 소멸합니다. 이렇게 프로그램 전체를 유효범위로 가지는 변수를 전역변수라고 합니다. param = 10 strdata = '전역변수' def func1(): strdata = '지역변수' print(strdata) def func2(param): param = 1 def func3(): global param # global 을 사용하면 지역변수를 전역변수처럼 사용하겠다. 라는 뜻 # 그래서 값이 10에서 50으로 바귄다. param = 50 func1() # ‘지역변수’가 출력됨 print(strdata) # ‘전역변수’가 출력됨 print(param) # 10이 출력됨 func2(param) print(param) # 10이 출력됨 func3() print(param) # 50이 출력됨 #global 을 사용했으니 값이 바뀐 것을 확인하기 위한 출력 그러나 보통 지역변수를 사용하며, global를 사용하는 것은 매우 위험합니다. 전역변수를 프로그램에서 사용하는 경우는 프로그램 전체에서 공통적으로 사용되고, 잘 변하지 않는 데이터는 전역변수로 사용합니다. 예를 들면 아래와 같습니다. # pi 값은 공통적으로 사용하니 그냥 전역변수로 선언한다. pi = 3.141592653589793 # 원의 넓이를 구하는 함수 def area_of_circle(radius): area=radius*radius*pi return area # 두개의 원의 넓이를 구하는 함수 def sum_circle(radius1, radius2): area1=area_of_circle(radius1) area2=area_of_circle(radius2) sumcircle=area1+area2 return sumcircle",
    "tags": "Python",
    "url": "/python/2020/03/12/python-global.html"
  },{
    "title": "파이썬으로 2진수 8진수 16진수 구하기",
    "text": "정수형(integer)이란 말 그대로 정수를 뜻하는 자료형을 말합니다. a = 123 b = -123 c = 0 일반 프로그래밍에서 지원하는 정수형 상수의 범위는 -214747367 ~ 2147483647인데 파이썬은 메모리가 허용하는 범위내에서 지원 가능한 수를 사용할 수 있습니다. 2진수, 8진수, 16진수를 사용하는 이유 컴퓨터가 2진수를 사용하는 이유는 무엇일까요 ? 전기적 신호로 의미를 전달할 수 있는 가장 간단한 방법이기 때문입니다. 컴퓨터가 의미를 구분할 수 있는 최소 단위가 on/off이기 때문에 컴퓨터는 2진수 0,1를 사용합니다. 컴퓨터 회로들은 2진수로 되어 있는데, 왜 16진수를 사용하는 걸까? 2진수를 컴퓨터에게 0과 1로 단순하게 처리할 수 있어서 속도를 높여주지만 조그만 숫자도 매우 길어질 수 있습니다. 2진수 : 0 ~ 1을 나타낼 수 있음 8진수 : 0 ~ 7을 나타낼 수 있음 16진수 : 0 ~ 15을 나타낼 수 있음 8진수는 0 ~ 7로 나타낼 수 있는데, 그 중 가장 큰 숫자 7의 2진수는 무엇일까요? 8 4 2 1 0 1 1 1 0111로 표현할 수 있는데, 즉 비트 3개로 표현할수 있습니다. 작은 숫자임에도 매우 길어질 수 있음을 볼 수 있습니다. 여기서 추가적으로 알아두면 좋은 것은 8진수는 리눅스 권한(chmod 777) 줄 때 주로 사용하며, 16진수는 rgb컬러 코드( #ff00ff )에서 주로 사용한다는 것을 알아두면 좋습니다. 간혹 모르는 분들이 계시는 것 같더라구요. 2진수, 8진수, 16진수 출력하기 for i in range(20): print(bin(i),oct(i),hex(i)) # bin : 2진수 # oct : 8진수 # hex : 16진수",
    "tags": "Python",
    "url": "/python/2020/03/12/python-bin-oct-hex.html"
  },{
    "title": "의사 결정트리와(Decision Tree)로 심장질환 데이터 분석하기",
    "text": "실제 심장질환 데이터를 분석하여 심장질환에 있어 가장 영향력이 큰 변수를 찾아보도록 하겠습니다. 데이터 상세 설명은 여기에서 볼 수 있으며, 데이터는 여기에서 다운 받을 수 있습니다. 그리고 코드는 여기 GitHub에서 확인 할 수 있습니다. DataSet 데이터 셋 설명은 아래와 같습니다. trestbps ( resting blood pressure ) : 안정시 혈압(수축기 고혈압) chol (serum cholesterol) : 고지혈증 fbs ( fasting blood sugar larger 120mg/dl (1 true) ) : 당뇨가 있냐 없냐 ? 보통 126을 기준으로 126이상이면 당뇨인데 여기 데이터셋은 120을 기준으로 한다. restecg (resting electroc. result (1 anomality)) : 몸의 소금기 상태. 전해질( 나트륨, 칼륨, 염소 ) 중에 몇개가 보이는가 ? 보통 나트륨, 칼륨이 많다. thalach (maximum heart rate achieved ) : 최대 심박수(60~80 이 정상이고 100회 이상이면 빠르다) exang (exercise induced angina (1 yes) ) : 뛸때 가슴통증을 유발하는가 (협심증의 판단의 기준) oldpeak (ST depression induc) : 운동부하로 ST(아래 심전도에 보이는선) 이 기준선 아래로 떨어지는지 slope (slope of peak exercise ST ) : ST 경사각도가 올라가는 경우 심근경색이 있을때 경사가 기울어진다. ca (number of major vessel ) : 심장으로 들어가는 3개의 혈관이 막혔는지 ( 데이터 셋에서 1개가 막혔는지 2개가 막혔는지 표시됨) thal (no explanation provided, but probably thalassemia (3 normal; 6 fixed defect; 7 reversable defect) : 살라세미아는 지중해 지역에 사는 사람들에게 발생하는 빈혈관련 병으로 우리나라에서는 거의 환자가 없어 희귀병으로 보고 있다. ( 데이터 셋 : 3은 정상이고 6은 해결할수 없으며 7은 해결가능한 상태다) num ( diagnosis of heart disease (angiographic disease status) ) : 이 데이터 셋의 라벨로 심장질환의 갯수를 나타낸다. 이 데이터에서 대부분 피검사와 운동검사를 통해 얻어진 데이터이고, 정보획득량은 thal(지중해성 빈혈)이 가장 높으나 우라나라에서는 희귀병이므로 이 외의 요소인 이 외의 요소인 가슴통증과 oldpeak(운동부하의 ST 선) 등이 심장질환 판단에 중요한 변수가 되겠습니다. 정보획득량 우선 예쁜 시각화를 그리기 위해서 시각화 그래프 패키지를 설치하겠습니다 install.packages(\"rpart\") install.packages(\"rattle\") install.packages(\"FSelector\") library(rattle) library(rpart.plot) library(FSelector) 시각화 그래프 깔면서 엔트로피 구하는 패키지도 깔았기 때문에 간단히 정보획득량 구해보겠습니다. hr&lt;-read.csv(\"Heart.csv\", header = T) head(hr) a&lt;-information.gain(AHD~., hr[,-1]) 의사 결정나무(Decision Tree) 위에서 깐 패키지를 이용하면 예쁜 의사 결정나무 트리를 구할 수 있습니다. tree &lt;- rpart(AHD~.,data=hr[,-1], control=rpart.control(minsplit=2)) fancyRpartPlot(tree) 아래와 같은 그래프를 볼 수 있는데, 여기서 이 데이터 분석에 대한 결과를 설명한다고 할때, 어떻게 할 수 있을까 ? 데이터 분석에 대한 결과 데이터 분석은 분석 결과를 잘 설명하는 것도 중요합니다. 그렇기 때문에 데이터를 시각화나 분석을 한 후에, 사람들에게 잘 설명할 수 있도록 글을 쓰는 연습을 충분히 해야합니다. 그렇다면 심장질환 데이터를 이용하여 정보획득량과 의사 결정나무를 시각화했는데, 이 결과를 바탕으로 환자나, 결정나무에 대해 상세 설명을 해야 한다면 어떻게 할까요 ? 아마 아래와 같이 분석 결과를 쓸 수 있을 것 같습니다. 심장 질환과 관련하여 76개의 변수를 관찰하였으나 의미 있는 변수는 14가지 정도로 추릴 수 있습니다. 이 14개의 변수는 각각 age(나이), sex(성별), cp(가슴 통증), trestbps(혈압), chol(콜레스트롤), fbs(혈중 당 농도), restecg(체내 전해질 상태), thalach(최대 심박수), exang(운동 시 가슴 통증), oldpeak (운동부하 시 심전도 상태), ca(심장 혈관의 원활함), thal(지중해성 빈혈), num(심장 질환과 관련한 진단 개수)이다. 이중 주목할 만한 변수는 thal, 즉 지중해성 빈혈이다. 이 변수가 다른 변수와는 달리 보편적이지 않은 이유는 이 데이터가 스위스, 캘리포니아의 롱 비치, 부다페스트 등 해안 연안에 위치한 기관에서 수집한 데이터이기 때문이다. 이 때문에 데이터에 바다 근처 환자들이라는 특징이 주어진다. 심장 질환의 여부 예측을 위한 알고리즘으로는 의사 결정 트리를 이용하였고, 정확도는 70%를 웃돌았다. 정확도가 통상적 기준으로 높지 않은 이유는 데이터의 건수가 303건으로 많지 않기 때문이며 보다 다양한 유형의 환자들에 대한 데이터가 아니기 때문으로 추정된다. 그러나 심장 질환에 대한 변수들의 전반적인 특징과 영향을 살피기에는 다양한 수치가 포함된 표본이라고 할 수 있다. 심장 질환을 진단할 때 가장 영향도가 큰 변수는 지중해성 빈혈로 나타났으며 빈혈이 심할수록 심장 질환의 가능성이 80%로 높아졌다. 지중해성 빈혈을 제외한 심장 질환에 영향력이 높은 변수는 가슴 통증이었다. 지중해성 빈혈이 환자들의 거주지와 관련한 특이성이라고 하면 가슴 통증이 심장 질환을 판별하는 데에 가장 영향력 있는 변수라고 할 수 있다. 결론적으로 심장 질환에 영향을 미치는 변수는 지중해성 빈혈, 가슴 통증, 혈액 순환의 정도로 지중해성 빈혈이 심장 질환에 가장 큰 영향을 미치는 것으로 나타났으나, 이 질병은 지중해 연안이나 해역에 사는 환자들에게 나타나는 질병으로 심장 질환 분류에 적용할 만한 보편성은 없는 것으로 보인다. 그러나 가슴 통증이나 심혈관의 혈액 순환 정도를 관찰하는 것은 심장 질환을 판단하는 데에 의미 있는 변수로 확인되었다. 이런 글쓰는 연습을 충분히 한다면 좋은 데이터 분석가가 될 수 있을 것 같습니다.",
    "tags": "R",
    "url": "/r/2020/03/12/R-Decision-Tree-rpart.html"
  },{
    "title": "의사 결정트리와(Decision Tree)엔트로피(entropy)",
    "text": "의사 결정트리에 대해 알아볼껀데, 의사 결정트리가 어떤 상황에서 사용되는지 구현하려면 무엇이 필요한지에 대해 알아보겠습니다. 의사 결정트리(Decision Tree) 회귀식을 세울 때 중요한 변수(컬럼)들을 선택해야 하는 상황에서 어떠한 컬럼이 중요한지 판단이 안설때, 의사 결정트리를 이용하면 중요한 변수를 골라낼 수 있습니다. 예를 들어 회사 지원자에게 떨어진 이유를 명확히 설명해줘야 하는 경우나 은행에서 대출을 해줄 때, 대출을 해줄지 말지의 여부를 기업 데이터를 보고 결정해야 하는 경우 등이 있습니다. 또 의학면에서는 질병에 대한 진행바탕으로 올바른 처방을 위해 결정해야하는 경우에 사용되기도 합니다. Decision Tree 예시 엔트로피(entropy)와 정보획득량 의사 결정트리를 이야기 하다가 왜 갑자기 엔트로피 이야기가 나올까요 ? 결정트리를 만들 때, 가장 먼저 해야할 일은 중요한 변수(컬럼)을 찾는 것입니다. 즉, 정보획득량이 높은 변수를 찾아야하는데, 그때 엔트로피 함수를 사용합니다. 엔트로피(entropy) 함수 엔트로피는 “데이터의 불확실성이 얼마나 되는가?”를 알수 잇는 지표입니다. 즉, 엔트로피 지수가 높다는 것은 불확실성이 높다는 것을 알 수 있습니다. 엔트로피 그래프 curve(-x * log2(x) - (1 - x) * log2(1 - x),col=\"red\", xlab = \"x\", ylab = \"Entropy\", lwd=4) 정보획득량은 분할전 엔트로피에서 분할 후 엔트로피를 빼면 정보획득량을 구할 수 있습니다.",
    "tags": "R",
    "url": "/r/2020/03/12/R-Decision-Tree-entropy.html"
  },{
    "title": "나이브 베이즈(Naive Bayes)자동화 스크립트 만들기",
    "text": "knn에 이어서 나이브 베이도 자동화 스크립트로 만들어보겠습니다. Naive Bayes 자동화 스크립트 우선 파일명을 입력 받고, 컬러명이 있는지에 관한 유무와 라벨이 위치한 번호 마지막으로 제거할 컬럼의 위치를 입력 받아보겠습니다. 추가로 if문을 돌려서 패키지 설치가 되어 있으면 패스하고 아니면 설치하도록 하겠습니다. if (length(setdiff(packages, rownames(installed.packages()))) &gt; 0) { install.packages(setdiff(packages, rownames(installed.packages()))) } library(tm) library(gmodels) input_table &lt;- readline('csv파일을 입력하세요. ex) emp.csv : ') input_header &lt;- readline('컬럼명이 있습니까? ex)T OR F : ') input_label &lt;- readline('라벨 컬럼의 번호를 입력하세요. ex)N (N&gt;=1) : ') input_laplace &lt;- readline('라플라스를 입력하세요. ex)n (0&lt;n&lt;1) : ') 그리고 헤더 유무 검사를 하겠습니다. ### 해더 유무 검사 if (input_header == 'T'){ table_name &lt;- read.csv(input_table, stringsAsFactors = F, header=T) } else { table_name &lt;- read.csv(input_table, stringsAsFactors = F, header=F) } 이제 거의 다 끝났네요. 마지막으로 factor로 변환하고 라벨 번호와 라플라스 값을 숫자화 합니다. for (i in 1:ncol(table_name)){ table_name[,i] &lt;- factor(table_name[,i]) } ### 라벨 번호, 라플라스 값 숫자화 input_label &lt;- as.integer(input_label) input_laplace &lt;- as.numeric(input_laplace) 그리고 데이터를 나누고 나이브 베이즈합니다. set.seed(1) train_cnt &lt;- round(0.75*dim(table_name[1])) train_index &lt;- sample(1:dim(table_name)[1], train_cnt,replace=F) train_data &lt;- table_name[train_index,] test_data &lt;- table_name[-train_index,] table_name[,input_label] model &lt;- naiveBayes(train_data[,input_label]~., data=train_data, laplace = input_laplace) result &lt;- predict(model, test_data[,-input_label]) CrossTable(result, test_data[,input_label]) knn보다 더 간단하게 나이브 베이즈 자동화 스크립트가 완성 됫네요.",
    "tags": "R",
    "url": "/r/2020/02/13/R-NB-Naive-Bayes-auto.html"
  },{
    "title": "나이브 베이즈(Naive Bayes)를 이용한 영화 장르 분류",
    "text": "나이브 베이즈를 활용한 영화 장르 분류를 이번에 해보겠습니다. 예전에 포스팅했던 knn과 마찬가지로 그리 어렵지도, 코드가 길지도 않아서 한두번 해보면 금방 익숙해 질꺼 같습니다. DataSet 데이터는 선호하는 영화장르 데이터를 사용했으며, 데이터는 여기에서 다운받아 보실 수 있습니다. 물론 포스팅에서 사용된 코드는 제 Github에서 전부 보실 수 있습니다. # 패키지 설치 install.packages(\"e1071\") library(e1071) movie &lt;- read.csv(\"movie.csv\", header=T) Naive Bayes 나이브 베이즈는 아래와 같이 한줄이면, 모델이 완성됩니다. knn과 마찬가지로 그리 어렵지도 않으며 간단합니다. model &lt;- naiveBayes(movie[1:5], movie$장르, laplace=0) 예측은 아래와 같이 작성하실 수 있습니다. result &lt;- predict(model, movie[1:5]) # 장르를 제외한 라벨들로 예측을 해보겠다는 코드 예측이 일치하는지 확인하는 코드는 아래와 같습니다. movie$result &lt;- result 거의 일치함을 알수 있습니다.",
    "tags": "R",
    "url": "/r/2020/02/12/R-NB-Naive-Bayes-movie.html"
  },{
    "title": "나이브 베이즈(Naive Bayes) 확률로 인한 데이터 분류(조건부확률과 베이즈 정리)",
    "text": "오늘부터 나이브 베이즈(Naïve Bayes)에 대해 알아보겠습니다. 나이브 베이즈는 확률을 기반으로 한 머신러닝의 한 알고리즘입니다. 현재까지도 유용하고 많이 사용되고 있어서 알아둬야합니다. 확률로 인한 데이터 분류 기상학자가 날씨예보를 할 때, 일반적으로 “비올 확률 70%” 라는 용어를 사용해 예측을 합니다. 여기서 나온 70%는 과거의 사건 데이터를 사용한 것이며, 과거에 이런 경우가 10번 중 7번은 비가 왔음을 의미하는 것입니다. 베이즈기법 기반인 분류기는 분류되지 않은 데이터를 분류기가 분류할때, 새로운 속성에 대한 가장 유사한 범주를 예측하기 위해 관찰된 확률을 사용합니다. 관찰된 확률은 훈련 데이터에서 의해서 미리 계산이 되어집니다. 확률 이론 확률에는 결합 확률, 조건부 확률, 베이즈 정리가 있습니다. 1. 결합 확률 결합 확률은 서로 배반되는 두 사상 E와 F가 있을 때, 두 사상이 연속적으로 또는 동시에 일어나는 확률을 결합 확률이라고 합니다. $P(E∩F)$ 예를 들면 로또에 당첨될 확률과 벼락에 맞을 확률이 동시에 일어나는 것이 있습니다. 2.조건부 확률 어떠한 상항이 주어졌을때, 그 상황속에서 다른 상황이 일어날 확률을 조건부 확률이라고 합니다. $P(A|B)$ 예를 들어 A를 우산이 팔릴 확률, B가 비가 올 확률이라고 할때, 비가 오면서 우산이 팔릴 확률을 들 수 있습니다. 3.베이즈 정리 베이즈 정리는 조건부 확률의 조건과 사건 자체를 바꿔서 생각할 수 있도록 해주는 방법입니다. $P(A∩B)*P(B)=P(B|A)*P(A)$ 사건 사건이라는 개념이 존재하며 사건이란, 화창하거나 또는 비가 올 날씨, 동전 던지기에서 앞면 또는 뒷면이 나오는 경우들, 스팸 메일과 햄 메일이 같이 일어날 사건 등을 이야기 합니다. 1.독립 사건 독립 사건은 두 사건이 동시에 일어났는데, 두 사건이 서로 전혀 연관되지 않았다면 그건 독립 사건입니다. 예를 들어 동전 던지기의 결과와 화창한 날씨는 서로 독립적입니다. 확률 이론을 적용하자면 아래와 같습니다. $P(A∩B)=P(A)*P(B)\\\\ P(A|B)=P(A)\\\\ | 의미는 사건 B가 일어날 때 사건 A의 확률 \\\\ P(B|A)=P(B)\\\\$ 2.종속 사건 종속 사건은 사건 A가 일어났을 경우와 일어나지 않았을 경우 따라서 사건 B가 일어날 확률이 다를 때 B는 A의 종속 사건이라고 합니다. 확률 이론을 적용하면 아래와 같습니다. $P(A∩B)=P(A)*P(B|A)\\\\ P(A)*P(A|B)$ 확률 예제 실제로 구해야 하는 공식이 $P( B | A )$ 이라고 할 때, “비아그라”라는 메시지가 메일에 포함되어져있을 때 스팸일 확률을 구하자면 아래와 같은 공식이 나올 수 있습니다. $P(B|A)=P(A∩B)/(A)\\\\ P(A|B)=P(A∩B)/(B)$ 또한 스팸 메일일 확률이 20%이고, 햄인 메일인 확률이 80%이면 스팸이 아닐 확률을 표기하는 방법은 아래와 같이 구할 수 있습니다. $스팸일 확률:P(스팸)=0.2\\\\ 햄일 확률:p(햄)=0.8\\\\ p(~스팸):0.8$ 위와 같이 구할 수 있습니다. 다음은 이를 토대로 나이브 베이즈 분류를 해보도록 하겠습니다.",
    "tags": "R",
    "url": "/r/2020/02/11/R-NB-Naive-Bayes.html"
  },{
    "title": "간단한 K-Nearest Neighbors(Knn)자동화 스크립트 만들기",
    "text": "이제 knn알고리즘이 무엇인지나, 코드짜는 것에 익숙해졌는데, 앞으로 다양한 데이터를 만나더라도 손쉽게 스크립트 하나로 knn을 돌리기 위해 간단하게 만들어보겠습니다. knn 자동화 스크립트 우선 파일명을 입력 받고, 컬러명이 있는지에 관한 유무와 라벨이 위치한 번호 마지막으로 제거할 컬럼의 위치를 입력 받아보겠습니다. library(data.table) library(class) input_table &lt;- readline('csv파일을 입력하세요. ex) emp.csv : ') input_header &lt;- readline('컬럼명이 있습니까? ex)T OR F : ') input_label_num &lt;- readline('라벨이 위치한 번호을 입력하세요. ex)N (N&gt;=1) : ') input_rm_num &lt;- readline('제거할 컬럼 위치의 번호를 입력하세요. ex) n,n,n') 그리고 헤더 유무 검사 그리고 셔플 기능과 라벨 위치를 정수형으로 변경하고, 혹시 모를 na 값을 제거가 필요할 것 같으니 작성해보겠습니다. ### 셔플 기능 추가 table_name &lt;- table_name[sample(nrow(table_name)), ] ### na값 제거 table_name &lt;- na.omit(table_name) ### 라벨 위치를 정수형으로 변경(추후 사용을 위해) input_label_num &lt;- as.integer(input_label_num 그리고 가장 중요한 컬럼제거와 라벨을 factor로 변경하는 것, 그리고 라벨을 가장 마지막 컬럼으로 이동하는 것을 작성해보겠습니다. ### 제거할 컬럼 제거 split_num&lt;-strsplit(input_rm_num, ',') for(i in split_num){ table_name &lt;- table_name[,-as.integer(i)] } ### 라벨 위치보다 앞에 위치한 컬럼들의 갯수를 카운트함 cnt &lt;- 0 split_num &lt;- unlist(split_num) cnt&lt;-sum(ifelse(as.integer(split_num) &lt; input_label_num, 1, 0)) ### 컬럼을 제거하고 실제 라벨 위치 label_loc &lt;- input_label_num - cnt ### 라벨을 factor로 변경 label_tmp &lt;- factor(table_name[,label_loc]) ### 라벨을 가장 마지막 컬럼으로 이동 table2 &lt;- table_name[,-label_loc] table2$label &lt;- label_tmp 그리고 가장 중요한 데이터를 정규화 해주는 함수와 데이터 나눠보겠습니다. ### 정규화 함수 normalize &lt;- function(x){ return((x-min(x))/(max(x)-min(x))) } ### 정규화 데이터 삽입 table2[,1:(ncol(table2)-1)] &lt;- as.data.frame(lapply(table2[,1:(ncol(table2)-1)],normalize)) ### 데이터 나누기 mm &lt;- round(nrow(table2)*2/3) ### 데이터 설정 train_data &lt;- table2[1:mm,1:(ncol(table2)-1)] test_data &lt;- table2[(mm+1):nrow(table2), 1:(ncol(table2)-1)] train_label &lt;- table2[1:mm,'label'] test_label &lt;- table2[(mm+1):nrow(table2),'label'] 그리고 마지막으로 k값을 훈련 데이터 건수의 제곱근으로 취하고 knn결과를 확인해보겠습니다. ### k값을 훈련데이터 건수의 제곱근으로 취하는 방법 k_n &lt;- round(sqrt(nrow(train_data))) ###knn 결과 확인 result &lt;- knn(train = train_data, test = test_data, cl = train_label, k=k_n) round(prop.table(table(ifelse(test_label==result,'o','x')))*100,1) knn_fun() 이러면 간단하게 knn 자동화 스크립트가 완성되네요. 복잡하게 만든게 아니고 간단한 데이터에서 사용하려고 만든 것이기 때문에 아마 복잡한 데이터에서는 돌아가지 않을수 있지만, 그래도 공부하면서 간단하게 돌려보기엔 좋을 것 같습니다. 다음엔 나이브 베이지 분류에 대해 알아보겠습니다. 전체 코드는 Github에 있습니다.",
    "tags": "R",
    "url": "/r/2020/02/10/R-knn-function.html"
  },{
    "title": "K-Nearest Neighbors(Knn)적잘한 k값 알아내는 방법",
    "text": "오늘은 적절한 k값을 알아내는 방법에 대해 이야기 해보겠습니다. 여태껏 이 블로그에서 다양한 데이터를 사용하여 knn 분류를 해보았는데, 그때마다 k값을 다르게 하면 매번 값이 달라진다는 것을 어렴풋 다들 알고 계실꺼라 생각합니다. 그렇다면 적절한 k값을 찾기 위해서는 어떻게 해야댈까요 ? 오늘은 그 방법에 대해 알아보겠습니다. 마찬가지로 데이터는 데이터는 여기에서 볼 수 있으며 전체 코드 역시 GitHub에서 보실 수 있습니다. DataSet 이번 데이터는 부도 데이터이며, load 후에 결측값과 공백값을 제거하겠습니다. HMEQ &lt;- read.csv('DT_HMEQ.csv', stringsAsFactors = F) HMEQ &lt;- HMEQ[HMEQ$REASON != \"\" &amp; HMEQ$JOB != \"\",] # 공백값 제거 HMEQ &lt;- na.omit(HMEQ) # 결측값 제거 nrow(HMEQ) knn 명록형 변수인 REASON, JOB을 더미코딩하겠습니다. HMEQ$REASON &lt;- ifelse(HMEQ$REASON == 'HomeImp',0,1) HMEQ$JOB0 &lt;- ifelse(HMEQ$JOB == 'Other', 1,0) HMEQ$JOB1 &lt;- ifelse(HMEQ$JOB == 'Office', 1,0) HMEQ$JOB2 &lt;- ifelse(HMEQ$JOB == 'Mgr', 1,0) HMEQ$JOB3 &lt;- ifelse(HMEQ$JOB == 'ProfExe', 1,0) HMEQ$JOB4 &lt;- ifelse(HMEQ$JOB == 'Sales', 1,0) HMEQ &lt;- cbind(HMEQ[1:5],HMEQ[7:18]) # 사용할 변수로만 데이터 다시 생성 그리고 라벨을 제외한 모든 컬럼을 정규화, 랜덤 샘플링, 데이터 프레임을 나눠줍니다. HMEQ[2:17] &lt;- as.data.frame(lapply(HMEQ[2:17], scale)) # 랜덤 샘플링 (8:2) set.seed(12345) HMEQ_rand &lt;- HMEQ[order(runif(3364)), ] # 데이터 프레임 나누기 train &lt;- HMEQ_rand[1:2691, 2:17] test &lt;- HMEQ_rand[2692:3364, 2:17] train_labels &lt;- HMEQ_rand[1:2691, 1] test_labels &lt;- HMEQ_rand[2692:3364, 1] 그리고 knn을 실행합니다. library(class) library(gmodels) test_pred &lt;- knn(train, test, cl = train_labels, k=3) res &lt;- CrossTable(x = test_labels, y = test_pred, prop.chisq=FALSE) accuracy &lt;- sum(diag(res$t)/nrow(test)) 적절한 k값 찾기 여기까지는 지금까지 계속 했던 knn과 다를것이 없지만, 적절한 k값은 지금부터 찾을 예정입니다. 우선 적잘한 k을 위해 변수를 생성합니다. kvalue &lt;- NULL accuracy_k &lt;- NULL 그리고 for loop를 사용하여 k값에 따른 정확도를 구합니다. # for loop를 사용하여 k값에 따른 정확도 구하기 for(kk in c(1:100)){ knn_k &lt;- knn(train, test, cl = train_labels, k = kk) kvalue &lt;- c(kvalue, kk) res &lt;- CrossTable(x = test_labels, y = knn_k, prop.chisq=FALSE) # 분류 정확도 계산하기 accuracy &lt;- sum(diag(res$t)/nrow(test)) accuracy_k &lt;- c(accuracy_k, accuracy) } 그리고 k값에 따른 분류정확도를 생성하고, 그래프를 그려 시각화합니다. # k값에 따른 분류정확도 생성 valid_k &lt;- data.frame(k = kvalue, accuracy = accuracy_k) # k에 따른 분류 정확도 그래프 그리기 plot(formula = accuracy ~ k, data = valid_k, type = \"o\", pch = 20, main = \"validation - optimal k\") # 그래프에 k 라벨링 하기 with(valid_k, text(accuracy ~ k, labels = rownames(valid_k), pos = 1, cex = 0.7)) 적절한 k값 찾기 2 그렇다면 저번 포스팅때 했던 유방암 데이터에 맞는 적절한 k값이 무엇인지 한번 구해보겠습니다. kvalue &lt;- NULL accuracy_k &lt;- NULL for(kk in c(1:100)) { knn_k &lt;- knn(wbcd_train, wbcd_test, cl=wbcd_train_label, k=kk) kvalue &lt;- c(kvalue, kk) res &lt;- CrossTable(x=wbcd[470:569,2], y=knn_k, prop.chisq=F) accuracy &lt;- sum(diag(res$t)/nrow(wbcd_test)) accuracy_k &lt;- c(accuracy_k, accuracy) } valid_k &lt;- data.frame(k = kvalue, accuracy = accuracy_k) plot(formula = accuracy ~ k, data = valid_k, type = \"o\", pch = 20, main = \"validation - optimal k\") with(valid_k, text(accuracy ~ k, labels = rownames(valid_k), pos = 1, cex = 0.7)) 여기서 알아야 할 점은 정확도 그래프 수치가 알고리즘을 돌릴 때마다 매번 바뀌므로 공통적으로 정확도가 높은 k값을 찾아아합니다.",
    "tags": "R",
    "url": "/r/2020/02/09/r-knn-hmeq.html"
  },{
    "title": "K-Nearest Neighbors(Knn)을 이용하여 유방암 분류하기",
    "text": "오늘은 knn을 사용하여 유방암 분류를 해볼까합니다. 데이터는 데이터는 여기에서 볼 수 있으며 전체 코드 역시 GitHub에서 보실 수 있습니다. 월래 오늘 적절한 k값을 알아내는 것에 관려하여 포스팅을 할 예정이였으나, R에서 knn 관련 데이터 예제로 가장 많이 사용하는 것중에서 하나만 집고 넘어가는 것도 나쁘지 않고 해서 오늘은 유방암 데이터를 가지고 분류를 해보도록 하겠습니다. DataSet 우선 데이터를 load 해보겠습니다. wisc&lt;-read.csv(\"wisc_bc_data.csv\",stringsAsFactors=FALSE, header = F) 데이터에서 diagnosis는 라벨이며, B는 양성이고, M이 악성입니다. 전체 라벨에 대해 분포를 보면, 총 569명중에서 1/3이 악성이고 2/3이 양성임을 알 수 있습니다. wbcd$diagnosis &lt;-factor(wbcd$diagnosis, levels = c(\"B\",\"M\"),labels=c(\"Benign\",\"maliganant\")) round(prop.table(table(wbcd$diagnosis)),1)*100 Normalize 저번에 만든 정규화 함수를 이용하여 정규화 작업을 하겠습니다. normalize &lt;- function(x){ return((x-min(x))/(max(x)-min(x)))} wbcd_n &lt;- as.data.frame(lapply(wbcd[,3:31], normalize)) summary(wbcd_n) # 정규화가 잘 되었는지 확인 # 0 ~ 1 사이에 있는지 / min 과 max 그리고 전체 데이터에서 훈련 데이터와 학습 데이터를 나누는 작업을 하고 knn으로 돌려주겠습니다. nrow(wbcd_n) # 건수를 확인하는 함수 . wbcd_train &lt;- wbcd_n[1:469,] wbcd_test &lt;- wbcd_n[470:569,] wbcd_train_label &lt;- wbcd[1:469,2] wbcd_test_label &lt;-wbcd[470:569,2] Knn knn을 아래와 같이 돌려줍니다. library(class) result1 &lt;- knn(wbcd_train, wbcd_test, wbcd_train_label, k=21) ## 실제 라벨과 테스트 라벨과 비교를 해본다 . data.frame(wbcd[470:569,2],result1) 모델의 정확도는 아래와 같이 구할 수 있습니다. prop.table(table(ifelse(wbcd[470:569,2]==result1,\"o\",'x'))) 그럼 정확도가 대략 97%정도임을 알 수 있습니다. 여기서 그냥 끝내기는 조금 아쉬우니 이원 교차표를 그려서 자세하게 모델에 대해 보겠습니다. CrossTable(x=wbcd[470:569,2], y=result1, prop.chisq = FALSE ) 이원 교차표를 보면 악성인데, 양성으로 오진한 결과 때문에 정확도가 떨어짐을 알 수 있습니다.",
    "tags": "R",
    "url": "/r/2020/02/08/r-knn-wisc-bc-data.html"
  },{
    "title": "파이썬과 텐서플로우의 차이",
    "text": "간단하게 텐서플로우와 파이썬 문법 비교 해보겠습니다. Python ? Tensorflow 우선 간단하게 1에서 5까지의 숫자를 출력해보도록 하겠습니다. x = 0 for i in range(5): x = x+1 print(x) 파이썬으로 하면 위와 같으며, Tensorflow로 하면 아래와 같습니다. x = tf.Variable(0, name='x') model = tf.global_variables_initializer() with tf.Session() as sess: for i in range(5): sess.run(model) x = x + 1 print(sess.run(x)) 아무래도 간단하게 출력하고 확인할 수 있는건 Python만한게 없네요. 이번에는 Numpy을 Tensor로 구현해보도록 하겠습니다. import numpy asnp a=np.zeros((2,2)) b=np.ones((2,2)) print(a) print(b) 아래와 같이 바꿀 수 있습니다. import tensorflow as tf a=tf.zeros((2,2)) b=tf.ones((2,2)) with tf.Session() as sess: print('=========tensorflow========') print(sess.run(a)) print(sess.run(b))",
    "tags": "MachineLearning",
    "url": "/machinelearning/2020/02/07/tensorflow-vs-python.html"
  },{
    "title": "Go에서if와 짧은 명령 사용하기",
    "text": "이번엔 Go에서 if문을 사용해보겠습니다. Go가 설치되어 있지 않지만 실행을 해보고 싶다면, 여기를 클릭해주세요. Go if if문 또한C와 Java와 비슷합니다. 조건 표현을 위해 ()는 사용하지 않습니다. 하지만 반드시 실행문을 위한 {}는 반드시 작성해야합니다. package main import ( \"fmt\" \"math\" ) func sqrt(x float64) string { if x &lt; 0 { return sqrt(-x) + \"i\" } return fmt.Sprint(math.Sqrt(x)) } func main() { fmt.Println(sqrt(2), sqrt(-4)) } for처럼 if에서도 조건문 앞에 짧은 문장을 실행할 수 있습니다. package main import ( \"fmt\" \"math\" ) func pow(x, n, lim float64) float64 { if v := math.Pow(x, n); v &lt; lim { return v } return lim } func main() { fmt.Println( pow(3, 2, 10), pow(3, 3, 20), ) } 짧은 실행문을 통해 선언된 변수는 if안쪽 범위(scope)에서만 사용할 수 있습니다. if else if에서도 짧은 명령문을 통해 선언된 변수는 else 블럭 안에서도 사용할 수 있습니다. package main import ( \"fmt\" \"math\" ) func pow(x, n, lim float64) float64 { if v := math.Pow(x, n); v &lt; lim { return v } else { fmt.Printf(\"%g &gt;= %g\\n\", v, lim) } // can't use v here, though return lim } func main() { fmt.Println(pow(3, 2, 10), pow(3, 3, 20), ) }",
    "tags": "Go",
    "url": "/go/2020/02/06/go-if-else.html"
  },{
    "title": "Go에서 While 사용하기",
    "text": "이전에 Go for문에 대해서 알아봤는데 for문과 비슷하게 조건문만 표시하면 C언에서 while을 사용하듯이 사용할 수 있습니다. 오늘은 이 while문에 대해 알아보겠습니다. Go가 설치되어 있지 않지만 실행을 해보고 싶다면, 여기를 클릭해주세요. Go while Go에서 while은 for문 처럼 조건문만 표시하면 사용 할 수 있습니다. package main import \"fmt\" func main() { sum := 1 for sum &lt; 1000 { sum += sum } fmt.Println(sum) } 또한 for에서 조건문을 생략하면 무한 루프를 간단하게 표현할 수 있습니다. package main func main() { for { } }",
    "tags": "Go",
    "url": "/go/2020/02/05/go-while.html"
  },{
    "title": "Go 반복문-for문",
    "text": "오늘은 GO 반복문과 조건문에 대해 배워보도록 하겠습니다. 예전 포스팅에서 한번 언급 하긴 했었는데 GO에서 반복문은 for문 밖에 없다고 이야기 했었습니다. 오늘은 그 for문에 대해 알아보도록 하겠습니다. Go가 설치되어 있지 않지만 실행을 해보고 싶다면, 여기를 클릭해주세요. Go 반복문(for문) Go 언어는 반복문이 for문 밖에 없으며 기본적인 for반복문은 c와 java와 거의 유사합니다. 다른점이 있다면 소괄호가 필요하지 않다는 점뿐입니다. 하지만 실행문을 위한 중괄호 {}는 필요합니다. package main import \"fmt\" func main(){ sum := 0 for i := 0; i&lt;=10; i++{ sum += i } fmt.Println(sum) } for문을 사용하여 1부터 10까지 더하는 함수 하나를 만들어보면 위와 같이 만들 수 있습니다. c와 java의 for문과 별다른 차이가 없기 때문에 사실 어렵지는 않지만 혹시 모르니 주석을 달아 설명을 하자면 아래와 같습니다. package main import \"fmt\" func main(){ //sum 이라는 변수 생성 sum := 0 // for문 시작 // 처음에 i는 0이라고 지정 // i가 10이 될때까지 반복문이 돌아감 for i := 0; i&lt;=10; i++{ sum += i // sum 이라는 변수에 계속 i를 더함 1+2+3+4+... } fmt.Println(sum) } 또한 C와 java처럼 향상된 for문을 사용할 수 있습니다. package main import \"fmt\" func main() { sum := 1 for sum &lt; 1000 { sum += sum } fmt.Println(sum) }",
    "tags": "Go",
    "url": "/go/2020/02/04/go-for.html"
  },{
    "title": "텐서플로우 사용하기, Hello, Tensorflow! (실행 구조와 구구단 출력)",
    "text": "언어를 새로 배우거나 무언가를 새로 배울 때, 우리는 가장 먼저 Hello, world! 를 가장 먼저 찍어냅니다. Tensorflow 을 포스팅 하는 기념으로 Hello, Tensorflow!를 찍어보겠습니다. Tensorflow 기본 구조 우선 텐서플로우가 설치가 안되어 있다면 아래와 같이 설치 해주세요. pip install tensorflow 설치가 되었다면 모듈을 가져와서 tf를 호출합니다. import tensorflow as tf 그리고 그래프를 실행할 세션을 구성합니다. (참고로 포스팅에서 사용하는 tf는 1.x 버전입니다.) sess=tf.Session() # 그래프를 실행할 세션을 구성한다. hello=tf.constant('Hello, Tensorflow') # # # # # # # # 모델을 구성하는 부분 ↑ # # # # # # # # 모델을 실행하는 부분 ↓ print(sess.run(hello)) print(str(sess.run(hello),encoding=\"utf-8\")) 위에서 변수를 정의 햇으나 실행은 정의한 시점에서 실행되는 것이 아니고 Session 객체와 run 메소드를 사용할 때 계산되어 실행됩니다. 이번에는 간단하게 덧셈을 출력해보겠습니다. import tensorflow as tf x=tf.constant(35, name='x') # x라는 상수값을 만들어 숫자 35 지정 y=tf.Variable(x+5, name='y') # y라는 변수를 만들고 방정식 x+5로 정의 model=tf.global_variables_initializer() # 변수 초기화 with tf.Session() as sess: # 값을 계산하기 위한 세션 생성 (세션 열기) sess.run(model) # 위에서 초기화한 model을 실행하겠다. print(sess.run(y)) # 변수 y를 실행하며 현재값 출력 월래는 세션을 연 뒤에는 닫아야는데 with절 후에는 자동으로 닫히기 때문에 그냥 출력합니다. 또한 텐서플로우는 빌딩 구조와 실행구조(session)가 분리되어 있습니다. # 빌딩 과정 import tensorflow as tf x2 = tf.linspace(-1.0, 1.0,10) # -1 ~ 1 사이의 숫자 중 10개를 랜덤으로 출력 g=tf.get_default_graph() print([op.name for op in g.get_operations()]) # 실행 과정 sess=tf.Session() print(sess.run(x2)) sess.close() Tensorflow 실행 구조 Tensorflow Session은 fetch와 feed 2가지 방법으로 처리 할 수 있습니다. fetch는 Tensor에 할당 되어야 실제 Session에서 실행을 할 수 있습니다. 한개 실행할 때 import tensorflow as tf a=tf.constant(1) b=tf.constant(2) c=tf.add(a,b) sess=tf.Session() print(sess.run(c)) sess.close() # 또는 import tensorflow as tf with tf.Session() as sess: print(tf.add(1,2).eval()) 여러 개 실행할 때 import tensorflow as tf input1=tf.constant(3.0) input2=tf.constant(2.0) input3=tf.constant(5.0) # fetch 여러 개 되는 부분 intermed=tf.add(input2,input3) mul=tf.multiply(input1,input3) with tf.Session() as sess: result=sess.run([mul,intermed]) print(result) feed 같은 경우는 Session에서 반드시 feed_dict로 처리 값을 할당해주어야 합니다. import tensorflow as tf a=tf.placeholder(\"float\") b=tf.placeholder(\"float\") y=tf.multiply(a,b) z=tf.add(y,y) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(sess.run(y,feed_dict={a:3, b:3})) print(sess.run(z,feed_dict={a:4, b:4})) 그래서 응용하여 구구단을 출력해보자면 아래와 같이 작성 할 수 있습니다. import tensorflow as tf ## 구조(그래프) 선언부 x = tf.Variable(0, name='x') # tensorboard 로 그래프를 그리기 위해서는 name 을 지정해 줘야한다. y = tf.Variable(0, name='y') # name 을 지정할 때 이름이 중복되면 안되고, 중복 사용을 위해서는 z = tf.multiply(x, y, name='z') # 다른 옵션을 사용해야 한다. model = tf.global_variables_initializer() # 변수(노드)를 위치 및 생성 merged = tf.summary.merge_all() # 그래프를 그리는데 사용될 변수(노드)를 취합 ## 선언한 구조(그래프) 실행부 with tf.Session() as sess: # 세션(하나의 사용자) 생성 sess.run(model) # 위에서 생성한 그래프 구조를 실행 for i in range(2, 10): for j in range(1, 10): print(i, ' x ', j, ' = ',sess.run(z,feed_dict={x:i, y:j})) # 초기화된 변수에 값을 feed writer = tf.summary.FileWriter('./logs', sess.graph) # tensorboard 그래프를 그리는데 사용할 실행 로그를 저장할 폴더 지정",
    "tags": "MachineLearning",
    "url": "/machinelearning/2020/02/03/ml-tensorflow-session.html"
  },{
    "title": "텐서플로우란?",
    "text": "머신러닝 개발자들이 많이 사용하는 Tensorflow에 대해서 간단하게 알아보겠습니다. 요즘에는 텐서플로우외에도 케라스나 파이토치등 다양하지만 저는 아직 텐서플로우가 편해서 텐서플로우를 쓰고 있습니다. 맨 처음 머신러닝을 접할 때, 텐서플로우로 접했기 때문인거 같습니다. 같이 공부했던 사람들 중 케라스나 파이토치로 시작한 사람들이 꽤 있었는데 케라스나 파이토치로 시작한 사람들은 텐서플로우가 조금 더 어렵다고 하더라구요. 그래서 커뮤니티 사이에서 가끔 텐서플로우로 먼저 시작하라는 글이 보이기도 하더라구요. 이 글 쓴 시점에서는 현재 텐서플로우는 2.0이 나와 있는 상황입니다. 2.0버전이랑 1.x 버전이랑은 문법 차이가 꽤 나기도 하고, 성능 차이도 있다고 합니다. 지금은 1.x 버전에 관하여 포스팅 하고는 있지만 조만간 2.0에 대해서도 포스팅을 해야겠네요. Tensorflow 텐서플로우는 딥러닝을 위해 구글에서 만든 오픈소스 라이브러리입니다. 간단하게 Tensorflow의 특징을 살펴보면 2가지 정도 있다고 보면 됩니다. Tensorflow 특징 1) 데이터 플로우 그래프를 통한 풍부한 표현력 데이터 플로우 그래프(Data Flow Graph)방식을 사용하며, 그래프를 시작하기 위해서 텐서보드라는 것을 사용할 수 있습니다. 2) 코드 수정 없이 CPU/GPU 모드 동작 특별한 코드 수정 없이 CPU나 GPU를 사용하여 학습을 시킬 수 있습니다. Tensorflow 용어 사실 특별히 용어라고 할 것까지는 없지만 한번 정리를 해보자면, 아래와 같은 용어가 있습니다. 오퍼레이션 (Operation) : 그래프 상의 노드는 오퍼레이션(OP)로 불린다. 오퍼레이션은 하나 이상의 텐서를 받을 수 있으며, 계산을 수행하고, 결과를 하나 이상의 텐서로 반환할 수 있다. 텐서 (Tensor) : 내부적으로 모든 데이터는 텐서를 통해 표현된다. 텐서는 일종의 다차원 배열인데, 그래프 내의 오퍼레이션 간에 텐서가 전달된다. 세션 (Session) : 그래프를 실행하기 위해서는 세션 객체가 필요하다. 세션은 오퍼레이션의 실행 환경을 캡슐화 한것이다. 변수 (Variables) : 그래프 실행 시 파라미터를 저장하고 갱신하는데 사용한다. 메모리상에서 텐서를 저장하는 버퍼 역활을 한다. Tensorflow 설치 Tensorflow 설치는 간단하며, 아래와 같다. Anaconda 환경 :pip install tensorflow 파이참 환경 : file → setting → project interpreter",
    "tags": "MachineLearning",
    "url": "/machinelearning/2020/02/02/ml-tensorflow.html"
  },{
    "title": "오버피팅 억제를 위한 방법! Dropout!",
    "text": "오버피팅을 억제하기 위한 방법으로 Dropout에 대해 알아보도록 하겠습니다. 오버피팅은 학습을 시키면서 흔히 접할 수 있는 문제이며, 풀어야 할 문제 중 하나 입니다. 오늘은 이러한 오버피팅에 대한 문제를 바로 잡을 수 있는 방법 중 가장 간단한 방법에 대해 알아보겠습니다. Dropout Dropout은 오버피팅을 억제하기 위해 뉴런을 임의로 삭제하면서 학습하는 방법입니다. 신경망 전체를 학습시키지 않고 일부 노드만 무작위로 골라 학습 시키는 방법입니다. 학습하는 중간중간 일정 비율로 노드들을 무작위로 골라 출력을 0으로 만들어 신경망의 출력을 계산하는 방법입니다. Dropout을 적용하면 학습되는 노드와 가중치들이 매번 달라지기 때문에 신경망이 과적합에 빠지는 것을 예방할 수 있습니다. Dropout의 개념을 쉬운 예를 들어서 애기하면, 전문가가 많으면 오히려 오답이 나올 수 있으며, 사공이 많으면 배가 산으로 간다. 라는 속담을 생각하면 조금 더 쉽게 이해 할 수 있습니다. 여기서 사공이 많으면 배가 산으로 가기 때문에 몇 명의 전문가만 선별하여 반복적으로 같은 결과가 나오면 그것을 답으로 선택하겠다. 라고 하는 것이 앙상블이라고 하는 학습 방법이 있는데, Dropout과 자주 사용 되는 학습 방식입니다. 예를 들어 위와 같은 결과를 내는 신경망이 있다고 할 때, 드롭아웃(Dropout)을 적용하게 되면 아래 그림과 같을 수 있습니다. Dropout은 신호를 보내지 않기 때문에 이런 결과를 얻을 수 있습니다.",
    "tags": "MachineLearning",
    "url": "/machinelearning/2020/02/02/ml-dropout-overfitting.html"
  },{
    "title": "Batch Nomalization 학습 실제 사용할때 주의 할 점",
    "text": "오늘은 Batch Nomalization 학습 실제 사용할때 주의 할 점과 가중치 초기화의 중요성에 대해 같이 보도록 하겠습니다. 가중치 초기화의 중요성 가중치 초기값을 적절하게 설정하면 각 층의 활성화 값의 분포가 적당히 퍼지는 효과가 발생합니다. 그리고 활성화 값이 적절하게 분포하게 되면 학습이 잘되고 정확도가 높아집니다. 아래의 그림은 학습이 잘 안되는 가중치 분포의 예입니다. 아래의 그림은 학습이 잘된 경우입니다. 한 눈에 봐도 고르게 잘 된것을 알 수 있습니다. 가중치 초기값 선정하는 5가지 방법 가중치 초기값을 선정하는 방법에는 대략 5가지 정도가 있습니다. 1. 가중치 초기값을 0으로 선정. 이 방법은 추천하지 않습니다. 왜냐하면 학습이 잘되지 않기 때문이죠. 하지만 이런 방법도 예전에 있는 있었다. 로 넘어가시면 될 것 같습니다. 2. 표준편차가 1인 정규분포를 사용해 초기값 선정. 표준편차가 클수룩 Data가 더 많이 흩어져 있습니다. ex) 시험문제가 어려우면 아주 잘하는 학생들과 아주 못하는 학생들로 점수가 딱 나눠진다. 1 * np.random.randm(10,100) 3. 표준편차가 0.01인 정규분포를 사용해 초기값 선정. 표준편차가 작을수록 Data가 평균에 가깝게 분포합니다. ex) 시험문제가 쉬우면 학생들 점수가 평균에 가깝다. 0.01 * np.random.randm(10,100) 4.Xavier 초기값 선정. 표준편차가 √(1/n)인 (n은 앞층의 노드 수) 정규분포로 초기화하는 방법이 있으며 sigmoid 함수와 짝궁이며 같이 사용됩니다. 5.He 초기값 선정. 표준편차 √(2/n)인 정규분포로 초기화하는 방법이 있습니다. He는 Relu함수와 짝궁으로 같이 사용됩니다. 배치 정규화 앞에서 가중치 초기값을 적절하게 설정하면 각 층의 활성화 값이 적절하게 분포하여 학습이 잘되고 정확도가 높아지는 것 또한 알 수 있었습니다. 그러나 가중치 초기화를 하더라도 가중치 값이 불규칙하게 분포할 가능성이 있습니다. 여기서 배치 정규화의 개념을 툭 튀어나오게 됩니다. 배치 정규화는 바로 각 층에서 활성화 값이 적당하게 분포 되도록 강제로 조정하게 합니다. 여기서 가장 중요한 사실은 Training data 전체에 대해 mean과 variance를 구하는 것이 아니라, mini batch 단위로 접근하여 계산합니다. 현재 택한 mini batch 안에서만 mean과 variance를 구해서 이 값을 이용해서 mormalize 합니다. 즉, 학습 시 미니배치를 단위로 정구화를 하며, 데이터 분포가 평균이 0, 분산이 1이 되도록 정규화 합니다. ${ \\mu }_{ B }\\quad \\leftarrow \\frac { 1 }{ m } \\sum _{ i=1 }^{ m }{ x_{ i } }$ 평균 ${ \\sigma }^{ 2 }_{ B }\\quad \\leftarrow \\frac { 1 }{ m } \\sum _{ i=1 }^{ m }{ (x_{ i }- } { \\mu }_{ B })^{ { 2 } }$ 분산 ${ \\hat { x } }_{ i }\\leftarrow \\frac { { x }_{ i }-\\mu _{ { B } } }{ \\sqrt { { \\sigma }_{ B }^{ 2 } } +E }$ 실제 값 - 평균 , 표준편차 + E 왜 층마다 배치 정규화 작업을 해야 할까요? 그 이유는 신경망은 파라미터(매개변수)가 많기 때문에 학습이 어렵습니다. 특히나 딥러닝 같은 경우는 레이어가 많은데 이 뜻은 가중치가 층마다 다르다 라고 해석 할 수 있습니다. 더욱이 층을 통과할수록 각기 다른 가중치가 쌓이며 가중치의 작은 변화가 가중 되어서 쌓이면 레이어가 많아 질수록 출력되는 값의 변화가 크기 때문입니다. 즉, 가중치 때문에 입력값에 대한 완전히 다른 출력값이 나옴을 알 수 있습니다. 이러한 이유로 배치 정규화가 등장했으며, 배치 정규화는 입력값이 활성화 함수를 통과하기 전에 가중의 변화를 줄이는 것이 목표입니다. 가중의 합이 배치 정규화에 들어오게 되면 기존 값의 스케일을 정규분포로 조정하게 되며 선형이 아닌 비선형 사이로 분포를 유지하여 데이터의 폭(scale), 분포(shift)의 위치를 조절하기 위해 감마(분산)과 베타(평균)을 적용하여 이를 학습하면서 층을 거치게 됩니다. Batch Normalization 을 사용할 때의 주의할 점. 배치 정규화를 하는 이유는 학습을 하기 위해서 라고 했습니다. 훈련 데이터를 통해서 이미 감마(분산)값과 베타(평균)값이 이미 최적에 맞춰진 상황입니다. 이 모델을 이용하여 테스트를 수행하려고 할 경우 훈련 데이터의 평군, 표준편차 값과 테스트 데이터의 평균, 표준편차 데이터가 다르기 때문에 제대로 테스트 되지 않을 수 있습니다. 즉, 훈련 데이터의 최적의 감마, 베타 값을 가지고 테스트를 하면 값이 제대로 나올 수 있기 때문에 테스트 데이터로 모델을 돌릴 때에는 배치 정규화를 사용하면 안됩니다. 학습 단계에서는 데이터 단위가 배치 단위로 들어오기 때문에 배치의 평균, 분산을 구하는 것이 가능하지만 테스트 단계에서는 배치 단위로 평균/분산을 구하기 어렵습니다. 이를 해결하기 위해서는 두가지 방법이 있습니다. 1. 학습 단계에서 배치 단위의 평균/ 분산을 저장. (이동평균) 2. 테스트시에는 구해진 이동평균을 사용하여 정규화. Training Data로 학습을 시킬 때는 현재 보고 있는 mini batch에서 평균과 표준 편차를 구하지만, Test Data를 사용하여 Inference를 할 때는 다소 다른 방법을 사용합니다. mini batch의 값들을 이용하는 대신 지금까지 본 전체 데이터를 다 사용한다는 느낌으로, Training할 때 현재까지 본 Input들의 이동평균 및 비편향추정량의 이동평균을 계산하여 저장해놓은 뒤 이 값으로 normalize를 합니다. 마지막에 감마와 베타를 이용하여 scale/shift 해주는 것은 동일합니다.",
    "tags": "MachineLearning",
    "url": "/machinelearning/2020/02/01/ml-batch-nomalization.html"
  },{
    "title": "파이썬 클래스로 신경망 구현하기(cross_entropy, softmax, Softmax With loss )",
    "text": "저번 포스팅에서는 forward와 backward 그리고 활성화 함수인 Relu함수를 클래스로 구현해보았습니다. 이번에는 cross entropy와 softmax도 함께 구현해보도록 하겠습니다. cross entropy 와 softmax 보통 신경망에서 분류할 때, softmax를 사용하며, softmax는 신경망의 출력층 마지막에서 사용합니다. softmax와 함께 오차 함수로 cross entropy함수를 사용하는데, cross entropy error는 줄여서 CEE라고도 쓸 수 있습니다. 식은 아래와 같습니다. $E\\quad =-\\sum _{ k }{ { t }_{ k } } { log\\, y }_{ k }\\quad$ y_k는 신경망에서 나오는 출력 값이며 0에서 1사이의 값이 나옵니다. t_k는 정답 레이블이며, 정답이 아닌 나머지 t_k가 0이며, log는 밑이 e인 자연로그입니다. cross entropy를 Python으로 작성할 때 아주 작은 값을 더해줘야 하는데, 그 이유는 y가 0인 경우 -inf값을 예방하기 위해서 입니다. 파이썬으로 구현하면 아래와 같이 구현할 수 있습니다. import numpy as np def crossEntropyError(y, t): return -np.sum(t*np.log(y)) 그러나 위와 같이 구현하게 된다면, y가 0되버리는 경우에 -inf값이 나올 수 있으므로 아주 작은 값을 더해줘야 합니다. import numpy as np def crossEntropyError(y, t): delta = 1e-7 #아주 작은 값 (y가 0인 경우 -inf 값을 예방) return -np.sum(t*np.log(y+delta)) 그래서 cross entropy를 구현할 때는 위와 같이 아주 작은 값을 y에 더해줘야 합니다. import numpy as np def crossEntropyError(y, t): delta = 1e-7 return -np.sum(t*np.log(y+delta)) t = np.array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0]) # label = 5 y = np.array([0.1, 0.03, 0.05, 0.2, 0.9, 0.0, 0.1, 0.2, 0.12, 0.03]) print(\"-- 정답인 경우 --\") print(\"CEE :\", crossEntropyError(y, t)) y = np.array([0.1, 0.03, 0.05, 0.2, 0.0, 0.1, 0.2, 0.12, 0.03, 0.9]) print(\"-- 오류인 경우 --\") print(\"CEE :\", crossEntropyError(y, t)) softmax는 아래와 같이 파이썬으로 구현할 수 있습니다. def softmax(a): c = np.max(a) exp_a = np.exp(a) sum_exp_a = np.sum(exp_a) y = exp_a / sum_exp_a return y Softmax With loss 클래스 만들기 클래스 이름은 원하는 걸로 하셔도 되지만, 저는 명확한 구분을 위해 이렇게 짓겠습니다. 위에서 softmax와 cross entropy 함수 두개 다 구현했기 때문에, 추가 할 함수는 없으며 loss함수에 대해 forward와 backward를 사용하여 클래스만 구현하면 Softmax With loss 클래스를 만들 수 있습니다. import numpy as np def cross_entropy_error(y, t): delta = 1e-7 return -np.sum(t * np.log(y + delta)) / y.shape[0] def softmax(a): c = np.max(a) # 추가한 부분 exp_a = np.exp(a) sum_exp_a = np.sum(exp_a) y = exp_a / sum_exp_a return y class SoftmaxWithloss: def __init__(self): self.loss = None self.y = None self.t = None def forward(self, x, t): self.t = t self.y = softmax(x) self.loss = cross_entropy_error(self.y, self.t) return self.loss def backward(self, dout=1): batch_size = self.t.shape[0] dx = (self.y - self.t) / batch_size return dx 여기까지 구했다면 아래와 같이 2층짜리 신경망을 쉽게 만들어볼 수 있습니다. import numpy as np def cross_entropy_error(y, t): delta = 1e-7 return -np.sum(t * np.log(y + delta)) / y.shape[0] def softmax(a): c = np.max(a) exp_a = np.exp(a) sum_exp_a = np.sum(exp_a) y = exp_a / sum_exp_a return y class Affine: def __init__(self, W, b): self.W = W self.b = b self.x = None self.dW = None self.db = None def forward(self, x): self.x = x out = np.dot(x, self.W) + self.b return out def backward(self, dout): dx = np.dot(dout, self.W.T) self.dW = np.dot(self.x.T, dout) self.db = np.sum(dout, axis=0) return dx, self.dW, self.db class Relu: def __init__(self): self.mask = None def forward(self, x): self.mask = (x &lt;= 0) # 설명 : x 값이 0 이하면 True 크면 False; True, False 를 가지는 numpy 배열 out = x.copy() out[self.mask] = 0 # 설명 : mask 가 Ture 인 곳은 x 의 원소 값이 0, False 인 곳은 그대로 출력 return out def backward(self, dout): dout[self.mask] = 0 dx = dout return dx class SoftmaxWithloss: def __init__(self): self.loss = None self.y = None self.t = None def forward(self, x, t): self.t = t self.y = softmax(x) self.loss = cross_entropy_error(self.y, self.t) return self.loss def backward(self, dout=1): batch_size = self.t.shape[0] dx = (self.y - self.t) / batch_size return dx x = np.array([[1, 2]]) w1 = np.array([[1, 3, 5], [2, 4, 6]]) w2 = np.array([[1, 4], [2, 5], [3, 6]]) b1 = np.array([1, 2, 3]) b2 = np.array([1, 2]) # 순전파 affine1 = Affine(w1, b1) affine2 = Affine(w2, b2) relu1 = Relu() relu2 = Relu() # 은닉 1층 out1 = affine1.forward(x) relu_out1 = relu1.forward(out1) # 은닉 2층 out2 = affine2.forward(relu_out1) relu_out2 = relu2.forward(out2) print('out : \\n', relu_out2) # softmax t = np.array([[0, 1]]) softmaxWithloss = SoftmaxWithloss() loss = softmaxWithloss.forward(relu_out2, t) # 역전파 dout = softmaxWithloss.backward() # dout = relu_out2 print('dout : \\n', dout) # 은닉 2층 # relu 통과 relu_dout = relu2.backward(dout) print('relu_dout : \\n', relu_dout) # affine 통과 dout1, dw2, db2 = affine2.backward(relu_dout) print('dout1 : \\n', dout1) # 은닉 1층 relu_dout1 = relu1.backward(dout1) print('relu_dout1 : \\n', relu_dout1) dx, dw1, db1 = affine1.backward(relu_dout1) print('dx : \\n', dx)",
    "tags": "MachineLearning",
    "url": "/machinelearning/2020/01/18/ml-class-softmax.html"
  },{
    "title": "파이썬 클래스로 신경망 구현하기(relu,forward,backward)",
    "text": "저번 포스팅 때, 순전파와 역전파 원리를 간단하게 보고, class를 만들어봤습니다. 이번에는 활성화 함수를 넣어 조금 그럴듯한 신경망을 만들어보도록 하겠습니다. 활성화 함수를 사용하지 않고 열심히 신경망만 깊게 쌓기만 하면 깊게 쌓는 의미도 없을 뿐더라 그냥 단층 신경망이라도 봐도 무방합니다. 그렇기 때문에 꼭 넣어주어야 깊게 쌓는 의미가 있습니다. Relu Class 만들기. 저번 포스팅에서 만들었던 Affine class는 냅두고 Relu(렐루) 클래스를 한번 만들어 보겠습니다. Relu 함수는 예전 포스팅에서 한번 다루었는데 혹시나 약간 개념이 헷갈리거나 다른 활성화 함수에 대해 알고 싶다면 여기를 눌러 참고해주세요. Relu의 특징은 0이하면 0을 출력하고 0을 넘으면 그냥 그대로 출력하는 비교적 간단하지만 대단히 효율 좋은 활성화 함수이며 그래프로 출력하지만 아래와 같이 출력할 수 있습니다. 그렇기 때문에 함수 구현이나 클래스는 간단하게 아래와 같이 구현할 수 있습니다. class Relu: def __init__(self): self.mask = None def forward(self,x): self.mask = (x &lt;= 0) out = x.copy() out[self.mask] = 0 return out def backward(self,dout): dout[self.mask] = 0 dx = dout return dx 저번에 만든 순전파와 역전파 클래스를 합쳐서 전체 코드를 보면 아래와 같습니다. # 활성화 함수 class Relu: def __init__(self): self.mask = None def forward(self,x): self.mask = (x &lt;= 0) out = x.copy() out[self.mask] = 0 return out def backward(self,dout): dout[self.mask] = 0 dx = dout return dx # 순전파 역전파 class Affine: def __init__(self,w,b): self.w = w self.b = b def forward(self, x): out = np.dot(x,self.w)+ self.b return out def backward(self,x,out): dx = np.dot(out,self.w.T) dw = np.dot(x.T, out) db = np.sum(out, axis = 0) return dx,dw,db 위 클래스를 이용하여 2층짜리 순전파 신경망을 만들어보겠습니다. # input x = np.array([[1,2]]) # 가중치와 바이어스 w1 = np.array([[1,3,5],[2,4,6]]) w2 = np.array([[1,4],[2,5],[3,6]]) b1 = np.array([1,2,3]) b2 = np.array([1,2]) # 객체화 affine1=Affine(w1,b1) affine2=Affine(w2,b2) relu1=Relu() relu2=Relu() # 순전파 y1=affine1.forward(x) out1=relu1.forword(y1) y2=affine2.forward(y1) out2=relu2.forword(y2) print(out2) 순전파를 만들었으니 이번에는 역전파룰 구현해야겠지요? 이미 class안에 함수가 있으니 그대로 불러오면 되겠네요. # input x = np.array([[1,2]]) # 가중치와 바이어스 w2 = np.array([[1,4],[2,5],[3,6]]) b1 = np.array([1,2,3]) b2 = np.array([1,2]) # 객체화 affine1=Affine(w1,b1) affine2=Affine(w2,b2) relu1=Relu() relu2=Relu() # 순전파 y1=affine1.forward(x) out1=relu1.forword(y1) y2=affine2.forward(y1) out2=relu2.forword(y2) # 역전파 dy=relu2.backword(out2) dx1,dw1,db1=affine2.backward(y1,dy) dx1=relu1.backword(dx1) dx,dw,db=affine1.backward(x,dx1) print('dx:\\n',dx) print('dw:\\n',dw) print('db:\\n',db) 잘 출력되는 것을 확인 할 수 있습니다. 다음 포스팅때는 계속 해서 다른 활성화 함수도 구현해보도록 하겠습니다.",
    "tags": "Python",
    "url": "/python/2019/12/25/ml-relu-class-2layer.html"
  },{
    "title": "Go언어 문자열 반환하는 함수 만들기",
    "text": "오늘은 GO언어에서 Package, Exported names, 함수에 대해 알아보도록 하겠습니다. 함수는 Python과 비슷해서 어렵지는 않고, 하면서 같이 패키지나 임포트도 잠시 훝어보는 시간을 가져보겠습니다. GO언어가 깔려 있다면 .go 파일을 만들고, 만약에 블로그 예제와 함께 테스트로 돌려보고 싶다면 여기를 눌러 GO놀이터를 이용하여 따라하시면 됩니다. 패키지(Package) GO언어의 모든 프로그램은 패키지로 구성되어 있습니다. 또한 main 패키지에서부터 실행을 시작하며, 패키지 이름은 디렉토리 경로의 마지막 이름을 사용하는 것이 규칙입니다. 예를 들어서 “path/filepath”를 사용한다면 패키지명은 filepath입니다. packahe main import \"fmt\" import \"math\" func main(){ fmt.Println(\"Happy\", math.pi, \"Day\") } // 출력 결과 : Happy 3.141592653589793 Day 임포트(Imports) Go언어에서는 여러개의 “Package”를 소괄호로 감싸서 import를 표현합니다. 그래서 아래와 같이 import문장을 여러번 사용 할 수 있습니다. package main import ( \"fmt\" \"math\" ) func main() { fmt.Printf(\"Now you have %g problems.\", math.Nextafter(2, 3)) } // 출력 결과 : Now you have 2.0000000000000004 problems. 익스포트(Exported names) 패키지를 Import 하면 패키지가 외부로 export한 것들(메서드나 변수, 상수등)에 접근 할 수 있습니다. Go언어에서는 첫 문자가 대문자로 시작하면 그 패키지를 사용하는 곳에서 접근 할 수 있는 exported name이 됩니다. 예를 들어 Foo와 FOO는 외부에서 참조할 수 있지만 foo는 참조 할 수 없습니다. 함수(function) 함수에 관해서는 사실 저번 포스팅에서도 잠깐 다룬적이 있습니다. 하지만 오늘은 조금 더 깊게 들어가보도록 하겠습니다. GO의 함수는 다음과 같은 규칙을 같습니다. 함수는 매개변수(인자)를 가질 수 있습니다. 두 개 이상의 매개변수가 같은 타입일때, 같은 타입을 취하는 마지막 매개변수에만 타입을 명시하고 나머지는 생락할 수 있습니다. 하나의 함수가 여러개의 결과를 반환할 수 있습니다. 하나의 함수가 두개의 문자열을 반환하는 함수를 한번 만들어보겠습니다. GO에서는 함수를 만들때, func를 붙이면 함수를 만들 수 있습니다. package main import \"fmt\" func swap(x, y string) (string, string) { return y, x } func main() { a, b := swap(\"hello\", \"world\") fmt.Println(a, b) } 코드를 보면 함수 swap 라는 함수를 만들었고 문자열 두개를 받아 문자열 두개를 반환하는 함수라는 것을 알 수 있습니다. swap(x,y string)은 x와 y값을 입력 받는데, 문자열을 받겠다는 뜻이며, 뒤에 있는 (string, strain)은 return값을 문자열로 하겠다는 뜻입니다. 변수 func main()을 살펴보면 :=라는 문자를 볼 수 있는데 함수내에서 짧은 선언을 위해 사용합니다. 월래는 변수를 선언할때, var를 사용합니다. 그래서 a라는 변수에 10이라는 숫자를 넣고 싶다면 아래와 같이 써야하는 것이 맞습니다. var a = 10 그렇지만 편하게 :=을 사용하면 var과 명시적 타입을 생략 할 수 있습니다. 하지만! 함수 밖에서는 사용할 수 없으니 주의 해주세요. GO에선 변수 선언과 함께 변수 각각을 초기화 할 수있습니다. 초기화 하는 경우 타입을 생략 할 수 있습니다.",
    "tags": "Go",
    "url": "/go/2019/12/18/go-package-import.html"
  },{
    "title": "논문 리뷰 - Predicting remaining useful life of rolling bearings based on deep feature representation and long short-term memory neural network",
    "text": "Paper URL: https://journals.sagepub.com/doi/full/10.1177/1687814018817184 논문 이름이 엄청 길지만 사실 그렇게 심오하지 않는 논문에 대해 요약 리뷰를 해보겠습니다. 이 논문은deep feature extraction에 관련된 논문입니다. 신경망쪽으로 조금 알고 계신분이라면 아마 deep feature extraction이 무엇인지는 알고 계실테지만 밑에서 한번 짚고 넘어가겠습니다. 또한 이번 논문은 RUL과 관련이 있기 때문에 RUL이 무엇인지도 알아보겠습니다. RUL(remaining useful life) 사실 RUL이라는 단어가 사실 정식 단어인지 아닌지는 잘 모르겠습니다. RUL은 어떠한 시스템이나 구성 요소가 교체하기 전에 의도한 목적에 따라 작동 할 수 있을 것으로 추정되는 잔여 수명을 이야기합니다. 즉, 어떠한 부품이 있을때, 이 부품의 남은 수명 같은 개념입니다. 이 논문은 이러한 RUL, 즉 잔존 수명이 얼마나 남았는지를 예측하기 위한 신경망 모델에 대한 논문이며 CNN과 LSTM을 사용하여 모델을 만들었습니다. Deep feature deep feature 간단하게 말하면 신경망에서 나오는 feature를 이야기 합니다. 아래의 이미지에서 사람의 사진이 신경망을 거치면서 나오는 저 feature들을 이야기 합니다. 신경망이 학습을 하면서 이미지에서 feature가 추출 되는데 우리는 이것을 deep feature이라고 이야기 합니다. 이러한 deep feature extraction(추출)은 신경망이 학습하면서 좋은 feature를 추출하면 할수록 더 좋은 성능을 뽑게 됩니다. 좋은 feature가 있다는 것은 그 많큼 좋은 성능의 Model이라고도 볼 수 있기 때문입니다. 예를 들어 사과와 바나나를 구별할 때, 사과는 동그랗다 라는 것을 알수 있는 feature와 바나나는 길다 라는 것을 알수 있는 feature를 위주로 뽑아가며 학습을 한다고 가정을 해봅시다. 이 feature들이 뚜렷하고 정확하면 할수록 분류를 잘하기 때문에 좋은 모델이라고도 애기할 수 있습니다. 반대로 이야기 하자면, 좋은 feature를 데이터 Input 값으로 넣는다면 좋은 모델을 뽑을수도 있다는 말로도 바꿀 수 있습니다. 이 점들을 이용해서 요즘은 여러 모델을 섞어서 쓰기도 합니다. 이 논문에서는 CNN과 LSTM을 가지고 모델을 만들었으며 이 논문의 모델의 핵심이 deep feature extraction이기 때문에 deep feature을 짚고 넘어가보았습니다. DataSet 논문에서 사용된 데이터는 PHM IEEE 2012 Challenge에서 사용된 데이터입니다. PHM 데이터는 진동 데이터입니다. 진동 데이터를 사용한 이유는 수명이 점점 다해가면서 사용 중인 기계의 진동이 점점 강해진다는 점을 파악하여 만들어진 데이터 입니다. 공장에서는 큰 기계를 주로 사용하는데, 고장이나, 수명을 쉽게 파악하기 위해 진동 센서를 붙여 진동의 세기 등을 보고 파악한다고 합니다. 데이터는 위와 같습니다. 베어링 부품의 진동을 수집하였고 수집 상태는 총 3가지로 나눌 수 있으며, 아래와 같습니다. First operating conditions: 1800 rpm and 4000 N Second operating conditions: 1650 rpm and 4200 N Third operating conditions: 1500 rpm and 5000 N 데이터 상세 컬럼은 아래와 같습니다. 시간, 분, 초 등으로 이루어진 데이터라는 것을 알 수 있으며, 데이터 다운로드는 여기에서 보실 수 있고 데이터 상세 설명은 PDF로 같이 있으니 참고 하시길 바랍니다. Data processing 첫 번째 데이터 전처리 단계에서 원본 데이터를 Hilbert Huang transform(HHT)로 변환합니다. 데이터 변환 후 SVD라고 하여 상관 계수를 구하는 단계를 거치고 데이터를 Input으로 CNN모델에 넣어줍니다. HHT는 FFT와는 조금 다르니, 위 이미지(흐릿하지만)를 보고 참고 하세요. (a)가 원본 데이터, (b)가 FFT, (c)가 HHT한 데이터 입니다. 더 선명한 이미지는 논문에 있으니 참고하면 될 것 같습니다. 두 번째 전처리 단계는 CNN에서 학습이 끝나면 바로 전 레이어인, full-connected layer 에서 feature를 PCA로 돌려서 4까지 뽑고 LSTM의 Input으로 넣었습니다. 데이터 전처리는 HHT와 SVD와 PCA 이 세가지를 했고 실제로 만들어서 돌려본 결과 HHT하는데 데이터 양이 많아서 그런지는 모르겠지만 시간이 무척이나 걸렸어요.. 시간을 조금 줄이고 싶으신 분은 사실 FFT만 하셔도 무난한 성능을 얻으실 수 있을 것 같습니다. CNN_LSTM 이 논문에서 만든 모델 Flowchart는 아래와 같습니다. 데이터 전처리 후 CNN Model을 돌리는데, 정확도가 99%가 될때까지 학습을 돌리고, 99%가 되는 순간 마지막 레이어 full-connected layer에서 deep feature extraction을 하게 됩니다. 아마 이 논문에서 가장 중요한 부분이 이 부분 같습니다. 정확도가 높을 때, feature를 뽑는 이유는 제일 좋은 feature를 LSTM Input값으로 넣기 위함이겠죠. 그런데 여기서 바로 feature를 뽑고 LSTM으로 넘기는게 아니라 PCA를 하게 됩니다. 주성분 분석으로 4까지 뽑은 후 LSTM의 Input으로 넣었습니다. CNN은 2layer, Kernel size는 2*2, feature maps 64, 128 입니다. 마지막으로 fun-connected는 25 neuron 입니다. 논문에서는 CNN Train acc 는 100%, Test acc 는 99.53%라고 합니다. LSTM의 파라미터는 자세히는 적혀 있지는 않지만 epoch 40이라고 하네요. 논문의 데이터는 시계열 데이터 이면서 RUL 예측 하기 위한 모델이기 때문에 평가방법은 RMSE를 사용했다고 합니다. 사실 중간에 25개의 통계식과 함께 통계적으로 feature를 나타내어 비교하는 부분이 있으나, 신경망 구현에 있어서 건너뛰어도 무방하여 건너 뛰었습니다. 요약 및 핵심 이 논문에서는 CNN을 좋은 feature를 추출하기 위함으로 사용하였고 LSTM을 이용하여 시계열 데이터를 예측하였습니다. 이렇게 어떠한 모델에서 feature를 추출하는 것을 Deep feature extraction 이라고 부르며, RUL 평가방법으로 RMSE를 사용합니다. 어렵게 볼 논문은 아닌지라, 간단하게 읽고 모델에 대한 영감 받기 좋은 논문인 것 같습니다. 논문보고 구현한 코드는 나중에 올리게 된다면 다시 업데이트 하겠습니다.",
    "tags": "Paper",
    "url": "/paper/2019/12/17/deep-feature-lstm-rul.html"
  },{
    "title": "Oracle 조인(join)의 종류(equi join, 1999 ANSI)",
    "text": "오라클 조인문법은 어떻게 쓰느냐에 따라 성능 차이가 많이 나기도 하며, 상당히 자주 사용하기 때문에 꼭 알아두어야 합니다. 우선 오늘은 간단하게 오라클 조인의 종류와 간단한 조인 문법에 대해 알아보도록 하겠습니다. Oracle join 이란 ? 오라클은 오라클이라는 회사에서 판매하는 제품 이름입니다. 요즘은 DB제품이 다양하여 꼭 무거운 오라클이 아니더라도 다른 여러 제품을 컨택하여 사용하는 회사들이 많습니다. 여태까지 포스팅 했던 문법들도 전부 Oracle 문법입니다. 그러나 다행이도 DB문법은 다른 회사 제품이더라도 엄청 크게 다르지 않으니, 다른 DB를 사용하고 있어도 큰틀 잡기에는 무리는 없을 것입니다. Oracle에서 조인(join)이란, 여러 개의 테이블의 컬럼의 결과를 하나의 결과값으로 출력할 떄 사용하는 SQL 문법을 이야기 합니다. 조인 문법은 equi join, non equi join, outer join, self join을 사용하는 오라클 조인 문법과 1999 ANSI 조인 문법이 있습니다. 오라클 조인 문법 오라클 조인 문법에는 equi join, non equi join, outer join, self join이 있다고 위에서 언급했습니다. 생각보다 많아서 어려워 보이지만 어렵지 않습니다. 아래의 표를 참고 한다면 쉽게 이해할 수 있습니다. equi join : 조인하려는 테이블 사이의 연결고리가 = 인 경우의 조인 문법 non equi join : 조인하려는 테이블 사이의 연결고리가 /= 인 경우의 조인 문법 outer join : equi 조인으로는 볼 수 없는 결과를 볼 때 사용하는 조인 문법 self join : 자기 자신의 테이블과 조인하는 조인 문법 두 번째 조인인 1999 ANSI 문법 종류는 아래와 같습니다. on 절을 이용한 join using 절을 이용한 join left/right/full outer join natural join cross join 이렇게 오라클 조인과 1999 ANSI조인으로 나뉘는 이유는 오라클 조인으로도 조인이 되지 않는 데이터가 있기 때문에 1999 ANSI조인을 사용하여 데이터를 뽑아냅니다.",
    "tags": "DB",
    "url": "/db/2019/12/11/sql-qracle-join-8.html"
  },{
    "title": "파이썬 while loop문을 사용하여 log함수 구현하기",
    "text": "for문과 if문은 가장 흔하고 많이 쓰이는 문법이지만, while문도 자주 쓰이는 문법 중 하나입니다. 문법도 어렵지 않아서 금방 익힐 수 있습니다. while문이 가장 쓰기 편한 문법 중 하나인데, 그 이유는 조건이 참일 때, 실행시킨다던가, 아니면 무한루프로 계속 돌리다가 어느 순간에 멈추는 등 편리하기 때문입니다. while while문 구조는 아래와 같습니다. 위 구조대로 간단하게 숫자 하나를 입력 받아서 입력받은 숫자까지 출력하는 것을 하나 출력해보겠습니다. a = int(input(\"숫자를 입력하세요 \")) i = 0 while i &lt; a: i = i+1 print(i) 이번에는 while loop문으로 log함수를 구현해볼까요? a = int(input(\"밑수를 입력하세요 ~\")) b = int(input(\"진수를 입력하세요 ~ \")) c=a cnt=1 while a &lt; b: cnt += 1 a=a*c print(\"로그 값은 %i 입니다\" %cnt) 이번에는 간단한 알고리즘을 풀어보도록 하겠습니다. 지겹도록 많이 풀었던 팩토리얼을 while문으로 구현해보겠습니다. a = int(input(\"팩토리얼 숫자를 입력하세요 ~ \")) x=a while x &gt; 1: x -= 1 a=a*x print(a)",
    "tags": "Python",
    "url": "/python/2019/12/06/python-while-loop.html"
  },{
    "title": "파이썬 for문에서 if문 안쓰고 특정 블럭만 실행하기(continue,break)",
    "text": "for문 같은 반복문에서 쓸 수 있는 문법들이 꽤나 많은데요, 오늘은 반복문이 실행되는 동안에 다른 코드 블럭만 실행되게 할 때, 사용하는 문법에 대해 알아보도록 하겠습니다. continue 문 continue문은 반복문이 실행되는 동안 특정 코드 블럭을 실행하지 않고 다른 블럭만 실행하게 할 때 사용하는 문법입니다. 예를 들어가면서 continue문을 작성해보겠습니다. 숫자 1부터 10까지 출력하는데 중간에 5가 나오지 않게 할때, continue문을 사용한다면 아래와 같습니다. for i in range(1,11): if i == 5: continue print(i) continue문은 생각보다 유용하게 많이 사용되는데, 조금 더 응용해서 사용한다면 60점 이상인 학생에게 축하 메세지를 보내고 나머지한테는 안보는 프로그램을 짜게 된다면 아래와 같이 짧게 짤 수 있습니다. jumsu = [ 90, 25, 67,45,80 ] num = 0 for i in jumsu: num = num +1 if i &lt; 60: continue print(\"%d 번 학생 축하합니다.\" %num) 생각보다 continue문은 유용하죠? break문 break문은 말 그대로 루프를 중단 시키는 역활을 하는 문법입니다. 그래서 예를 들어보면 아래와 같이 사용할 수 있습니다. scope = [ 1,2,3 ] for x in scope: print(x) break else: print('perfect') 위와 같이 쓸 수 있습니다.",
    "tags": "Python",
    "url": "/python/2019/12/05/python-for-continue-break.html"
  },{
    "title": "GO언어 변수 선언하기",
    "text": "GO언어의 구문과 자료형 그리고 변수에 대해 알아보도록 하겠습니다. GO언어는 언어의 장벽이 그렇게 높지 않기 때문에 금방 배우실 수 있습니다. 특히나 C언어를 주로 이용하여 개발을 하셨던 분이거나 익숙하시다면 더욱이 GO언어가 쉽다고 느낄 수 있습니다. GO언어의 구문 주석 처리 할때, 한줄 주석은 (//), 여러 줄 주석은 (/* */)으로 합니다. 들여쓰기는 일반적으로 탭을 사용하지만 이부분은 크게 신경쓰지 않으셔도 됩니다. 그 이유는 Format이라는 기능을 이용해 일괄적으로 다 맞추어주기 때문이죠. 이 부분은 아래에서 설명하겠습니다. GO언어에서 코드블록을 시작하는 중괄호의 위치가 중요한데, 중괄호의 위치는 시작하는 코드와 꼭 같은 줄이여야 합니다. 문장의 끝에 보이지는 않지만 (;)을 붙이기 때문입니다. 그렇기 때문에 같은 줄에 중괄호가 존재하지 않는다면 에러가 난답니다. 예는 아래와 같습니다. 위와 같이 코드를 func main() { 으로 시작하는 것이 올바른 구문입니다. 하지만 아래와 같이 코드를 짜게 된다면 에러가 발생하여 실행이 되지 않으니 주의해야한답니다. 위와 같은 예는 실행이 되지 않습니다. 이렇게까지 맞쳐야 하는 가장 큰 이유는 사람들과의 코드 스타일을 맞추기 위해라고 들었던 것 같습니다. 하지만 이런것이 불편하다면, GO놀이터에선 Format버튼을 누르면 되고 GO를 설치했다면, 코드 실행 전 gofmt -w 파일이름.go 한 줄 적고 실행하면 자동으로 맞춰준답니다. 그렇다면 간단하게 Hello를 출력해볼까요 ? 자료형 및 변수 GO언어에서 자료형은 정적 자료형이지만 동적 자료형처럼 사용할 수 있습니다. var x int var arr [5]int // 배열 그리고 변수 선언과 동시에 값을 할당하며, 정적 자료형이지만 자료를 보고 추론하는 기능이 있어서 자료형을 생략해도 된답니다. var x int = 10 // 선언과 동시에 값 할당 var i = 10 // 자료형 추론 함수에서 자료형의 표현은 아래와 같습니다. func(int , int) // 두 정수를 인자로 받는 함수 func(int) int // 하나의 정수를 인자로 받고 하나의 정수를 반환하는 함수 func(int , func(int, int)), func(int) int // 정수와 두 정수를 받아들이는 함수를 받고, 정수 하나를 받고 정수를 반환하는 함수를 반환 GO에서 포인트 선언을 할 수 있는데, 아래와 같이 선언 할 수 있습니다. var p *int 자료형 관련 주의 사항이 하나 있는데, int와 uint는 환경에 따라 32비트일 수 도 있고, 64비트일 수 도 있습니다. 그래서 반드시 64비트여야 한다면 int64나 uint65를 이용해야합니다. 자료형 추론 위에서 GO에서는 자료형이 무엇인지 알 수 있는 경우에는 자료형을 생략 가능하다고 했습니다. 예는 아래와 같습니다. var i = 10 var p = &amp;i // i의 주소값을 p에 저장 또한 var 키워드도 생략이 가능합니다 대신에 = 대신 := 를 사용해야 합니다. i := 10 p := &amp;i GO언어가 약간 생소하게 다가오지만 그래도 할만한것 같네요. 다음 포스팅에선 Go의 기본 문법에 대해 자세하게 다루겠습니다.",
    "tags": "Go",
    "url": "/go/2019/12/05/go-function.html"
  },{
    "title": "GO 언어 설치하기",
    "text": "GO언어를 설치해보도록 하겠습니다. 생각보다 설치하는 건 어렵지도 않고 여러가지 설정 할 것도 별로 없으니 10분이면 설치할 수 있습니다. 우선 설치파일 다운해봅시다. 여기를 클릭하면 아래와 같은 화면이 나오는데 OS에 맞는 설치파일 다운합니다. 저는 맥과 윈도우 두가지를 같이 쓰고 있는데, 우선 윈도우랑 맥이랑 그렇게 설치방법이 다르지 않으니 그냥 윈도우로 진행하겠습니다. 혹시나 GO언어를 설치는 하기 싫지만, 예제나, 코드를 구현하고 싶다면 여기에 들어가시면 GO언어를 사용하실 수 있습니다. 이 웹은 Go놀이터라고 합니다. 설치도 안해도 되면서 항상 최신버전으로 유지하고 있으니 간접적으로 체험하시고 싶거나 간단히 테스트 할때, 사용하시면 유용할 것같네요. 설치를 끝냈으면 GOROOT를 설정해야합니다. 만약 설치 경로가 C:\\Go라면 GOROOT를 설정할 필요가 없습니다. 그렇지 않은 경우에는 따로 설정을 해주셔야 합니다. 따로 설정 할 경우에는 시스템 속성 -&gt; 환경변수 -&gt; ‘새로 만들기(N)’ 선택 후 설정 해주시면 되겠습니다. GOPATH설정 또한 해줘야하는데, 아래와 같이 설정하시면 됩니다. 전 gogo라는 폴더를 따로 만들었고 GOROOT 잡은것과 동일하게 gogo폴더로 PATH잡아주면 설정은 끝납니다. cmd.exe실행 해서 아래와 같이 명령어를 치면 설정했던 경로를 확인 해볼 수 있습니다. echo %GOROOT% echo %GOPATH% 이제 설치는 끝났습니다. 생각보다 간단하네요. 다음 포스팅때, 기본 문법과 실행법에 대해 알아보겠습니다.",
    "tags": "Go",
    "url": "/go/2019/12/04/go-installing.html"
  },{
    "title": "GO 언어란?",
    "text": "GO언어는 요즘 Docker가 떠오르면서 같이 떠오르는 새로운 언어입니다. 또한 구글에서 만든 언어라고 해서 사람들이 더 관심을 가지고 보는 언어죠. GO언어가 어떤 언어인지 알아보겠습니다. GO GO언어는 범용 프로그래밍 언어이며, 깔끔하고 간결하게 생산성 높은 프로그래밍이 가능합니다. GO언어는 문법이 그렇게 복잡하지 않아서 생각보다 문법적으로는 어렵지 않습니다. 또한 빠른 컴파일이 가능하다는 것과 정적 자료형 언어이지만 동적 자료형 언어로 프로그래밍을 작성하는 것 같은 느낌을 주는 것이 가장 큰 장점입니다. 마지막으로 동시성을 지원하는 코드를 쉽게 작성 가능 하다는 것도 장점으로 들 수 있겠습니다. GO언어가 생산성이 높은 이유 위에서 말했던 것처럼 GO언어가 생산성이 높다고 이야기 했는데, 그 이유는 무엇일까요? GO언어는 편리한 자료형 추론을 제공하는 것은 물론이고 반복해서 자료형 이름을 쓰지 않아도 된답니다. 특히나 제일 좋은 것은 소스 코드 형식을 자동으로 맞춰주는 도구 등 여러 편리한 도구들이 있습니다. 함수 리터럴 및 클로저를 자유자재로 사용 가능하고, 명시적으로 인터페이스를 지정하지 않아도 인터페이스 구현이 가능합니다. 유닛테스트 또한 다른 언어에 비해 무척이나 간단합니다. GO 언어에서는 채널이라는 개념이 있는데, 이 채널을 이용하여 동시성 구현을 락 등을 이용하지 않고 간편하게 할 수 있습니다. 그래서 언어 고유의 지원으로 교착 상태나 경재 상태 파악이 쉬다는 것을 알 수 있습니다. 즉, 고성능 동시성 프로그램을 간편하게 구현을 할 수 있으며, 컴파일 속도가 매우 빨라서 컴파일 및 테스트를 반복적으로 수행하면서 코드를 작성 할 수 있으며, 메모리 관리에 쉽습니다. 현재 GO언어는 2015년 2월 17일에 1.4.2 버전이 안정 버전으로 출시 되었으며 1.0 이후 2.0이 되기 전까지 하위 호환성을 해치지지 않는 선에서 새로 업데이트 되고 있다고 합니다. 다음 시간에는 GO설치 및 구문에 대해서 포스팅 해보도록 하겠습니다.",
    "tags": "Go",
    "url": "/go/2019/12/03/go.html"
  },{
    "title": "재귀 알고리즘으로 팩토리얼과 최대공약수 구하기 (Python)",
    "text": "계속해서 재귀 알고리즘에 익숙해지기 위해 알고리즘 단골 문제인 팩토리얼과 최대공약수를 풀어보겠습니다. 재귀 함수가 무엇인지는 저번 포스팅에서 설명했으므로 이번에는 자세한 이야기는 생략하겠습니다. factorial 10!을 구할껀데, 10!을 풀어서 이야기하면 10 * 9 * 8 * 7 * 6 * 5 * 4 * 3 * 2 * 1 이고 , 계산하면 3,628,800 입니다. 구현하면 아래와 같습니다. def factorial2(count): if count &gt; 0: return count * factorial2(count-1) elif count==0: return 1 factorial2(10) 코드 설명을 조금 하자면, 아래와 같습니다. fact(10) 10 * fact(9) 9 * fact(8) 8 * fact(7) 7 * fact(6) .... 그렇다면 이번에 최대 공약수를 출력해보는 재귀함수를 만들어 보겠습니다. 사람마다 알고리즘 짜는 방식이 조금 다르니, 그 점을 유의 해주시고 저는 함수 두개를 만들어 구현해보겠습니다. def find_gcd_1(a,b): return a if b == 0 else find_gcd__1(b,int(a%b)) def find_gcd_2(a,b): return find_gcd_1(a,b) if a &gt; b else find_gcd_1(b,a) print(find_gcd(16,20)) 흐음… 만약에 함수로 한다고 하면 아래와 같이 만들 수 있습니다. def find_gcd(num1,num2): if num1&lt;num2: (num1,num2)=(num2,num1) if num1&gt;num2: num1=num1-num2 return find_gcd(num1,num2) else: return num1 # 또는 def find_gcd(a,b): return a if b == 0 else find_gcd(b,int(a%b)) # 두 수의 나머지와의 공약수 print(find_gcd(20,16)) 이렇게 보니 재귀가 쉬운것 같으면서도 어렵네요…",
    "tags": "Algorithm",
    "url": "/algorithm/2019/12/03/algorithm-recursive-function-python.html"
  },{
    "title": "재귀 알고리즘으로 구구단 출력하기(Python)",
    "text": "알고리즘 문제를 풀면서 반드시 알아야 할 알고리즘 중 하나는 아마 재귀 알고리즘일거라 생각합니다. 재귀 함수를 사용할 수 있는 곳에서는 재귀 함수를 사용하며 알고리즘을 풀면서 공부를 해야 된답니다. 재귀 함수 재귀 함수는 함수내에서 다시 자신을 호출한 후 그 함수가 끝날때까지 함수 호출 이후의 명령문을 수행하지 않습니다. 즉, “반복문 + 스택구조가 결합된 함수를 재귀함수라고 볼 수 있습니다. 스택 구조가 결합된 함수에 대해서 조금 더 풀어서 이야기해보도록 하겠습니다. 스택구조가 결합되었다는 의미는 먼저 들어간 데이터가 가장 마지막에 나오는 구조, 나중에 들어간 데이터가 가장 먼저 나오는 구조(후입선출) 을 말합니다. 쉬운 예를 들어보겠습니다. def hap(a,b): print(a+b) def gop(a,b): print(a*b) def hap_gop(a,b): hap(a,b) gop(a,b) print(hap_gop(7,8)) hap_gop함수는 그냥 a와 b값을 받아서 hap 함수와 gop함수에 던져주는 역활만 수행합니다. hap_gop함수가 hap함수와 gop함수를 호출(다른 함수)한다는 뜻이죠. 그런데, “재귀함수는 자기 자신의 함수를 호출한다” 라는 특징이 있습니다. def countdown(n): if n == 0: print('발사') else: print(n) countdown(n-1) # 자기 자신을 호출 ! print(countdown(5)) 더 쉬운 예로 위의 함수와 똑같이 구현하여 구구단 2단을 출력해보도록 하겠습니다. def tow_count(n): if n != 0: print('2 x %d = %d' %(n,2*n)) tow_count(n-1) tow_count(10) 위와 같이 결과가 나왔을 때, 다시 2 x 1 = 2 … 로 출력하려면 어떻게 해야댈까요 ? 재귀 함수를 이용하면 쉽게 구현할 수 있습니다. 아까 말했던 것 처럼 스택구조가 결합된 함수이기 때문에 먼저 들어간 데이터가 가장 마지막에 나오는 구조라고 했습니다. 그래서 아래와 같이 재귀함수를 구현하여 출력할 수 있습니다. def tow_count(n): if n != 0: tow_count(n-1) print(('2 x %d = %d') %(n,2*n)) tow_count(10) 구구단이 제대로 출력되는 모습을 볼 수 있습니다. 즉, 재귀는 숫자를 0까지 계속 waiting 시켰다가 나가는 스택구조를 띄고 있다는 것을 알수 있습니다.",
    "tags": "Algorithm",
    "url": "/algorithm/2019/12/03/algorithm-python.html"
  },{
    "title": "K-Nearest Neighbors(Knn)을 이용한 동물 분류하기",
    "text": "여태까지 작은 데이터로 분류했으니 이번엔 약간 조금 더 큰 데이터를 이용해보도록 하겠습니다. 데이터는 여기에서 zoo.csv를 다운 받아주세요. 제가 사용한 데이터의 원문은 여기를 클릭하면 보실 수 있습니다. DataSet 데이터는 동물 종류에 따른 특징들이 있고, 라벨은 포유류, 조류, 파충류, 어류, 양서류, 곤충류, 갑각류 총 7가지가 있습니다. 우선 데이터를 불러와서 동물의 비율이 어떻게 되는지 확인해보겠습니다. zoo&lt;-read.csv(\"zoo.csv\",stringsAsFactors=FALSE, header =F ) table(zoo[18]) # 종류와 건수 보기 round(prop.table(table(zoo[18])), 2)*100 # 비율 보기 이제 데이터를 정규화하는 작업을 위해 normalize함수를 이번에 만들어보려고 합니다. normalize는 벡터 정규화로써 정규화 방식과 조금 다르지만 knn 알고리즘을 사용할 때 단위를 맞출 수 있는 방법 중 하나입니다. 여기서 TMI로 하나 더 이야기 하자면, 정규화는 데이터 군 내에서 특정 데이터가 가지는 위치를 볼 때 사용하고 표준화는 표준편차를 이용한 식이며 2개 이상의 단위가 다를 때, 대상 데이터를 같은 기준으로 보려고 할 때 주로 사용합니다. Normalize 우선 normalize 함수를 만들어보도록 하겠습니다. normalize &lt;- function(x){ return((x-min(x))/(max(x)-min(x)))} 정규화가 잘되는지 확인하기 위해 가라 데이터를 넣어보도록 하겠습니다. normalize(c(1,2,3,4,5)) # 0.00 0.25 0.50 0.75 1.00 normalize(c(10,20,30,40,50)) # 0.00 0.25 0.50 0.75 1.00 동일한 결과를 얻을 수 있음을 확인 할 수 있습니다. 즉, 정규화가 되엇다는 걸 확인 할 수 있습니다. 이제 데이터 전체에 정규화를 하려고 하는데, normalize 함수에 하나씩 넣을순 없으니, 내장함수 하나와 같이 사용하여 전체 데이터를 정규화 시켜보도록 하겠습니다. lapply()와 위에서 만들었던 normalize함수를 사용하면 전제 데이터를 정규화 시킬 수 있습니다. ( 단, 라벨 데이터는 정규화 작업을 하시면 안된다는 점 ! ) zoo2_n &lt;- as.data.frame(lapply(zoo[,2:17], normalize)) knn 이제 train과 test 데이터 라벨을 변수로 생성 하겠습니다. zoo2_n_train &lt;- zoo2_n[1:100, ] zoo2_n_test &lt;- zoo2_n[101,] zoo2_train_label &lt;- zoo[1:100,18] zoo2_test_label &lt;- zoo[101,18] 이제 knn만 돌리는 일만 남았네요. 제 포스팅을 쭉 보셨다면 여태까지 많이 해봐서 익숙하죠 ? result &lt;- knn(zoo2_n_train, zoo2_n_test, zoo2_train_label, k=1) 원본 데이터를 확인해보도록 하겠습니다. 마지막 2와 출력 결과인 2가 일치하는 결과를 확인할 수 있습니다. 다음 포스팅에는 유방암 데이터를 이용하여 악성인지 양성인지 분류해보고 조금 더 깊게 들어가기 위해 이원교차표와 적절한 k값을 찾는 방법에 대해 알아보겠습니다. 전체 코드는 여기에서 보실 수 있습니다.",
    "tags": "R",
    "url": "/r/2019/12/03/k-nearest-neighbors-knn5.html"
  },{
    "title": "파이썬 for문과 if문을 사용해서 파일 읽기",
    "text": "컴퓨터 언어도 “언어” 이기 때문에 계속 사용하면서 익히는 버릇을 길러야 합니다. 그러니, 저번 포스팅때 배운 for문과 if문에 조금 더 익숙해지기 위해 csv파일을 이용하여 if문과 for 문을 같이 사용해보도록 하겠습니다. 데이터는 여기에서 다운받으실 수 있습니다. 혹시나 csv파일이 있다면, 굳이 다운은 받지 않으셔도 됩니다. csv 파일 불러오기 우선 csv 파일을 불러오겠습니다. 만약에 같은 경로에 있는 파일이 아니라면, “../../” 이런식으로 Path를 잡아주시면 된답니다. python에서 csv 파일 읽은 모듈이 따로 있어서 코드가 길지 않습니다. import csv file = open(\"emp2.csv\") emp_csv = csv.reader(file) for문과 if문 이제 불러왔으니, for 문을 이용하여 SCOTT인 사원의 월급을 출력해겠습니다. for emp_list in emp_csv: if emp_list[1] =='SCOTT': print(emp_list[5]) (사실 이렇게까지 힘들게 안해도 되지만, 저는 for문과 if문에 익숙해지기 위해 이렇게 짜증나게 코드를 짜는것 뿐..) 이번엔 input을 주고 출력하는 것을 만들어보겠습니다. for문으로 데이터를 하나씩 읽으면서 input값과 값이 일치한다면 출력하는 형태로 코드를 작성하면 되요. 저는 이름을 입력받아서 월급을 출력하는 코드를 작성해보도록 하겠습니다. import csv file = open(\"emp2.csv\") emp_csv = csv.reader(file) a = input(\"이름을 입력하세요 \") for emp_list in emp_csv: if emp_list[1] == a: print(emp_list[5]) 그럼 이번엔 if문을 여러개 사용하여 여러가지 조건을 걸어보겠습니다. python에서 if 문을 여러개 사용할때는 elif를 사용합니다. 해석하자면 조건이 true이면 실행하고 조건이 flese이면 실행을 하지 않다라는 뜻이고, 마지막 else는 위에 있는 조건 그 무엇도 암것도 걸리는게 없다면 실행하여라 라는 뜻입니다. 그렇다면 위 코드 예제를 보고 이름을 입력했을때, 고소득자인지, 저소득자인지를 출력하는 python을 작성해보겠습니다. import csv file=open(\"emp2.csv\") emp_csv = csv.reader(file) a=input(\"이름을 입력하세요 \") for emp_list in emp_csv: if emp_list[1]==a.upper(): # upper은 소문자를 입력해도 대문자로 바꿔준다. if int(emp_list[5]) &gt;=3000: print(\"고소득자입니다\") elif int(emp_list[5]) &gt;=2000: print(\"적당합니다\") else: print(\"저소득자입니다\") 역시 파이썬은 코드가 간결하고 좋은 것 같네요.",
    "tags": "Python",
    "url": "/python/2019/12/02/python-for-if-file-read.html"
  },{
    "title": "K-Nearest Neighbors(Knn)을 이용한 과일 데이터 분류",
    "text": "오늘은 조금 재미있는 데이터를 가져와봤습니다. 토마토가 야채인지, 과일인지, 단백질인지를 knn을 통하여 분류해서 알아내는 작업을 해보겠습니다. 이번 포스팅에서 사용 되는 데이터는 아래에서 데이터를 직접 만들어서 작성 할 거라서, 따로 다운 받으실 필요는 없습니다. DataSet 데이터는 아래와 같습니다. 과일은 사과, 바나나, 당근, 치즈 등 작지만 다양한 데이터 입니다. food &lt;- data.frame(ingredient = c(\"apple\", \"bacon\", \"banana\", \"carrot\", \"celery\", \"cheese\", \"cucumber\", \"fish\", \"grape\", \"green bean\", \"lettuce\", \"nuts\", \"orange\", \"pear\",\"shrimp\"), sweetness = c(10,1,10,7,3,1,2,3,8,3,1,3,7,10,2), crunchiness = c(9,4,1,10,10,1,8,1,5,7,9,6,3,7,3), class = c(\"Fruits\",\"Proteins\",\"Fruits\",\"Vegetables\", \"Vegetables\",\"Proteins\",\"Vegetables\", \"Proteins\",\"Fruits\",\"Vegetables\", \"Vegetables\",\"Proteins\",\"Fruits\", \"Fruits\",\"Proteins\")) 출력하면 아래와 같이 출력이 됩니다. 그럼 이제 토마토 데이터를 만들어 보겠습니다. 토마토 데이터는 아래와 같이 만들어주면 된답니다. tomato &lt;- data.frame(sweetness = 6, crunchiness = 4) knn food 에서 2번째 컬럼과 3번째 컬럼을 train 데이터로 사용하겠습니다. train_data &lt;- food[,c(2,3)] 그리고 라벨은 데이터의 마지막 컬럼인 class 입니다. 그래서 아래와 같이 라벨을 따로 변수에 담아주겠습니다. train_label&lt;-food[,4] 그럼 이제 바로 토마토를 넣어서 knn을 해보겠습니다. library(class) result1&lt;-knn(train_data,tomato ,train_label, k=3, prob=TRUE) 전체 코드는 여기에서 확인하실 수 있습니다.",
    "tags": "R",
    "url": "/r/2019/12/02/k-nearest-neighbors-knn4.html"
  },{
    "title": "파이썬 for문을 사용하여 별찍기",
    "text": "반복문은 어떤 언어라고 할 것도 없이 정말 중요하고, 자주 쓰이는 문법입니다. 특히나 for문은 더욱 더 그렇죠. 그럼 오늘은 간단하게 for문의 기본적인 구조와 별찍기를 해보겠습니다 . for 파이썬의 for 문의 기본적인 구조는 아래와 같습니다. for문은 리스트나 튜플, 문자열의 첫번째 요소부터 마지막 요소까지 차례로 변수에 대입되어 “수행할 문장1”, “수행할 문장2”등이 수행됩니다. 그래서 조금 더 자세하게 문법을 설명하자면, 아래와 같습니다. 숫자 1부터 3까지를 for문을 이용하여 출력해보겠습니다. for i in (1,2,3): print(i) 문자열도 for문으로 돌려보겠습니다. for i in 'i like tteokbokki': print(i) 이번에 range를 사용하여 1부터 10까지 증가하는데, 2씩 증가하도록 출력해보도록 하겠습니다. for i in range(1,11,2): # 1부터 10까지 증가하는데, 2씩 증가시켜라 print(i) # 출력 결과 1,3,5,7,9 range를 사용하여, 별찍기를 해보도록 하겠습니다. a=str(\"★\") for i in range(1,11): print(a*i) 또는 for i in range(1,11): print(\"★\"*i) 이번에 반대로 별을 찍어볼까요? 이렇게 별을 찍어보겠습니다. a=str(\"★\") for i in range(10,0,-1): print(a*i) 이번엔 조금 더 응용해서 ★으로 사각형을 만들어보겠습니다. 단, 이번에는 가로/세로를 입력을 받아서 ★를 출력하되, 중첩 for문을 사용하지 않고 작성해보도록 하겠습니다. a=int(input(\"가로의 숫자를 입력하세요 \")) b=int(input(\"세로의 숫자를 입력하세요 \" )) for i in range(b): print(\"★\"*a) 중첩 for문을 작성하여 사용한다면 아래와 같이 작성 할 수 있습니다. for i in range(b): c = ' ' # 초기화 for j in range(a): c += '★ ' print(c) 마지막으로 for문으로 구구단을 출력해보겠습니다. for i in range(1,10): result = ' ' for j in range(2,10): result += (str(j) + 'X'+ str(i) + '=' + str(i*j).ljust(13)) print(result) 또는 for i in range(1,10): result = ' ' for j in range(2,10): result += (str(j) + 'X'+ str(i) + '=' + str(i*j) + \\'t' ) print(result)",
    "tags": "Python",
    "url": "/python/2019/12/01/python-for-stars-print.html"
  },{
    "title": "신경망이 학습 하는 원리? 역전파(backpropagation)에 대해 알아보자.",
    "text": "흔히들 역전파, 영어로 하면 backpropagation이라고 하는 용어를 한번쯤은 들어보셨을 거라 생각합니다. 인공지능은 어떻게 학습하는 것인지, 역전파는(backpropagation)가 무엇인지 알아보도록 하겠습니다. 역전파 (backpropagation) 역전파는 말이 어려워보이는 것 뿐이지, 생각보다 어려운 개념은 아닙니다. 역전파를 이야기 할때는 순전파라는 개념을 알아야 되기 때문에 함께 이야기 하겠습니다.우선 개념 이해를 위해 쉬운 예를 들어보도록 하겠습니다. 우리는 중간고사와 기말고사등과 같은 시험을 잘 보기위해 학교에서나 학원에서 수업을 듣고는 합니다. 이렇게 공부를 하고 우리는 시험을 보게 되는데, 이러한 과정을 순전파(forwardpropagation)라고 합니다. 반대로 역전파는 이러한 우리가 다음 시험을 더 잘보기 위해 오답노트를 작성하여 공부를 다시 하고는 하는데 이 과정을 역전파(backpropagation)라고 합니다. 그렇다면 실제로 인공지능이 학습하는 부분은 순전파일까요? 역전파일까요? 인공지능이 실제로 학습하는 부분은 역전파 부분입니다. 그래서 역전파 부분이 중요하답니다. 저번 포스팅때 이야기 했던 수치미분! 다들 기억 하시나요 ? 수치 미분을 기울기를 구하여 좋은 학습 결과를 뽑아낼 수 있지만, 매번 다시 계산하면서 기울기를 다시 구하는 방법은 비효울적이고 성능이 느립니다. 그래서 신경망에서 학습 처리할때, 최소화 되는 함수의 경사를 효율적으로 계산하기 위한 방법이 역전파랍니다. 역전파라는 개념이 없을 경우 아래 그림과 같이 순전파 라는 개념만 있게 됩니다. 이렇게 되면 무엇이 문제가 되냐면, 만약 output의 결과가 이상하다면, 다시 처음부터 학습을 해야 합니다. 그렇게 되면 학습시간이나, 컴퓨터 성능이 느리고 비효율적이게 되죠. 그렇기 때문에 역전파라는 개념이 생기게 된거랍니다. output층부터 차례대로 역방향으로 따라 올라가서 각 층에 있는 노드의 오차를 계산하는 겁니다. 그렇게 각 노드이 오차만 계산하고 그 오차를 사용해서 함수의 기울기를 계산하는 방식으로 학습 결과에 영향을 주는거죠. 즉, 전파된 오차만을 다시 계산해서 가중치를 조정하는 것이 역전파라고 할 수 있습니다. 간단한 신경망 구현하기. 우선 행렬을 입력받아서 결과를 출력하는 forward 함수를 만들어 보겠습니다. import numpy as np x = np.array([1,2]) # 입력값 w = np.array([[1,3,5],[2,4,6]]) # 가중치 b = np.array([1,1,1]) # 바이어스 def forward(x,w,b): out = np.dot(x,w)+b return out print를 찍으면 [6 12 18]이 나오는 것을 알 수 있습니다. 이번에는 역전파 함수를 backward라는이름으로 생성해볼까요? x = np.array([[1,2],[3,4]]) w = np.array([[1,3,5],[2,4,6]]) # 가중치 dy = np.array([[1,1,1],[2,2,2]]) # 역전파로 들어오는 값 def backward(x,w,dy): dx = np.dot(dy,w.T) # x의 역전파 dw = np.dot(x.T, dy) # w(가중치)의 역전파 db = np.sum(dy,axis = 0) # bias의 역전파 (역전파로 들어온 값의 합) # axis = 0 열을 더한다. return dx,dw,db 그럼 이번엔 조금 더 나아가서 위에서 만든 forward 함수와 backward 함수를 묶어서 class 로 생성해보겠습니다. 저는 class이름을 Affine 이라고 하겠습니다. class Affine: def __init__(self, w,b): self.w = w self.b = b def forward(self,x): out = np.dot(x, self.w)+self.b return out def backward(self, x, dy): dx = np.dot(dy, self.w.T) dw = np.dot(x.T, dy) db = np.sum(dy,axis=0) return dx,dw,db 이제 이렇게 클레스를 만들었으니 affine 클래스를 사용해서 2층짜리 신경망을 구현해보도록 하겠습니다. 입력값과 w,b가 아래와 같을때, 순전파 결과 행렬을 출력해보도록 하겠습니다. x=np.array([[1,2]]) # x.shape : (1,2) w1=np.array([[1,3,5],[2,4,6]]) # w1.shape : (2,3) w2=np.array([[1,4],[2,5],[3,6]]) b1=np.array([1,2,3]) b2=np.array([1,2]) 클래스는 이미 만들었으니, 출력해보겠습니다. affine1=Affine(w1,b1) affine2=Affine(w2,b2) y1=affine1.forward(x) y2=affine2.forward(y1) print('출력 행렬:\\n',y2) 출력하면 [93 211]이 나오는 것을 확인 할 수 있습니다. 이번엔 역전파를 구현해보도록 하겠습니다. dy=y2 affine1=Affine(w1,b1) affine2=Affine(w2,b2) y1=affine1.forward(x) y2=affine2.forward(y1) dx1,dw1,db1=affine2.backward(y1,dy) dx,dw,db=affine1.backward(x,dx1) print('dx:\\n',dx) print('dw:\\n',dw) print('db:\\n',db) 생각보다 어렵지 않네요. 여기서 나중에 활성화 함수나, loss 함수 등 여러가지 추가하면 신경망이 되는데, 그 부분은 담 포스팅에서 다루겠습니다.",
    "tags": "MachineLearning",
    "url": "/machinelearning/2019/11/29/machine-learning-backpropagation.html"
  },{
    "title": "PLSQL로 두개의 숫자의 합 구하기",
    "text": "주 사용언어는 Python이지만, 복습의 의미로 다양한 언어로 알고리즘을 차근차근 풀어볼까 합니다. 오늘은 간단하게 오래전에 습득했지만, 전혀 사용하고 있지 않은 언어인PLSQL을 복습할겸, PLSQL로 간단한 문제를 풀겠습니다. PLSQL로 어떻게 알고리즘을 풀 수 있는가? 를 물어보신다면, PLSQL이 무엇인가? 라는 포스팅이 있으니 참고 하라고 말씀드리고 싶습니다만! 안보실꺼 같으니 다시 한번 설명 드리자면, PLSQL은 비절차적 언어인 SQL + 프로그래밍(if,loop)를 이야기합니다. 즉, 절차적 언어로 만드는 프로그래밍입니다. if문과 loop문을 사용할 수 있습니다. 그렇다면 가장 기본적인 두 수자의 덧셈을 만들어보겠습니다. accept p_num1 prompt '첫번째 숫자를 입력하세요 ~ ' accept p_num2 prompt '두번째 숫자를 입력하세요 ~ ' declare v_num1 number(10) :=&amp;p_num1; v_num2 number(10) :=&amp;p_num2; v_num3 number(10); begin v_num3 := v_num1 + v_num2; dbms_output.put_line('총합은 : ' || v_num3); end; / 사실 너무 쉬운거라 알고리즘이라고 하기가 좀 그렇지만, 그래도 간단하게 PLSQL로 숫자의 합을 구하는 문제를 풀어보았습니다.",
    "tags": "Algorithm",
    "url": "/algorithm/2019/11/29/algorithm-find-the-sum-of-numbers-plsql.html"
  },{
    "title": "파이썬 if문 사용해서 홀짝 출력하기",
    "text": "Python의 if문 개념에 대해 알아보겠습니다. Python 이라는 언어 자체가 사람의 언어와 비슷하기 때문에 문법을 금방 습득 하실 수 있습니다. 그럼 이제 예제와 함께 알아보도록 하겠습니다. if문 문법은 역시 예를 들면서 봐야 빠릅니다. x와 y를 비교하여 x가 크면 “x가 y보다 크거나 깉습니다”를 출력하고 그것이 아니라면 “x가 y보다 작습니다” 라는 문구를 출력해보겠습니다. Python은 다른 문법과 다르게 사람의 언어와 비슷하다고 했지요? 아래의 예를 보면 더 쉽게 이해실 수 있을꺼예요. # x가 1이고 y가 2인 변수가 있다. x = 1 y = 2 # 만약에(if) x가 y보다 크다면(&gt;) if x &gt; y: # print문을 출력해주세요 print('x가 y보다 크거나 같습니다') # 만약에 x가 y보다 크지 않다면, else: # print문을 출력해주세요. print('x가 y보다 작습니다.') 쉽죠? 조금더 간단하게 보자면 아래와 같이 볼 수 있습니다. # 조건이 true이면 실행이 되고 #false이면 다른 실행문 실행 if 조건 1: 실행문 elif 조건 2: 실행문 else: 실행문 그리고 파이썬에서는 콜론(:)을 쓰는 경우는 if문, for loop문, while loop문, def함수문등에서 사용합니다. 그럼 if문에 조금 더 익숙해지기 위해 간단하게 숫자를 입력받아서 짝수인지, 홀수인지를 출력하는 파이썬 코드를 작성해보겠습니다. 파이썬에서 입력을 받을때는 간단하게 “input”을 써주시면 입력받을 수 있습니다. 정수만을 처리 하기 위해 int로 input을 한번 둘러주면 아래와 같이 작성할 수 있습니다. a = int(input(\"숫자를 입력해주세요\")) 그럼 이제 다시 if문을 작성해보겠습니다. 만약 코드 짜는 것에 어려움을 느낀다면, 먼저 글로 작성하고 짜셔도 익히는데는 아무런 지장이 없으니 그렇게 하셔도 좋습니다. if a%2 == 0: print(\"짝수입니다.\") else: print(\"홀수입니다.\") 간단하죠? 이번엔 조금 응용해서 함수로 만들어보겠습니다. if문은 그대로 두고 위에 한줄만 추가한다면 함수가 될 수 있습니다. def mod(a): # 추가 된 한줄 if a%2 == 0: print(\"짝수입니다.\") else: print(\"홀수입니다.\") 앞에 def를 쓰게 될 경우에 함수로 만들겠다는 의미입니다. def 함수 이름 (입력받을 곳) 라고 생각하시면 된답니다. 그래서 제가 만든 위의 함수 이름은 mod 가 되는 것이고 괄호 안에는 변수를 넣거나 숫자를 넣기 위해 빈공간을 생성한 거라 생각하면 쉽습니다. 그래서 아래와 같이 mod함수를 작성하셔도 무방합니다. def mod(a): if a%2 == 0: print(\"짝수입니다.\") else: print(\"홀수입니다.\") mod(10) # 짝수입니다가 출력됨 또는 아래와 같이 사용하실 수 있습니다. def mod(a): if a%2 == 0: print(\"짝수입니다.\") else: print(\"홀수입니다.\") b = 10 mod(b) # 짝수입니다가 출력됨 그럼 이번에 재밌는 문제 하나를 풀면서 익혀보겠습니다. 가우스 덧셈 공식은 아래와 같습니다. 이 가우스 덧셈 공식을 이용하여 1부터 10까지 숫자의 합을 출력해보겠습니다. 문법에 조금 익숙해지기 위해 숫자를 입력 받아서 작성해보겠습니다. a = int(input(\"첫번째 숫자를 입력하세요\")) b = int(input(\"마지막 숫자를 입력하세요\")) if a &lt; b : result = (a + b) * int( b/2) print(result)",
    "tags": "Python",
    "url": "/python/2019/11/28/python-if-odd-numbers.html"
  },{
    "title": "파이썬의 자료형 개념",
    "text": "요즘 핫한 데이터 분석용으로 많이 사용하는 파이썬은 회사나 대학에 파이썬을 많이 사용하고 있습니다. 파이썬이 퇴근을 빨리 하기 위해 만들어진 언어라는 재밌는 썰 알고 계신가요? 그래서 그런지 생각보다 문법이 어렵지 않습니다. 이번에는 간단하게 파이썬의 자료형 개념에 대해 알아보겠습니다. 자료형 개념과 종류 파이썬의 자료형이란? 프로그래밍을 할 때 쓰이는 숫자, 문자열 등 자료 형태로 사용하는 모든 것을 말합니다. 자료형 종류에는 숫자, 문자, 리스트, 튜플, 딕셔너리, 집합 자료형이 있으며, 숫자 자료형은 정수형과 실수형이 있습니다. 정수형은 말 그대로 정수를 뜻하는 자료형이고 양의 정수, 음의 정수, 숫자 0을 말합니다. a = 123 b = -178 실수형은 파이썬에서 소수점이 포함된 숫자를 말합니다. a = 1.2 b = -3.4 문자 자료형은 말그대로 문자를 말하며, 아래와 같습니다. a = \"python\" b = \"is fun\" print(a+b) 리스트는 Python에서 흔히 쓰이는 자료형이며, 데이터의 목록을 다루는 자료형입니다. 리스트를 이용하면 1,3,5,7,9라는 숫자 모음을 다음과 같이 간단하게 표시 할 수 있습니다. a = [1,3,5,7,9] a = [] # 비어있는 리스트 생성하는 방법 리스트에서 특정 데이터를 인덱싱 하는 방법은 아래와 같습니다. a = [1,3,5,7,9] a[0] # 리스트의 첫번째 데이터인 '1'만 꺼내는 방법 튜플은 리스트와 마찬가지로 요소의 집합을 나타내는 자료형이고, 리스트와는 다르게 리스트는 [] 로 감싸지만 튜플은 () 소괄호로 감싸는게 차이점입니다. 리스트는 그 값의 생성, 삭제, 수정이 가능하지만 튜플은 값을 바꿀 수 없습니다. t = () t2 = (1,) t3 = (1,2,3) 이라는 튜플의 요소에 요소 값을 삭제하려고 하면 진짜 삭제가 안되는지 확인 해보겠습니다. del t2[0] # 삭제가 되지 않음 삭제가 되지 않음을 확인 할 수 있습니다. 딕셔너리 자료형은 key와 value를 조합해서 사용하는 자료형입니다. dic = {} dic['파이썬'] = 'www.python.org' 위의 예제에서 “파이썬”이 key(인덱스)가 되고 ‘www.python.org’가 key값이 되는 것을 확인 할 수 있습니다. 딕셔너리의 편리한 점은 key값이나 valuse 값만 따로 볼수 있다는 점입니다. # key만 확인하기 dic.key() # valuse만 확인하기 dic.valuse() 집합 자료형(set 자료형)은 리스트 데이터 집합의 교집합, 합집합, 차집합을 출력할 때, 유용하게 사용되는 자료형입니다. s = set([1,2,3,4,5,6]) s2 = set([4,5,6,7,8,9]) 위의 예제에서 교집합, 합집합, 차집합을 구하면 아래와 같습니다. print(s&amp;s2) # 교집합 print(s|s2) # 합집합 print(s-s2) # 차집합",
    "tags": "Python",
    "url": "/python/2019/11/26/python-data-type-concept.html"
  },{
    "title": "머신러닝에서 단층/다층퍼셉트론(And, XOR Perceptron) 구현하기",
    "text": "저번 포스팅에 이어 이번에는 퍼셉트론에 대해 좀더 자세히 다뤄보겠습니다. 또한 직접 PYTHON으로 구현해보겠습니다. 단층 퍼셉트론 우선 AND Perceptron에 대해 알아보겠습니다. 그림의 원을 뉴런 혹은 노드라고 부릅니다. 입력 신호가 뉴런에 보내질 때는 각각의 고유한 가중치가 곱해집니다. (x0,x1,x2,w0,w1,w2) 뉴런에서 보내온 신호의 총 합이 정해진 한계를 넘어설 때만 1을 출력합니다. 그렇다면 퍼셉트론의 동작원리를 잠시 수식으로 보겠습니다. 퍼셉트론 동작원리는 수식으로 나타내면 위와 같습니다. 퍼셉트론은 복수의 입력 신호 각각에 고유한 가중치를 부여합니다. 가중치는 각 신호가 결과에 주는 영향력을 조절하는 요소로 적용됩니다. AND게이트는 아래의 표와 같습니다. 입력(and) x1 x2 t (타겟) i1 0 0 0 i2 0 1 0 i3 1 0 0 i4 1 1 1 그러면 이제 AND Perceptron 구현해보겠습니다. def AND_Perceptron(x1,x2): w = np.array([0.5,0.5]) b = -0.7 theta = 0 x = np.array([x1,x2]) tmp = np.sum(w*x)+b if tmp &gt; theta: return 1 elif tmp &lt;= theta: return 0 inputData = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) print(\"＊ ＊ ＊ ＊ AND Perceptron ＊ ＊ ＊ ＊ \") for xs1 in inputData: print(str(xs1) + \" ==&gt; \" + str(AND_Perceptron(xs1[0], xs1[1]))) w는 가중치고, b는 bias라고 합니다. bias란 뉴런이 얼마나 활성화 하느냐를 조정하는 매개변수입니다. 그렇다면 이번엔 OR Perceptron을 구현해보겠습니다. def OR_Perceptron(x1, x2): w = np.array([0.5, 0.5]) b = 0 theta = 0 x = np.array([x1, x2]) tmp = np.sum(w * x) + b if tmp &gt; theta: return 1 elif tmp &lt;= theta: return 0 inputData = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) print(\"＊ ＊ ＊ ＊ OR Perceptron ＊ ＊ ＊ ＊ \") for xs1 in inputData: print(str(xs1) + \" ==&gt; \" + str(OR_Perceptron(xs1[0], xs1[1]))) OR Perceptron의 결과는 아래의 표와 같습니다. 입력(or) x1 x2 t (타겟) i1 0 0 0 i2 0 1 1 i3 1 0 1 i4 1 1 1 마지막으로 Not And Perceptron도 구현해보겠습니다. import numpy as np def NotAnd_Perceptron(x1, x2): w = np.array([0.5, 0.5]) b = 0 theta = 0 x = np.array([x1, x2]) tmp = np.sum(w * x) + b if tmp &gt; theta: return 1 elif tmp &lt;= theta: return 0 inputData = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) print(\"＊ ＊ ＊ ＊ Not And Perceptron ＊ ＊ ＊ ＊ \") for xs1 in inputData: print(str(xs1) + \" ==&gt; \" + str(NotAnd_Perceptron(xs1[0], xs1[1]))) Not And Perceptron은 위와 같으며 결과는 아래와 같습니다. 입력(Not And) x1 x2 t (타겟) i1 0 0 1 i2 0 1 1 i3 1 0 1 i4 1 1 0 다층 퍼셉트론 다층 퍼셉트론은 단층퍼셉트론 연산에 중간층을 끼어넣은 것을 말합니다. 함수로 생성해보겠습니다. def xor(x1, x2): # 입력값 : 0층 s1=OR_Perceptron(x1,x2) # 1층 s2=NotAnd_Perceptron(x1,x2) # 1층 s3=AND_Perceptron(s1,s2) # 2층 return s3 inputData = np.array([[0,0],[0,1],[1,0],[1,1]]) print(\"--Xor Perceptron---\") for xs1 in inputData: print(str(xs1) + \" ==&gt; \" + str(xorPerceptron(xs1[0], xs1[1]))) 이렇게 중간층이 있으므로 다층 퍼셉트론이라고 합니다. 단층 신경망 ? 다층 신경망 ? 그렇다면 단층 신경망과 다층 신경망의 차이점이 무엇을까요 ? 단층 신경망 다층신경망 입력층-&gt;출력층 얕은 신경망 ( 입력층 - 은닉층 - 출력층 )   심층 신경망 ( 입력층 - 은닉층들 - 출력층 ) 그렇다면 퍼셉트론과 신경망의 차이점은 무엇일까요? 퍼셉트론 ? 신경망 ? 퍼셉트론은 원하는 결과를 출력하도록 사람이 직접 가중치를 정해줘야 합니다. 신경망은 가중치 매개변수의 적잘한 값을 기계가 데이터로부터 학습해서 자동으로 알아냅니다.",
    "tags": "MachineLearning",
    "url": "/machinelearning/2019/11/07/machine-learning-perceptron.html"
  },{
    "title": "Relu함수에서 변형된 활성화 함수들(leake Relu,Swish)",
    "text": "저번 포스팅에서 이야기 한 활성화 함수 3가지는 가장 기본적인 활성화 함수이며, 흔히 쓰이는 활성화 함수입니다. 그런데 이 활성화 함수 외에도 더 많은 활성화 함수가 있습니다. 오늘은 그 외 나머지 활성화 함수에 대해서 이야기 해보도록 하겠습니다. 오늘 포스팅할 대부분의 활성화 함수는 Relu함수의 변형 버전이 많습니다. Relu 함수에서 변형된 활성화 함수들 Relu 함수에서 변형된 활성화 함수들은 아래와 같습니다. 물론 여기서 Softplus와 Tanh함수는 변형된 함수는 아니라는 점! 기억해주세요. 위의 그래프에서 Leaky ReLu, ELU가 Relu의 변형 함수입니다. 위 그래프 외에도 p-Relu와 swish함수도 다루어보겠습니다. Relu함수는 저번 포스팅때 현업에서도 많이 쓰인다고 이야기 했습니다. 그런데 왜, Relu의 번형들이 많이 생겨나는 걸까요? Relu 또한 저번에 말했던 vanishing현상이 생긴답니다. 왜냐하면 x &lt; 0 때는 0을 출력하고 x&gt;0을 출력하기 때문에 학습할 때, 뉴런이 죽는 경우가 발생하기 때문입니다. 그래서 Relu에 약간의 변화를 줘서 0이 되어 뉴런이 죽지 않도록, 신경망이 죽지 않도록 활성화 함수를 변형 시킨겁니다. 우선 leake Relu함수에 대해 알아보겠습니다. leake Relu는 기존의 Relu의 음수 부분에 약간의 기울기를 준 함수입니다. 때문에 이 활성화 함수는 기울기가 0일때의 문제점을 해결하기 위해 나온 함수입니다. 두번째 활성화 함수는 P-Relu입니다. P-Relu는 leake Relu와 비슷하지만 피라미터는 ∂를 추가해서 x &lt; 0 때 학습에 따라 각도를 바꾸는 활성화 함수입니다. 좋아보이는 이 활성화 함수도 단점이 존재합니다. Out of memory라는 에러를 만날 수 있는데, 그 이유는 학습을 하는 동안에 그 속에서도 또 다시 학습을 하기 때문입니다. 세번째 함수는 ELU함수입니다. ELU 함수는 OOM(Out of memory)를 해결하기 위해 나온 함수입니다. 0이하의 기울기가 곡선이므로 미분이 잘되며, OOM이 뜨지 않으며 정확도가 높다고 합니다. 대신 계산량이 많아 학습시간이 오래걸리는 단점이 있습니다. 마지막 함수는 비교적 최근에 나온 함수인 Swish함수입니다. 2017년 11월 경 구글에서 만든 함수라고 합니다. 가장 효율성이 좋았다고 이야기 합니다. 아마 그래프를 보면서 느끼셨겠지만, 정확도와 효율이 좋은 함수를 보면 음수 기 울기 부분이 상당히 매끄러운 모습을 볼 수 있습니다. 이 뜻은 미분과 기울기의 매끈함이 활성화 함수의 가장 주요 포인트라 할 수있습니다. Relu와 Swish, 이 두가지의 차이점을 분명하게 보여주는 그래프는 아래와 같습니다. Relu와 Swish의 차이점이 분명하게 보이네요. Swish 또한 미분으로 인한 계산량이 많다는 단점은 존재하지만, 정확도와 OOM, vanishing현상을 막기 위해 여러가지 연구를 하고 있다는 점을 잘 알아둬야겠네요.",
    "tags": "MachineLearning",
    "url": "/machinelearning/2019/11/07/machine-learning-leake-relu-swish.html"
  },{
    "title": "머신러닝에서 활성화 함수란?(Activation function)",
    "text": "이번 포스팅에서는 활성화 함수가 무엇인지와 Python으로 간단하게 함수 구현을 해보도록 한다. 활성화 함수(Activation function) 활성화 함수란 신호의 총합을 받아서 다음 신호로 내보낼지 말지를 결정하는 함수를 이야기 한다. 활성화 함수에는 사실 시그모이드(Sigmoid)와 렐루(Relu)함수 외에도 많은 활성화 함수들이 있다. 그중에서도 가장 기본적인 활성화 함수인 Sigmoid, Relu, Softmax 이 3가지에 대해 알아보겠다. 시그모이드(Sigmoid) 시그모이드 함수는 0에서 1사이의 실수를 출력하는 함수를 이야기한다. $h(x)= \\frac { 1 }{ 1+exp(-x) }$ 시그모이드는 자연 상수인 exp을 사용하는데, 덕분에 나중에는 vanishing 현상을 일으키고는 한다. vanishing현상은 레이어의 깊이가 깊어질수록 신경망이 죽는 현상을 이야기 한다. 시그모이드가 포화되어서 가중치가 죽었다라고도 이야기 할 수 있으며, 조금 더 쉽게 예를 들면 0.9 라는 가중치가 있을 때, 업데이트를 위해 똑같이 0.9 * 0.9 * 0.9 …. 을 100번 반복했다고 했을때를 생각 하면 쉽다. 또한 반대로 0.01이라는 가중치가 있을 때, 100번 업데이트를 한다고 하면 0.01 * 0.01 * 0.01 … 반복하게 되는데 점점 작아지는 것을 알 수있다. 또한 Relu보다는 약 4배정도 느린데, 그 이유는 모든 값을 실수로 0에서 1사이로 변환해서 계산하기 때문이다. 시그모이드 함수는 아래의 코드와 같다. import numpy as np def sigmoid(x): return 1/(1+np.exp(-x)) x = np.array([1.0,2.0]) print(sigmoid(x)) 시그모이드 그래프는 아래와 같이 그릴 수 있다. import numpy as np import matplotlib.pylab as plt def sigmoid(x): return 1/(1+np.exp(-x)) x = np.arange(-5, 5, 0.1) # -5부터 5 사이에 0.1 간격으로 x에 담아라 y = sigmoid(x) plt.plot(x,y) plt.ylim(-0.1, 1.1 ) plt.show() 위 코드로 그래프를 출력하면 아래와 같이 출력되는 것을 확인 할수 있다. Relu (Rectifued Linear Unit) 렐루 함수는 입력이 0을 넘으면 그 입력을 그대로 출력하고 0이하면 0을 출력하는 함수이다. 비교적 간단한 수식과 가장 이해하기 쉬운 함수이면서도 현업에서도 가장 많이 사용하는 함수이다. Relu함수를 Python으로 구현하면 아래와 같다. def relu(x): if x &gt; 0: return x else: return 0 print(relu(-2)) # 0 print(relu(0.3)) # 0.3 그래프는 아래와 같다. import numpy as np import matplotlib.pylab as plt def relu(x): return np.maximum(0,x) x = np.arange(-5, 5, 0.1) # -5부터 5 사이에 0.1 간격으로 x에 담아라 y = relu(x) plt.plot(x,y) plt.ylim(-0.1, 1.1 ) plt.show() 소프트맥스(softmax) 소프트맥스(softmax)는 다른 활성화 함수와 다르게 마지막 출력층에서 사용하는 함수이다. 소프트맥스(softmax)함수는 0 ~ 1 사이의 숫자로 출력되는 함수이기는 소프트맥스를 거쳐서 나온 실수의 합은 무조건 1이 되는 함수이다. [1.2],[0.9],[0.4]의 값이 있다고 할때, 소프트 맥스 함수를 거치고 나온 실수들이 [0.46],[0.34],[0.20]의 실수값이 나왔다. 이 실수들을 전부 다 합치게 되면 1이 된다. 이것을 확률로 나타내면 46%/ 34% /20% 이런식으로 표현할 수 있다. 그래서 주로 분류 문제에서 자주 사용 되곤 하는데 대표적으로 mnist 데이터가 되겠다. 필기체 숫자가 있을 때, 어떠한 이미지가 숫자 1인 확률/ 2인 확률 … 하는 확률 문제에서 풀수 있다. 소프트맥스 함수는 아래와 같다. def softmaxFunction(x): expX = np.exp(x) sumExpX = np.sum(expX) return expX / sumExpX a = np.array([2.3, -0.9, 3.6]) y4 = softmaxFunction(a) print(y4, np.sum(y4)) 아래와 같이 소프트 맥스 함수를 사용하게 된다면 nan 값을 볼 수 있다. def softmaxFunction(x): expX = np.exp(x) sumExpX = np.sum(expX) return expX / sumExpX a = np.array([900, 1000, 1000]) y4 = softmaxFunction(a) print(y4, np.sum(y4)) nan 값을 확인 할수 있다. 소프트 맥스 함수 구현시 문제점이 바로 이것이다. 지수함수를 사용하다보니 계산 결과가 너무 크면 오버플로(너무 큰값을 표현할 수 없는 문제)를 야기하는데, 주로 입력신호 중 최댓값을 이용하면 이 문제를 해결 수 있다. 고쳐진 최종 소프트맥스 함수 코드는 아래와 같다. def softmaxFunction(x): expX = np.exp(x - np.max(x)) sumExpX = np.sum(expX) return expX / sumExpX a = np.array([900, 1000, 1000]) y4 = softmaxFunction(a) print(y4, np.sum(y4))",
    "tags": "MachineLearning",
    "url": "/machinelearning/2019/11/06/machine-learning-activation-function.html"
  },{
    "title": "PL/SQL의 예외처리(exception)란?",
    "text": "이번 포스팅에서는 PL/SQL의 예외처리(exception)처리에 대해서 알아보겠습니다. 그리고 예외처리를 왜 해야하는지와 종류에 대해 알아보겠습니다. 예외처리(exception) ? 에외처리는 오라클에 에러가 났을 때, 엔드유져(프로그램 사용자)의 눈높이를 맞추기위해 사용하는 문법입니다. 예를들어 홈플러스 계산원이 컴퓨터 프로그램 화면에 고객번호를 입력하고 해당고객정보를 보려고 했는데, 오라클 에러메세지가 화면에 “ORA-0001 Data not found ….” 이런식으로 나온다면 사용자의 경우 알 수 없습니다. 그렇기 때문에 “해당 고객은 존재하지 않습니다.”와 같이 에러 메세지를 바꿔주어야 합니다. 또한 비정상장적으로 프로그램이 종료되지 않기 위해서도 사용합니다. Data가 잘못되어서 프로그램이 종료되버린 현상을 막기위해서 프로그램이 종료되는게 아니라 정상적으로 처리가 되고 Data가 잘못되어서 발생하는 에러 메세지만 따로 출력해주겠금 하려고 예외를 쓰기도 합니다. 예외처리(exception) 종류 3가지 PL/SQL의 예외처리에는 3가지의 종류가 있습니다. 오라클에서 미리 정의한 예외처리, 오라클에서 미리 정의하지 않은 예외처리, 사용자 정의 에외처리가 있습니다. 우리가 알아야 할것은 사용자 정의 예외처리입니다. 사용자 정의 예외처리는 오라클에서 미리 정의한 예외나 오라클에서 미리 정의하지 않는 예외는 둘다 오라클 에러 메세지가 출력될 때, 발생하는 예외인데, 그에 반해 사용자 정의 예외는 오라클에서 에러가 발생하지 않지만 이것은 예외다 라고 사용자가 정의하는 것을 말합니다. 예외처리(exception) 사용하기 PL/SQL을 이용하여 사원번호를 입력하면 해당 사원의 월급이 출력 해보겠습니다. accept p_empno prompt ' 사원번호? : ' declare v_empno emp.empno%type := &amp;p_empno; v_sal emp.sal%type; begin select sal into v_sal from emp where empno = v_empno; dbms_output.put_line(v_sal); end; / 위와 같이 입력하면 출력이 되는데, 저 코드에서 예외처리를 해보겠습니다. 예외처리를 할때에는 begin절에 exception~ 을 넣으면 됩니다. accept p_empno prompt ' 사원번호? : ' declare v_empno emp.empno%type := &amp;p_empno; v_sal emp.sal%type; begin select sal into v_sal from emp where empno = v_empno; dbms_output.put_line(v_sal); exception when no_data_found then dbms_output.put_line('없는 사원입니다.'); end; / 위의 코드는 실행절에 넣는 예외처리였습니다. 그렇다면 declare절에도 예외처리를 할 수 있을까요? 물론 가능합니다. declare 절에서 예외처리를 사용할 경우는 보통 먼저 체크해야할 부분이 있을 때 사용합니다. 예를 들어 월급을 물어보고 월급을 입력하지 않고 enter를 치면 ‘월급을 반드시 입력해야 합니다’ 라는 메세지를 출력하는 경우에 declare절에 사용 할 수 있습니다. accept p_empno prompt ' 사원번호를 입력하세요 : ' accept p_ename prompt ' 이름을 입력하세요 : ' accept p_sal prompt ' 월급을 입력하세요 : ' declare e_sal_nl exception; pragma exception_init( e_sal_nl, -01400 ) ; begin insert into emp(empno, ename, sal) values(&amp;p_empno, '&amp;p_ename', '&amp;p_sal'); dbms_output.put_line(SQL%rowcount || ' 건이 입력되었다. '); exception when e_sal_nl then dbms_output.put_line ( '월급을 반드시 입력해야 합니다.'); end; /",
    "tags": "DB",
    "url": "/db/2019/11/05/plsql-exception.html"
  },{
    "title": "PL/SQL이란?",
    "text": "PL/SQL이란 무엇인지, 기본 block 구조와 변수 사용법에 대해 간단하게 알아보겠습니다. PLSQL 이란? PLSQL이란 비절차적 언어인 SQL + 프로그래밍(if,loop)를 이야기합니다. 즉, 절차적 언어로 만드는 프로그래밍입니다. if문과 loop문을 사용할 수 있으며, 반복되는 단순한 업무를 자동화 할 수 있습니다. PL/SQL을 배워야 하는 이유 우선 PL/SQL은 수많은 단순작업을 자동화하기 위해서 사용합니다. 우리가 분석해야할 데이터들은 크게 2가지 타입으로 볼 수 있습니다. samll data -&gt; business data business data를 분석하려면, 알고리즘 사고방식을 갖춰야할 필요가 있습니다. 그래서 수학식을 PL/SQL 또는 Python 으로 구현하려고 합니다. big data ? “ 지금까지 한번도 분석해보지 않은 data” 소셜미디어에 올라오는 data, 트위터, 페이스북, 인스타그램 data는 마케팅에 조금 도움되는 data일뿐 크게 중요하지 않은 data입니다. 우리가 분석해야할 가장 중요한 데이터는 대부분 RDBMS에 저장되어 있습니다. SQL을 사용해서 분석하기에는 노가다를 해야할 일들이 많습니다. 그래서 힘들게 하드코딩을 하지 않고 쉽게 business data를 분석을 하려면 PL/SQL을 사용해야합니다. PL/SQL을 사용하게 되면 SQL로 보여줄수 있는 분석 결과와 PL/SQL로 보여줄수 있는 분석 결과가 훨신 더 많습니다. SQL로는 DATA에서 정보를 추출해서 보여지는 현상을 증명할 수 있을 뿐이지만, PL/SQL로 데이터 분석을 하게 되면, 회귀 분석을 구현하여 방정식을 도출해내고 결정트리를 이용해 확률 및 엔트로피 등을 구할수 있수 있습니다. PL/SQL에서의 기본 용어 식별자는 v_ename, v_sal 과 같은 변수를 이야기 합니다. 구분자는 * / + - 와 같은 연산자를 이야기 합니다. 리터럴은 7788, SCOTT 과 같은 문자나 숫자를 이야기 합니다. PL/SQL에서도 주석을 사용할 수 있으며 실행하고 싶지 않은 구문을 실행하지 않고자 할때 사용합니다. PL/SQL에서의 SQL 함수 사용법 declare 절의 경우 사용할 수 있는 함수는 단일행 함수 뿐이며, 복수행 함수나 decode 함수 등은 사용할 수 없습니다. begin 실행절의 경우 어떠한 함수든 다 사용할 수 있습니다. PL/SQL의 기본 block 의 구조 PL/SQL은 선언절, 실행절, 종료절이 있습니다. declare -- 선언절 -- 변수, 상수,예외 등을 선언한다. begin -- 실행절 -- 실행할 SQL, IF문 , LOOP 문등을 실행한다. end ; -- 종료절 / -------------------------PL/SQL 을 종료하겠다. * 주의 : /는 벽에 딱 붙여야 한다.",
    "tags": "DB",
    "url": "/db/2019/11/04/plsql.html"
  },{
    "title": "K-Nearest Neighbors(Knn)을 이용한 소개팅 데이터 분류",
    "text": "포스팅에서 사용된 데이터는 여기에서 데이터를 다운 받을 수 있다. Dataset 데이터 설명을 하자면, 소개팅을 수천번하지 않기 때문에 그동안 14번정도 소개팅을 했다고 가정하고 만난 남학생들에 대한 라벨을 호감라벨로 만들었다. 14개의 데이터를 knn으로 학습시켜서 예측 모델을 생성하고, 새로 만나게 될 남학생에 대한 데이터로 이 학생의 호감 라벨을 예측해보려고 한다. 머신러닝을 할 때는 필수적으로 데이터 전처리라고 하는 Input 데이터를 알고리즘에 맞게 잘 정재하는 작업이 필요하다. 이것을 표준화 또는 정규화 작업이라고도 한다. 데이터 양이 작을 경우에는 scale() 이란 함수를 사용한다. 음수 ~ 양수 반대로 데이터의 양이 많을 경우에는 nomalize() 이란 함수를 사용한다. 0 ~ 100 왜 표준화나 정규화를 해야하는가? 어떤 데이터는 cm이고 어떤 데이터는 kg이면 결과가 제대로 나올수가 없다. 그래서 cm이던 kg이던 0~100 사이의 데이터로 일관되게 만드는 작업이 필요하다. knn Example R에서 csv파일을 불러올때는 아래와 같이 하면 불러올 수 있다. like&lt;-read.csv(\"like.csv\", header = T) 출력하면 아래의 데이터가 나오는 것을 확인할 수 있다. 한글 컬럼명이 생각보다 불편한거 같아서 컬럼명을 바꿔보았다. 컬럼명은 아래와 같이 입력하면 바꿀 수 있다. colnames(like)&lt;- c( \"taik\" ,\"book\" ,\"travel\", \"school\" ,\"tall\" ,\"skin\", \"muslce\" ,\"label\") 출력하면 아래와 같이 컬럼명이 변경되어 나오는 것을 확인할 수 있다. 이제 knn을 이용해서 분류를 하려고 하는데 맨 끝 컬럼에 있는 label과 data를 분리해야한다. 분리는 아래와 같이 하시면 컬럼을 분리하실 수 있다. train_data&lt;-like[,-8] train_label&lt;-like[,8] [,-8]은 8번째 컬럼을 제외하고 라는 뜻이고, [,8]는 8번째 컬럼만 이라는 뜻이다. 이제 진짜 knn을 사용해서 분류를 하려고 하는데 간단하게 test 데이터를 만들고 분류를 해보았다. 여기서 test data는 위에서 언급했듯이 새로 만나게 될 사람에 대한 호감 라벨을 알아보기 위한 데이터다. 데이터를 생성을 위해 아래의 코드를 넣어 데이터를 생성하자. test.data&lt;-data.frame(talk=70, book=50,travel=30,school=70, tall=70, skin=40,muslce=50) 데이터를 넣었으면, 이제 knn분류를 해보았다. library(class) result&lt;- knn(train_data, test.data, train_label, k=3 , prob=TRUE) 그리고 출력하면 3타입이라는 결과를 얻을 수 있다. 그렇다면 위의 모델을 다시 생성하는데, k값을 다르게 했을때, 호감 라벨이 어떻게 변화하는지 확인해보자.",
    "tags": "R",
    "url": "/r/2019/11/01/machine-learning-knn2.html"
  },{
    "title": "R을 배워야 하는 이유와 자료구조",
    "text": "R은 데이터 분석할 때 주로 사용하는 언어이다. 요즘 데이터 분석가 모집 요망을 보면 SQL과 R은 기본적으로 다루는 기술이기 때문에 데이터 분석을 하고 싶어하는 사람에게는 R은 필수이다. 데이터 분석이 필요한 이유에 대해 사례 하나를 들어볼까한다. 카카오톡은 2010년 3월에 서비스를 시작한 이후에 폭발적으로 성장해서 2012년 말에 8000만명의 가입자를 넘어섰다고 한다. 하루 평균 43분간 카카오톡을 하고 하루 방문자가 2700만명, 하루 최대 메세지 이용건수가 42억건에 달한다고 한다. 예전에는 “문자해”라는 말이 현재는 “카톡해”로 바뀐거를 보면 정말 카카오톡은 성공했다고 볼 수 있다. 이런 성공에도 불고하고 마땅한 수익모델이 없어서 2010년에 41억 적자, 2011년 153억 적자였다고 한다. 카톡 사용자들에 대한 데이터 분석을 하고난 다음부터 2012년에 애니팡의 인기에 힘 입어 흑자라인으로 들어섰다고 한다. 그렇다면 카카오는 카톡 사용자들을 어떻게 분석했을까 ? 누가(성별, 연령) 무엇을(게임 아이템) 언제(아이템 구매시간) 얼마나(아이템 구매빈도) 어떻게(결제 방법) 카카오는 위 5가지를 데이터 분석해서 사용자 패턴을 알아내어 마케팅에 성공했다고 한다. 그렇다면, 똑같이 데이터를 다루는 SQL과 R의 차이점은 무엇일까? SQL과 R의 차이점 R과 SQL의 차이점이 무엇일까? 아주 긴 SQL코드를 R코드로 단순하게 작성할 수 있다. 예) select deptno, sum(decord(job,'SALESMAN',sal0), sum(decord(job,'ANALYST',sal,0) ..... 이것을 R로 만들면 , emp&lt;-read.csv(\"emp.csv\",header=T) attach(emp) tapply(sal,list(deptno,job),sum) 데이터를 시각화 할 수 있다.(가장 큰 이유) pie(emp$sal) R이란? 뉴질랜드의 aukland 대학의 robert gentlman 과 Ross ihaka 가 1995년에 개발한 소프트웨어이고 데이터 분석을 위한 통계 및 그래픽스를 지원하는 무료 소프트웨어있다. R을 왜 사용해야하는가? R is free data 분석을 위해 가장 많이 쓰는 통계 플랫폼이다. 복잡한 데이터를 다양한 그래프로 표현할 수 있다. 분석을 위한 데이터를 쉽게 저장하고 조작할 수 있다. 누구든지 유용한 패키지를 생성해서 공유할 수 있고 새로운 기능에 대한 전달이 빠르다. 어떤 os에서 설치가 가능하다. 심지어 아이폰에서도 설치가 된다. R의 자료구조 vector : 같은 데이터 타입을 갖는 1차원 배열구조 matrix : 같은 데이터 타입을 갖는 2차원 배열구조 array : 같은 데이터 타입을 갖는 다차원 배열구조 data.frame : 각각의 데이터 타입을 갖는 컬럼으로 이루어진 2차원 배열구조 list : 서로 다른 데이터 구조인 데이터 타입이 중첩된 구조 그림으로 설명하면 아래의 그림과 같다.",
    "tags": "R",
    "url": "/r/2019/10/30/data-structure.html"
  },{
    "title": "머신러닝 알고리즘, Knn(K-Nearest Neighbors)이란?",
    "text": "오늘은 K-Nearest Neighbors(KNN)라는 알고리즘에 대해 알아보려고 한다. Knn 머신러닝을 공부하면 가장 쉽게 먼저 접하는 알고리즘 중 하나이다. 이번 포스팅에서는 Knn이 무엇인지, 필요한 이유에 대해 알아보자. knn이란 무엇인가? 사회적인 관계를 관찰해본적인 있나요? 대략적으로 비슷한 사람끼리 모이는 성질이 있다고 한다. 그래서 비슷한 취향의 사람들끼리 모여서 동회회를 만들거나 비슷한 부류의 계층의 사람들끼리 친분을 맺기도 한다. 그렇다면 공간적은 관계를 관찰해볼까? 길을 지나다가 보면 가구점이 모여있는 상가지역이 따로 형성된 곳이 있다. 한약방이 밀집되어 있는 지역이나, 가구점, 음식점 등 밀집되어 있는 지역이 따로 모여 있는 경우가 많은 것을 우리는 길을 지나다가 느낄 수 있다. 이러한 특성을 가진 데이터를 겨낭해서 만들어진 알고리즘이 knn이다. knn이 필요한 이유 유방암의 데이터를 가지고 있다고 가정을 했을때, 유방암의 종양의 크기에 대한 데이터(반지름, 둘레, 면적 등)만을 가지고 이 종양이 악성인지, 양성인지 예측할 수 있다. 그렇다면 환자에 대한 치료 스케줄에 큰 영향을 미칠 수 있다. knn 분류 이해 knn은 어떤 라벨을 정의할 때, 그 데이터의 주변 반경 ε(입실론)안의 데이터들의 라벨들을 조사하여 다수결로 k개 이상이면 가장 많은 라벨로 정의 하는 것을 knn 분류라고 한다. knn 분류 예시 knn을 5로 잡았을 때를 가정으로 하면 위처럼 표시 할 수 있다. A에는 빨간색이 많았으므로 물음표안에는 빨간색이 들어가고, B에는 파란색이 들어간다. 어떠한 라벨을 정의할 때, 다수결로 k개 이상이면 가장 많은 라벨로 정의 하는 것을 knn 분류라고 한다.",
    "tags": "R",
    "url": "/r/2019/10/30/machine-learning-knn.html"
  },{
    "title": "머신러닝 신경망에서 미분이 필요한 이유",
    "text": "신경망이라는 것을 접하면서 미분, 기울기와 같은 말을 몇번 들어봤다. 우리는 왜 미분을 사용하는 것인지, 미분이 필요한 이유와 도함수를 Python 으로 간단하게 구현해보도록 하겠다. 미분이 필요한 이유 미분이 필요한 이유는 기존의 가중치를 갱신하기 위해서 사용한다. 하지만 진정한 미분은 컴퓨터로 구현하기 어렵기 때문에 중앙 차분 오차가 발생한다. 그래서 컴퓨터로 미분을 구현하기 위해서는 수치 미분을 사용해야한다. 함수 y=f(x)에 대해서 x가 x_{0} 에서 x_{0}+h 로 변화할 때, y의 값은 f(x_{0}+h)로 변화한다고 할 때, 평균 변화율은 아래와 같다 그림을 보면 h가 서서히 0으로 다가가면 할선은 접선으로 다가간다. 이를 코드로 구현하려면 h가 되었을때가 진정한 미분이라고 할수 있는데, 아까 말했던 것과 같이 컴퓨터로는 진정한 미분을 구현하는 것은 어렵다. h가 0이 될 수 없기 때문이며, 그래서 h를 0.0001로 두고 수치미분을 한다. 수치 미분 함수 이름은 numerical_diff 라고 짓었다. 위의 공식을 보고 그대로 만들면 되기 때문에 간단하게 함수를 구현할 수 있다. def numerical_diff(f,x): h=0.001 return (f(x+h)-f(x-h)) / (2*h) 위의 미분 함수를 이용하여 f(x)=x^2 + 4^2 함수를 미분하고 x=4에서의 미분계수(기울기)를 구하면, 2x + 0 상수는 도함수가 되면 0이 되고, 2*4 = 8 이 된다. def samplefunc1(x): return x**2 + 4**2 print(numerical_diff(samplefunc1,4)) 결과 값을 출력하면 8이 나오지 않고 근사치가 나오는 것은 수치미분이기 때문에 중앙 차분 오차가 발생하기 때문이다. import numpy as np import matplotlib.pylab as plt def samplefunc1(x): return x**2 + 4**2 x = np.arange(0.0, 20.0, 0.1) y = samplefunc1(x) plt.plot(x,y) plt.show() 여기에 도함수 그래프를 넣어 겹쳐서 보려고 한다. import numpy as np import matplotlib.pylab as plt def numerical_diff(f,x): h=0.001 return (f(x+h)-f(x-h)) / (2*h) def samplefunc1(x): return x**2 + 4**2 def afterfunc(x): return numerical_diff(samplefunc1,4)*x x1 = np.arange(-10.0, 20.0, 0.1) y1 = samplefunc1(x1) plt.plot(x1,y1) x2= np.arange(-10.0, 20.0, 0.1) y2 = afterfunc(x2) plt.plot(x2,y2) plt.show()",
    "tags": "MachineLearning",
    "url": "/machinelearning/2019/10/29/machine-learning-differential-neural-network.html"
  },{
    "title": "K-Nearest Neighbors(Knn)을 이용한 구매여부 분류하기",
    "text": "knn을 사용하여 조금 의미 있는 결과를 가지고 결과를 뽑아내보려고 한다. 데이터는 여기에서 데이터를 다운 받을 수 있다. DataSet 데이터를 열어보면 나이, 월수입, 상품 구매여부, 나이가 있다. 이 데이터를 이용해, 백화점 또는 소셜커머스 회사에서 데이터 분석을 통해 구매자가 제품을 구매할 고객인지 아닌지를 알아내려고 한다고 가정해보고 knn을 이용하여 문제를 풀어보도록 한다. Nomalize (정규화) 데이터를 불러오자. buy&lt;-read.csv(\"buy.csv\",fileEncoding = \"euc-kr\") 이 데이터에서 scale함수를 이용하여 나이 컬럼을 정규화 해주었다. buy$age &lt;- scale(buy$나이) 뒤에 age컬럼이 추가와 동시에 정규화 된 컬럼이 추가가 된 것을 확인할 수 있다. buy$pay &lt;- scale(buy$월수입) 위와 같이 컬럼이 또 추가 된 것을 확인 할 수 있다. 그럼 이제 knn을 이용하여 구매분류를 해보자. knn 구매여부 분류하기 train_data&lt;-buy[,c(4,5)] train_label&lt;-buy[,3] 첫번째 줄은 4, 5번째 컬럼만 따로 train_data이라는 변수에 담겟다는 의미이다. 두번째 줄은 3번째 컬럼만 train_label이라는 변수에 담겠다는 의미이다. test_data &lt;- data.frame(age=44, pay=400) 나이가 44살에 월급이 400이라는 데이터를 하나 만들었다. 여기서 주의! test data도 마찬가지로 정규화를 해주어야 결과값을 볼 수 있다. test data도 정규화 작업을 한 후에 knn을 해준다. test data도 정규화 했다고 치고, 결과를 출력해 보겠다. result &lt;-(test_data, test_data, train_label , k=5, prob =TRUE) 출력하면 “구매”라는 결과를 볼 수 있다.",
    "tags": "R",
    "url": "/r/2019/10/27/k-nearest-neighbors-knn3.html"
  },{
    "title": "인공지능 R&amp;D 그랜드 챌린지 대회 나간 후기",
    "text": "우연찮게 대학원분들과 우리 회사랑 해서 같이 인공지능 챌린지에 나갔다. 그러나 이 챌린지에 나간 것을 반은 후회한다고 이야기 할 수 있다. ‘인공지능에 대해 개념이 없다’라고 해야댈지 아니면 까다롭다고 해야댈지 사실 잘모르겠다. 대회가 열리기까지 정말 많은 사건사고가 있던거 같다. 제대로 된 공지도 하지 않았고, 대회 일자가 갑자기 미뤄진 이유가 대학생들 시험기간이라고 편의를 위해 미뤘다면서 이야기 한다거나, 데이터도 제대로 된 것을 제공하지 않았다. 그래서 중간에 설명회 열렸을 때, 기관과 공모전 참여자들의 엄청난 싸움이 있었다. 정말 말 그대로 개판이였고 월래 이렇게 다들 싸우나 싶을 정도였다. 대회가 열리는 그 순간까지 참가자들의 많은 욕을 들었던 것 같다. 대회 내부적으로 사용하는 프로그램이 있어서 json형식으로 결과물을 제출해도 프로그램이 파일을 읽지 못한다면, 그냥 0점처리 되는 체계였다. 그덕에 우리팀 파일도 해당 대회에서 파일을 읽지못해 0점처리가 되었고, 많은 팀들이 0점 처리 되어 대회가 끝난 후 항의도 만만찮게 왔다. 이 대회를 하면서 느꼈던건 국가 대회라고 해서 시스템이 잘되어 있을꺼라 생각하지는 말것과 팀원들과의 많은 소통을 하지 못해서 더 좋은 결과를 뽑아내지 못했다는 아쉬움뿐이다.",
    "tags": "ETC",
    "url": "/etc/2019/10/01/ai-challenge.html"
  }]};
